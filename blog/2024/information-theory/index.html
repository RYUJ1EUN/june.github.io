<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="n8dLJYW35x7DJL8Qm6jd505pT0eW3ucsWsW4_GaH3MA"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Information Theory | June </title> <meta name="author" content="Jieun Ryu"> <meta name="description" content="Information theory: Probability and Entropy"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/june.github.io/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/june.github.io/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/june.github.io/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/june.github.io/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/june.github.io/assets/img/whale.png?v=169c6de4639e20133f0d09309d538e58"> <link rel="stylesheet" href="/june.github.io/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ryuj1eun.github.io/june.github.io/blog/2024/information-theory/"> <script src="/june.github.io/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/june.github.io/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/june.github.io/"> June </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/june.github.io/">ABOUT </a> </li> <li class="nav-item active"> <a class="nav-link" href="/june.github.io/blog/">BLOG </a> </li> <li class="nav-item "> <a class="nav-link" href="/june.github.io/til/">TIL </a> </li> <li class="nav-item "> <a class="nav-link" href="/june.github.io/publications/">PUBLICATIONS </a> </li> <li class="nav-item "> <a class="nav-link" href="/june.github.io/cv/">CV </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">SUB </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/june.github.io/books/">Books</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/june.github.io/projects/">Exhibitions</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Information Theory</h1> <p class="post-meta"> Created on January 02, 2024 </p> <p class="post-tags"> <a href="/june.github.io/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> Â  Â· Â  <a href="/june.github.io/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> MATH</a> Â  Â· Â  <a href="/june.github.io/blog/category/study"> <i class="fa-solid fa-tag fa-sm"></i> Study</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>ğŸ“– Cover, Thomas M.Â <em>Elements of information theory</em>. John Wiley &amp; Sons, 1999.</p> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_153653.png" alt="ì¼ë°˜ì ì¸ í†µì‹  ì‹œìŠ¤í…œì˜ ë„ì‹" width="90%"></p> <p>ì´ìƒì ì¸ channelì€ transmitterë¡œë¶€í„° receiverê¹Œì§€ ì˜¤ë¥˜ ì—†ì´ ì •ë³´ê°€ ì „ë‹¬ë˜ëŠ” channelì´ì§€ë§Œ, ì¼ë°˜ì ì¸ channelì€ noise channelì„</p> <h3 id="probability">Probability</h3> <p><strong>Event</strong> : ì¼ì–´ë‚  ìˆ˜ ìˆëŠ” ì–´ë–¤ ì‚¬ê±´, í™•ë¥  ë³€ìˆ˜ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’, $X=a$</p> <ul> <li> <p>í™•ë¥  ë³€ìˆ˜ê°€ ì£¼ì–´ì§ â‡” í™•ë¥  ë³€ìˆ˜ë¥¼ ì• â‡” í™•ë¥  ë¶„í¬ë¥¼ ì•</p> <p>ex. ë™ì „ì„ ë˜ì¡Œì„ ë•Œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ë©´ì— ëŒ€í•œ í™•ë¥  ë³€ìˆ˜ X</p> <p>í™•ë¥  ë³€ìˆ˜ Xâˆˆ{ì•ë©´, ë’·ë©´}</p> <p>í™•ë¥  ë¶„í¬ëŠ” P(X=ì•ë©´)=1/2, P(X=ë’·ë©´)=1/2</p> </li> </ul> <p>ê²°ê³¼ë¡œ ë„ì¶œë  ìˆ˜ ìˆëŠ” ëª¨ë“  eventë¥¼ ëª¨ì€ ì§‘í•©ì´ sample spaceì´ê³ , â€˜ì–´ë–¤ eventê°€ ì–¼ë§ˆë‚˜ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ”ê°€?â€™ì— ëŒ€í•œ í™•ë¥ ì´ ì£¼ì–´ì¡Œì„ ë•Œ eventì— ëŒ€í•œ ë³€ìˆ˜ê°€ í™•ë¥  ë³€ìˆ˜ì´ê³  ì´ í™•ë¥  ë³€ìˆ˜ì˜ eventsì— ëŒ€í•œ í™•ë¥ ì´ í™•ë¥  ë¶„í¬ë¥¼ ì´ë£¸</p> <p>â‡’ í™•ë¥  ë³€ìˆ˜ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì€ ì–´ë–¤ eventì˜ í™•ë¥ ì„ ë‹¤ë£¨ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë³€ìˆ˜ì˜ í™•ë¥  ë¶„í¬ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒ</p> <p>í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜(probability mass function) : ì´ì‚° í™•ë¥  ë³€ìˆ˜ì˜ í™•ë¥  ë¶„í¬ì— ë”°ë¥¸ í•¨ìˆ˜</p> <ul> <li> <p>í™•ë¥  ë³€ìˆ˜ $X$ë¥¼ ì‚¬ìš©í•œ í•¨ìˆ˜ $f(X)$ë„ ë³€ìˆ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê°–ëŠ” í™•ë¥  ë³€ìˆ˜ì„</p> <p><strong>í™•ë¥  ë³€ìˆ˜</strong></p> <p>ì‚¬ê±´ ê³µê°„ì—ì„œ ê°€ì¸¡ ê³µê°„ìœ¼ë¡œì˜ ê°€ì¸¡ í•¨ìˆ˜</p> <p>ex. $f(X)= \begin{cases} 1 &amp; \text{ if } X= \textrm{Head}<br> 0 &amp; \text{ if } X= \textrm{Tail} \end{cases}$ ì´ë•Œ, $X$ëŠ” eventë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜</p> </li> </ul> <p>í™•ë¥  ë³€ìˆ˜ $f(X)$ì˜ í‰ê·  $\mathbb{E}[f(X)]:=\sum P(X=x)f(X=x)$</p> <h4 id="joint-distribution"><strong>Joint distribution</strong></h4> <p>P(X,Y) : joint distribution</p> <p>P(X), P(Y) : marginal distribution</p> <p>â†’ joint distributionì´ ì£¼ì–´ì§€ë©´ marginal distributionì„ êµ¬í•  ìˆ˜ ìˆìœ¼ë‚˜ ì—­ì€ ì„±ë¦½í•˜ì§€ ì•ŠìŒ</p> <table> <tbody> <tr> <td>P(Y</td> <td>X=x) : conditional distribution</td> </tr> </tbody> </table> <p>YëŠ” R.V.ì´ê³ , ì¡°ê±´ë¶€ì˜ xëŠ” value</p> <p>X=2ì¸ í–‰ì— ëŒ€í•œ í™•ë¥ ì€ X=2ì¼ ë•Œì˜ Yì— ëŒ€í•œ ì¡°ê±´ë¶€ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ„</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled.png" alt="marginal dist. and joint dist."></p> <ul> <li> <p>ex</p> <table> <thead> <tr> <th>Y \ X</th> <th>0</th> <th>1</th> <th>2</th> <th>P(Y)</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1/4</td> <td>1/8</td> <td>1/8</td> <td>1/2</td> </tr> <tr> <td>1</td> <td>1/4</td> <td>0</td> <td>1/4</td> <td>1/2</td> </tr> <tr> <td>P(X)</td> <td>1/2</td> <td>1/8</td> <td>3/8</td> <td>Â </td> </tr> </tbody> </table> </li> </ul> <h3 id="information-and-uncertainty">Information and Uncertainty</h3> <ul> <li> <p><strong>Information : level of surprise</strong></p> <p>â†’ ë‚®ì€ í™•ë¥  = ë†’ì€ entropy</p> <p>ì–¼ë§ˆë‚˜ ì ì€ í™•ë¥ ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì•Œê²Œ ë˜ì—ˆì„ ë•Œ ì–¼ë§ˆë‚˜ ë†€ë¼ì›€ì„ ì£¼ëŠ”ê°€</p> <p>$X$: random variable, $X\in \mathcal{X}= \left{ x_1, \ldots, x_n \right}$</p> </li> <li> <p>The information of an event $X=x_i$ is defiend by $I(x_i) := \log\frac{1}{P(X=x_i)} = -\log p(x_i)$</p> <p>ì •ë³´ëŠ” $P(X=x_i)$ì—ë§Œ dependí•œë‹¤. ì¦‰, $I(x_i)=I(p(x_i))=-\log p(x_i)$</p> <p>ğŸ’¡ ì™œ $\log$ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ê°€?</p> <p>ì •ë³´ëŠ” ë‹¤ìŒì„ ë§Œì¡±í•¨</p> <ol> <li>í™•ë¥ ì´ ë‚®ìœ¼ë©´ ì •ë³´ê°€ ë§ê³ , í™•ë¥ ì´ ë†’ìœ¼ë©´ ì •ë³´ê°€ ì ìŒ</li> <li> <p>$0 \le P(X=x_i) \le 1$</p> <p>$I(x_i)\ge0\quad\because0&lt;p(x)\le1$</p> <p>(í™•ë¥ ì´ 0ì¸ eventëŠ” $\mathcal{X}$ì— ì†í•˜ì§€ ì•ŠìŒ)</p> </li> <li> <p>Fundamental Axioms (axiomatic apprach)</p> <p>$I(p)$ : information measure</p> <ol> <li>$I(p)\ge0$</li> <li>$p=1\quad \Rightarrow \quad I(p)=0$</li> <li> <p>For tow independent events $P(X=x_i), P(Y=y_i)$,</p> <p>$I(X=x_i, Y=y_j)=I(X=x_i)+I(Y=y_j)$</p> <p>because, $P(X=x_i ,\ Y=y_i)=P(X=x_i)(Y=y_i)$</p> </li> </ol> </li> <li>The information measure $I(p)$ is continuous</li> </ol> <p><img src="/june.github.io/assets/img/post_information_theory/PZLMYNWXnYn2pWQVXLCAm7rDpopTP36K_477OXTN8SD2LDdPCXXy-K3JFuJlMHfQRXRSGf7cP9WIUFRtU1-FcMDwuJdBazKIHPQ8psEu_w0CKdiMDVe82MpHK_qxUIluLzc-X_Zfm_S-NWXNkA9GVA.webp" alt="log_a function"></p> <p>logì˜ ë°‘ì„ 2ë¡œ ì‚¬ìš©í•˜ë©´ ì •ë³´ëŸ‰ì˜ ë¹„íŠ¸ ìˆ˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŒ</p> </li> <li> <p>ë¶„í¬ì˜ information = $\mathbb{E}[I(X)] = \sum_x P(X=x)I(X=x) = -\sum_x p(x)\log p(x)$</p> <p>â€œí™•ë¥  ë³€ìˆ˜ë¥¼ ì• â‡” í™•ë¥  ë¶„í¬ë¥¼ ì•â€ì´ê¸° ë•Œë¬¸ì— R.V.ì˜ informationì„ ì•„ëŠ” ê²ƒì€ ë¶„í¬ì˜ informationì„ ì•„ëŠ” ê²ƒê³¼ ê°™ìŒ</p> <p>ë¶„í¬ì— ëŒ€í•œ informationì€ ê°ê°ì˜ $X=x_i$ì— ëŒ€í•œ informationì„ ë¬¼ì–´ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ëª¨ë“  $x$ì— ëŒ€í•œ informationì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•˜ì—¬ ë¶„í¬ì˜ informationì„ ì •ì˜í•¨</p> </li> </ul> <h4 id="mutual-information">Mutual information</h4> <ul> <li> <p>For two R.V. $X\leftarrow{x_1,x_2,\ldots,x_m},\ Y\leftarrow{y_1,y_2,\ldots,y_m}$,</p> <p>(case 1) $X, Y$ are independent</p> <p>$Y=y_j$ì˜ ë°œìƒì´ $X=x_i$ì— ëŒ€í•œ ì–´ë– í•œ ì •ë³´ë„ ì œê³µí•˜ì§€ ì•ŠìŒ</p> <p>(case 2) $X,Y$ are fully dependent</p> <p>$Y=y_j$ì˜ ë°œìƒì´ $X=x_i$ì˜ ë°œìƒì„ ê²°ì •</p> <p>â†’ í†µì‹  ê³¼ì •ì—ì„œ Y=yë¥¼ ì „ì†¡ ë°›ìœ¼ë©´ ì‹¤ì œë¡œ ë³´ë‚´ì§„ ê°’ Xê°€ xì˜€ìŒì„ ë³´ì¥í•  ìˆ˜ ìˆìŒ</p> <p><em>(case 2)ì˜ ê·¸ë¦¼</em></p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%201.png" alt="(case 2)ì˜ ê·¸ë¦¼"></p> </li> <li> <p>$I(X;Y)$ : mutual information (average of mutual information $X=x_i, Y=y_j$) â€” for R.V.s</p> <p>$I(X;Y)=\sum_{x,y}p(x,y)I(x,y)$</p> <p>I(x,y)ëŠ” X=x,Y=yì¼ ë•Œì˜ ìƒí˜¸ ì •ë³´ëŸ‰ì„ ë‚˜íƒ€ëƒ„</p> <p>ì¼ë°˜ì ìœ¼ë¡œ joint informationì„ ì •ì˜í•œ ê¸°í˜¸ê°€ ì—†ê¸° ë•Œë¬¸ì— ê³ ì •ëœ $x_i, y_j$ì— ëŒ€í•œ mutual information í‘œê¸°ë¥¼ ;ì´ ì•„ë‹Œ ,ë¡œ ì‚¬ìš©í•¨</p> </li> <li> <table> <tbody> <tr> <td>$I(X;Y):=\sum_{x,y}p(x,y)[I(x)-I(x</td> <td>y)]=\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$</td> </tr> </tbody> </table> <p>(ìœ„ ì‹ì˜ ì²« ë“±ì‹ì— ë‚˜ì˜¤ëŠ” $p(x,y)$ëŠ” $x=x_i,y=y_j$ì— ëŒ€í•œ $p(x_i,y_j)$ì„. $I(x_i;y_j)=I(x_i)-I(x_i|y_j)$) â€” for event</p> <p>Y=yì„ì„ ì•Œê²Œ ë¨ìœ¼ë¡œì¨ ì¤„ì–´ë“œëŠ” xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„(ì •ë³´ëŸ‰)</p> <p>ìƒí˜¸ ì •ë³´ëŸ‰ì´ ë†’ìœ¼ë©´ Yë¥¼ ì•Œ ë•Œ Xë¥¼ ì•Œ í™•ë¥ ì´ ë†’ì•„ì§</p> <table> <tbody> <tr> <td>$I(x)-I(x</td> <td>y) = \log\frac{1}{p(x)}-\log\frac{1}{p(x</td> <td>y)} = \log\frac{p(x</td> <td>y)}{p(x)} = \log\frac{p(x,y)}{p(x)p(y)}$</td> </tr> </tbody> </table> <p>$I(X;Y)=I(Y;X)$</p> </li> <li> <table> <tbody> <tr> <td>$I(X;Y)= H(X)-H(X</td> <td>Y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>pf) $I(X;Y)=\sum_{x,y} p(x,y)\log\frac{p(x,y)}{p(x)p(y)} \ \qquad\qquad\quad = \sum_{x,y} p(x,y)\log \frac{1}{p(x)} - \sum_{x,y}p(x,y)\log\frac{p(y)}{p(x,y)} \ \qquad\qquad\quad = \sum_x p(x) \log\frac{1}{p(x)} - \sum_{x,y}p(x,y)\log\frac{1}{p(x</td> <td>y)} \ \qquad\qquad\quad = H(X)-H(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>ë„¤ ë²ˆì§¸ ë“±í˜¸ ë„˜ì–´ê°ˆ ë•Œ ì•„ë˜ ì°¸ê³ </p> <table> <tbody> <tr> <td>$H(X</td> <td>Y)=\sum_y p(y)H(X</td> <td>Y=y) \ \qquad\qquad = \sum_y p(y)\sum_x p(x</td> <td>y)\log\frac{1}{p(x</td> <td>y)} \ \qquad\qquad = \sum_{x,y}p(x</td> <td>y)p(y)\log\frac{1}{p(x</td> <td>y)} = \sum_{x,y} p(x,y)\log\frac{1}{p(x</td> <td>y)}$</td> </tr> </tbody> </table> <ul> <li> <p>ë³€í˜•</p> <table> <tbody> <tr> <td>$H(X)=H(X</td> <td>Y)+I(X;Y)$ ; Xì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì€ Yë¥¼ ì•Œ ë•Œ Xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„ì™€ Yë¥¼ ì•Œ ë•Œ Xë¥¼ ì•„ëŠ” ì •ë„ì˜ í•©</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <p>ì¢‹ì€ ì±„ë„ = ìƒí˜¸ ì •ë³´ëŸ‰ì´ ë†’ì€ ì±„ë„</p> <ul> <li>Ex 1 <ol> <li> <table> <tbody> <tr> <td>independent : $p(x</td> <td>y)=p(x) \Rightarrow I(x;y)=0$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>fully dependent : $p(x</td> <td>y)=1 \Rightarrow I(x;y)=I(x)$</td> </tr> </tbody> </table> </li> </ol> </li> <li> <p>Ex 2 (<em>binary symmetric channel</em> )</p> <p>$p$ : error rate</p> <p>(case 1)</p> <table> <thead> <tr> <th>X</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>P(X)</td> <td>1</td> <td>0</td> </tr> </tbody> </table> <p>â†’</p> <table> <thead> <tr> <th>Y</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>P(Y)</td> <td>1-p</td> <td>p</td> </tr> </tbody> </table> <p>(case 2)</p> <table> <thead> <tr> <th>X</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>P(X)</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <p>â†’</p> <table> <thead> <tr> <th>Y</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>P(Y)</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_183918.png" alt="ìŠ¤í¬ë¦°ìƒ· 2023-09-21 183918.png"></p> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_184010.png" alt="ìŠ¤í¬ë¦°ìƒ· 2023-09-21 184010.png"></p> <table> <tbody> <tr> <td>$P(Y=0)=P(Y=0</td> <td>X=0)P(X=0)+P(Y=0</td> <td>X=1)P(X=1)\ \qquad\qquad\ \ =(1-p)(1/2)+p(1/2)=1/2$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$P(Y=1)=P(Y=1</td> <td>X=0)P(X=0)+P(Y=1</td> <td>X=1)P(X=1)\ \qquad\qquad\ \ =p(1/2)+(1-p)(1/2)=1/2$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X=0;Y=0)=\log\frac{p(x</td> <td>y)}{p(x)}=\log\frac{p(y</td> <td>x)}{p(y)}=\log\frac{1-p}{1/2}=\log 2(1-p)=\log(1-p)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X=0;Y=1)=\log\frac{p(x</td> <td>y)}{p(x)}=\log\frac{p(y</td> <td>x)}{p(y)}=\log\frac{p}{1/2}=\log 2p=\log p$</td> </tr> </tbody> </table> <p>â†’ $pâ†’0$ì¼ ë•Œ $\log p\rightarrow -\infin$ ; ê°œë³„ ìƒí˜¸ ì •ë³´ëŸ‰ì€ ìŒìˆ˜ê°€ ë‚˜ì˜¤ëŠ” ê²½ìš°ë„ ìˆìœ¼ë‚˜, í‰ê· ì„ êµ¬í•˜ë©´ 0ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ë³´ì •ë¨</p> </li> </ul> <h4 id="conditional-mutual-information">Conditional Mutual Information</h4> <table> <tbody> <tr> <td>$I(X;Y\</td> <td>\ Z)=H(X</td> <td>Z)-H(X\,</td> <td>\,\ Y,Z)$</td> </tr> </tbody> </table> <ul> <li> <p><strong>Theorem (Chain rule for mutual information)</strong> $I(X_1,\ldots, X_n;Y)=\sum_{i=1}^n I(X_i;Y\ |\ X_{i-1},\ldots,X_1)$</p> <table> <tbody> <tr> <td>pf) $I(X_1,\ldots,X_n;Y) \ \qquad =H(X_1,\ldots,X_n)-H(X_1,\ldots,X_n\</td> <td>\ Y) \ \qquad = \sum_{i=1}^n H(X_i\</td> <td>\ X_{i-1},\ldots,X_1) -\sum_{i=1}^n H(X_i\</td> <td>\ X_{i-1},\ldots,X_1,Y) \ \qquad = \sum_{i=1}^n [H(X_i\</td> <td>\ X_{i-1},\ldots,X_1) - H(X_i\</td> <td>\ {\color{red}Y},X_{i-1},\ldots,X_1)] \ \qquad = \sum_{i=1}^n H(X_i;Y\</td> <td>\ X_{i-1},\ldots,X_1)$</td> </tr> </tbody> </table> </li> </ul> <h3 id="entropy">Entropy</h3> <p>The Shannon entropy of R.V. $X\sim p(x)$ is defined by</p> <p>$H(X):=-\sum_x p(x)\log p(x)$ ; $\log\frac{1}{p(x)}$ì˜ ê¸°ëŒ€ê°’(í‰ê· )</p> <ul> <li> <p>í™•ë¥  ë³€ìˆ˜ì˜ (í™•ë¥  ë¶„í¬ì˜) í‰ê·  information</p> <p>$H(X):=\mathbb{E}[I(X)]$</p> <ul> <li>ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥  ë³€ìˆ˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë¹„íŠ¸ ìˆ˜</li> <li>ë°ì´í„°ë¥¼ ìµœëŒ€ë¡œ ì••ì¶• ë•Œì˜ í¬ê¸° (ë°”ë¥¸ ë³µì›ì„ ë³´ì¥í•  ìˆ˜ ìˆì–´ì•¼ í•¨)</li> </ul> </li> <li> <p>Example 1.1.2 (<em>xì˜ í™•ë¥ ì´ uniformí•˜ì§€ ì•Šì€ ê²½ìš° code wordì˜ ê¸¸ì´ê°€ ì¤„ì–´ë“œëŠ” ìƒí™©</em> )</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%202.png" alt="example"></p> <table> <thead> <tr> <th>$x_i$</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>5</th> <th>6</th> <th>7</th> <th>8</th> </tr> </thead> <tbody> <tr> <td>$p(x_i)$</td> <td>1/2</td> <td>1/4</td> <td>1/8</td> <td>1/16</td> <td>1/64</td> <td>1/64</td> <td>1/64</td> <td>1/64</td> </tr> <tr> <td>code</td> <td>0</td> <td>10</td> <td>110</td> <td>1110</td> <td>111100</td> <td>111101</td> <td>111110</td> <td>111111</td> </tr> </tbody> </table> <p>í‰ê·  ì½”ë“œ ê¸¸ì´ = entropy ; $\mathbb{E}[\textrm{code len}] = \mathbb{E}[f(X)]$</p> </li> <li> <p>Example 1.1.3</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>$p$</td> <td>$1-p$</td> </tr> </tbody> </table> <p>$H(X)=H(p)$</p> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_184032.png" alt="1-bit Shannon entropy"></p> </li> </ul> <h4 id="joint-entropy">Joint entropy</h4> <p>R.V. $Z:= (X,Y) \ \sim\ p(z)=p(x,y):=P[X=x \textrm{ and } Y=y]$</p> <p>$H(Z) = H(X,Y) =\sum_x\sum_y p(x,y)\log\frac{1}{p(x,y)}=\mathbb{E}_{x,y}[-\log(X,Y)]$</p> <h4 id="conditional-entropy">Conditional entropy</h4> <table> <tbody> <tr> <td>$H(Y</td> <td>X)$ ; $X=x_i$ì˜ $x_i$ê°€ ë³€í™”í•  ë•Œ $Y$ì˜ ì—”íŠ¸ë¡œí”¼</td> </tr> </tbody> </table> <p>ğŸ’¡ Shannon entropy : ì •ë³´ëŸ‰ì˜ í‰ê· </p> <table> <tbody> <tr> <td>$H(Y</td> <td>X) := \sum P(X=x_i)H(Y</td> <td>X=x_i)$</td> </tr> </tbody> </table> <h4 id="mutual-entropy">Mutual entropy</h4> <ul> <li> <p>$H(X;Y):=\sum_x\sum_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$</p> <p>Y=yì„ì„ ì•Œê²Œ ë¨ìœ¼ë¡œì¨ ì¤„ì–´ë“œëŠ” xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„(ì •ë³´ëŸ‰)</p> <table> <tbody> <tr> <td>$I(x)-I(x</td> <td>y) = \log\frac{1}{p(x)}-\log\frac{1}{p(x</td> <td>y)} = \log\frac{p(x</td> <td>y)}{p(x)} = \log\frac{p(x,y)}{p(x)p(y)}$</td> </tr> </tbody> </table> </li> </ul> <h4 id="theorem-chain-rule">Theorem (Chain Rule)</h4> <table> <tbody> <tr> <td>$H(X,Y)=H(Y)+H(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>joint dist (X,Y)ì˜ ë¶ˆí™•ì‹¤ì„± = Yì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± + Yë¥¼ ì•Œ ë•Œ Xì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±</p> <ul> <li> <p>ì¦ëª…</p> <table> <tbody> <tr> <td>$H(X,Y) \ =-\sum_{x,y}\log p(x,y) \ = -\sum_{x,y} p(x,y)\log p(x)p(y</td> <td>x) \ = -\sum_{x,y} p(x,y)\log p(x) - \sum_{x,y} p(x,y)\log p(y</td> <td>x) \ = -\sum_x p(x)\log p(x) - \sum_{x,y} p(x,y)\log p(y</td> <td>x) \ = H(X)+H(Y</td> <td>X)$</td> </tr> </tbody> </table> </li> </ul> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%203.png" alt="conditional entropy"></p> <ul> <li> <p><strong>Corollary (Diagram about Entropy &amp; M.I.)</strong></p> <table> <tbody> <tr> <td>$H(X,Y\,</td> <td>\,Z)=H(X</td> <td>Z)+H(X\,</td> <td>\,Y,Z)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X;Y)=H(X)-H(X</td> <td>Y)\ =I(Y;X)=H(Y)-H(Y</td> <td>X)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$H(X,Y)=H(X)+H(Y</td> <td>X)\ =H(Y,X)=H(Y)+H(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>$I(X;X)=H(X)$</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%204.png" alt="entropy diagram"></p> </li> </ul> <p><strong>Chain rule for entropy</strong></p> <p>$X_1, X_2, \ldots, X_n$ are R.V.s $\sim\ p(x_1, \ldots, x_n)$</p> <table> <tbody> <tr> <td>$H(X_1, \ldots, X_n)=\sum_{i=1}^n H(X_i</td> <td>X{i-1},\ldots, X_1)$</td> </tr> </tbody> </table> <ul> <li> <p>pf 1</p> <table> <tbody> <tr> <td>$H(X_1,X_2)=H(X_1)+H(X_2</td> <td>X_1)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$H(X_1,X_2,X_3)=H(X_1)+H(X_2,X_3</td> <td>X_1)\ \qquad\qquad\qquad\ \ = H(X_1) + H(X_2</td> <td>X_1) + H(X_3</td> <td>X_2,X_1)$ (by corollary)</td> </tr> </tbody> </table> <p>â€¦</p> <table> <tbody> <tr> <td>$H(X_1,\ldots,X_n)\ \quad =H(X_1)+H(X_2,\ldots,X_n</td> <td>X_1) \ \quad = H(X_1)+H(X_2</td> <td>X_1)+ H(X_3</td> <td>X_2,X_1) + \cdots + H(X_n</td> <td>X_{n-1},\ldots,X_1) \ \quad = \sum_{i=1}^n H(X_i</td> <td>X_{i-1},\ldots,X_1)$</td> </tr> </tbody> </table> </li> <li> <p>pf 2</p> <table> <tbody> <tr> <td>$p(x_1,x_2) = p(x_1)p(x_2</td> <td>x_1)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$p(x_1,x_2,x_3)=p(x_1,x_2)p(x_3</td> <td>x_1,x_2)=p(x_1)p(x_2</td> <td>x_1)p(x_3</td> <td>x_1,x_2)$</td> </tr> </tbody> </table> <p>â€¦</p> <table> <tbody> <tr> <td>$p(x_1,x_2,\ldots,x_n)=\prod_{i=1}^n p(x_i</td> <td>x_{i-1},\ldots,x_1)$</td> </tr> </tbody> </table> <p>Then,</p> <table> <tbody> <tr> <td>$H(X_1,\ldots,X_n)=-\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\log p(x_1,\ldots,x_n) \ \qquad\qquad\qquad\ \ \ = -\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\log \prod_{i=1}^n p(x_i</td> <td>x_{i-1},\ldots,x_1) \ \qquad\qquad\qquad\ \ \ = -\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\sum_{i=1}^n \log p(x_i</td> <td>x_{i-1},\ldots,x_1) \ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n) \log p(x_i</td> <td>x_{i-1},\ldots,x_1) \ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n\sum_{x_1,\ldots,{\color{red} x_i}}p(x_1,\ldots,{\color{red} x_i}) \log p(x_i</td> <td>x_{i-1},\ldots,x_1) \ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n H(X_i</td> <td>X_{i-1},\ldots,X_1)$</td> </tr> </tbody> </table> </li> </ul> <h4 id="relative-entropy">Relative entropy</h4> <ul> <li> <p>$D(p\parallel q) = \sum_i p(x_i)\log\frac{p(x_i)}{q(x_i)}$ â€” pì™€ qì˜ ì°¨ì´</p> <p>$\neq D(q\parallel p)$ (in general)</p> <ul> <li>A measure of the distance between two distributions $p$ and $q$</li> <li>$p$ê°€ ì¤‘ì‹¬ì´ ë˜ì–´ ë°”ë¼ë³¼ ë•Œ $q$ì˜ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ê°€</li> </ul> <p>R.V. distribution</p> <ul> <li>$X\sim p \; \wedge \; \widetilde{X}\sim q$</li> <li>$\chi={ x_1, \ldots, x_n }$ (the same sample space) â‡’ ì„œë¡œ ë‹¤ë¥¸ sample spaceì—ì„œ ì •ì˜ëœ ë³€ìˆ˜ëŠ” ë¹„êµ ëŒ€ìƒì´ ë  ìˆ˜ ì—†ìŒ</li> </ul> <p>$q(x_i):=P(\widetilde{X}=x_i)$</p> <p>$p(x_i):=P({X}=x_i)$</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%205.png" alt="prob dist."></p> </li> <li> <p>$\sum p(x)\log\frac{1}{p(x)} + \sum p(x)\log\frac{p(x)}{q(x)}=\sum p(x)\log\frac{1}{q(x)}$</p> <p>$p$ : true distribution (unknown) $\wedge$ $q$ : approximation (guessing) of $p$</p> <p>If we can construct a â€œgood codeâ€ for $X\sim p$, then the average cod length = $H(p)$</p> <p>Instead, we use (of mis-use) a code for $X\sim q$, then the average code length = $\sum p(x)\log\frac{1}{q(x)}$ &gt; $H(p)$</p> <p>â‡’ We need $H(p)+D(p\parallel q)$ bits on the average.</p> <p>í˜„ì‹¤ì—ì„œ code wordë¥¼ ë¶€ì—¬í•˜ì—¬ ì½”ë”©í•  ë•Œ í•„ìš”í•œ í‰ê·  ë¹„íŠ¸ ìˆ˜ëŠ” ìµœì†Œ ìš”êµ¬ ë¹„íŠ¸ ìˆ˜ + ì˜ëª» ë¶€ì—¬í•¨ì— ì˜í•œ ì¶”ê°€ ë¹„íŠ¸ ìˆ˜</p> </li> <li> <p>ex. Code word</p> <table> <thead> <tr> <th>$x_i$</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>5</th> <th>6</th> <th>7</th> <th>8</th> </tr> </thead> <tbody> <tr> <td>$p(x_i)$</td> <td>1/2</td> <td>1/4</td> <td>1/8</td> <td>1/16</td> <td>1/64</td> <td>1/64</td> <td>1/64</td> <td>1/64</td> </tr> <tr> <td>code</td> <td>0</td> <td>10</td> <td>110</td> <td>1110</td> <td>111100</td> <td>111101</td> <td>111110</td> <td>111111</td> </tr> </tbody> </table> <p>sequence of R.V.s: $X_1, X_2,\ldots,X_n \textrm{ where } X_i\sim P(X)$</p> <p>â‡’ ë³€ìˆ˜ì˜ ì ˆë°˜ì€ 1, ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ 2, â€¦ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê°’ì´ ë‚˜ì˜¬ ê²ƒì„ ì˜ˆìƒí•  ìˆ˜ ìˆìŒ</p> <p>ì£¼ì–´ì§„ ìƒí™©ì—ì„œ $H(X) = - \sum p(x)\log p(x)$ì´ê³ , ì—”íŠ¸ë¡œí”¼ëŠ” eventì— ëŒ€í•œ informationì˜ í‰ê· ì´ë©° ë™ì‹œì— ìµœì í™”ëœ (minimized) code lengthì˜ í‰ê· ì„</p> <p>ê²°êµ­ informationì´ì code lengthì¸ $\log\frac{1}{p(x)}$ê°€ ë³€ìˆ˜ë¥¼ ë½‘ì„ ë•Œë§ˆë‹¤ ê³„ì‚°ë¨ â‡’ $X_1,\ldots,X_n$ ì¤‘ ì ˆë°˜ì€ code lengthê°€ 1, ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ 2, â€¦ ê°€ ë¨ $H(X)=2$ì´ë¯€ë¡œ $X_1$ë¶€í„° $X_n$ê¹Œì§€ í‘œí˜„í•˜ê¸° ìœ„í•œ ë¹„íŠ¸ëŠ” í‰ê· ì ìœ¼ë¡œ $2\cdot n$ bitsê°€ í•„ìš”í•¨</p> <p>R.V.ì˜ sequenceë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©´ ì´ sequenceë¥¼ ì–´ëŠ ì •ë„ì˜ ê¸¸ì´ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ê°€ëŠ” ë¶„í¬ì— ì˜í•´ ì •í•´ì§€ê³ , ìµœì í™”ëœ code lengthì¸ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒì´ Shannonâ€™s entropy ê°’ì„</p> </li> <li> <p>ì„¤ëª…</p> <p>known $X_1, X_2,\ldots,X_n \sim p(x) \Rightarrow \log\frac{1}{p(x)}$; code length $\Rightarrow \sum p(x)\log{1}{p(x)}$</p> <p>í™•ë¥  ë¶„í¬ë¥¼ ì•Œë©´ ìµœì í™”ëœ code wordì˜ ê¸¸ì´ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ</p> <p>ë°˜ëŒ€ë¡œ fixed $p(x)$ë¥¼ ëª¨ë¥´ëŠ” ê²½ìš°, $q(x)$ë¡œì¨ code wordì˜ ê¸¸ì´ë¥¼ guessingí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•¨</p> <p>$q(x)\Rightarrow \log\frac{1}{q(x)}$</p> <p>â‡’ ìµœì í™” code word ê¸¸ì´ = ì‹¤ì œ ë‚˜ì˜¬ í™•ë¥  $\times$ ë¶€ì—¬í•œ ì½”ë“œ ê¸¸ì´ $\Rightarrow\sum q(x)\log\frac{1}{q(x)}$</p> <p>$p(x)$ë¥¼ ëª¨ë¥´ëŠ” ì±„ $X_1, X_2,\ldots,X_n$ì„ ë½‘ì•„ì„œ average lengthë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŒ</p> <p>ì‹¤ì œë¡œ êµ¬í•´ì„œ ê³„ì‚°í•˜ê²Œ ë˜ëŠ” ê°’ì€ $\sum p(x)\log\frac{1}{p(x)}$ì´ë¯€ë¡œ ë‘ ê°’ì˜ ì°¨ì´ë¥¼ ì•Œ ìˆ˜ ìˆìŒ</p> <p>$D(p\parallel q) :=\sum p(x)\log\frac{1}{q(x)}-\sum p(x)\log\frac{1}{p(x)}=\sum p(x)\log\frac{p(x)}{q(x)}$</p> </li> <li> <p>Mutual informationê³¼ì˜ ê´€ê³„</p> <p><strong>Mutual inforamtion</strong></p> <table> <tbody> <tr> <td>$I(x_i\,;y_j)=I(x_i)-I(x_i</td> <td>y_j)=\log\frac{1}{p(x_i)}-\log\frac{1}{p(x_i</td> <td>y_i)}=\log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X;Y)\;=\sum p(x_i,y_j)I(x_i\,;y_j)\ \qquad\qquad =\sum p(x_i,y_j)\log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}=D(p(x,y)\parallel p(x)p(y))$ â€” joint</td> <td>Â </td> <td>product</td> </tr> </tbody> </table> <p>$\therefore I(X;Y)=0 \Leftrightarrow p(x,y)=p(x)p(y) \textrm{ for } \forall x,y$ â€” independent</p> <p>â‡’ joint ë¶„í¬ì™€ productì˜ ë¶„í¬ì˜ ì°¨ì´ê°€ 0</p> </li> </ul> <p>ğŸ’¡ $0\log\frac{0}{0} \rightarrow 0$ $0\log\frac{0}{g}\rightarrow 0$ $p\log\frac{p}{0}\rightarrow\infin$</p> <p>define $p(x), q(x)$ for $\exist \, x\in\chi$ s.t. $p(x)&gt;0, q(X)=0$ then $D(p\parallel q)=\infin$</p> <ul> <li> <p>ex. $D(p\parallel q) \neq D(q\parallel q)$</p> <p>$\chi={0,1}$</p> <p>$H(p)=-\sum p(x)\log p(x) \ \qquad\;\;= -(1-r)\log(1-r) \ \qquad\qquad-r\log r$</p> <p>$H(p)\leftarrow$</p> <p>$H(q)\leftarrow$</p> <p>$H(q) = -(1-s)\log(1-s)=s\log s$</p> <table> <thead> <tr> <th>x</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>p(x)</td> <td>1-r</td> <td>r</td> </tr> <tr> <td>q(x)</td> <td>1-s</td> <td>s</td> </tr> </tbody> </table> <p>For $r=\frac{1}{2},s=\frac{1}{4}$,</p> <p>$D(p\parallel q)=\sum p(X)\log\frac{p(x)}{q(x)}=\frac{1}{2}\log\frac{1/2}{3/4}+\frac{1}{2}\log\frac{1/2}{1/4}\approx 0.2075$</p> <p>and $D(q\parallel q)=\frac{3}{4}\log\frac{3/4}{1/2}+\frac{1}{4}\log\frac{1/4}{1/2}\approx 0.1887$</p> </li> </ul> <h4 id="jensens-inequality">Jensenâ€™s inequality</h4> <p>convex ë˜ëŠ” concaveë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œëŠ” ì¡°ê±´ì„ ë§Œì¡±í•˜ë„ë¡ êµ¬ê°„ì„ ì„¤ì •í•´ ì£¼ì–´ì•¼ í•¨</p> <p><strong>Def: Convex</strong></p> <p>$f:(a,b)\rightarrow\mathbb{R}$ is convex if for every $x_1, x_2\in (a,b),$</p> <p>$f(\lambda x_1+(1-\lambda)x_2)\le \lambda f(x_1)+(1-\lambda)f(x_2)$ where $0\le \forall\lambda \le 1$</p> <p>(êµ¬ê°„ ë‚´ x=aì¼ ë•Œ, ê³¡ì„  ìœ„ point â‰¤ êµ¬ê°„ ë‚´ x=aì¼ ë•Œ, ì§ì„  ìœ„ì˜ point)</p> <p><strong>NOTE:</strong> $\lambda x_1 + (1-\lambda)x_2 \Rightarrow$ $x_1$,$x_2$ë¥¼ $\lambda:1-\lambda$ë¡œ ë‚´ë¶„í•˜ëŠ” ì </p> <p><strong>Def: Concave</strong></p> <p>$f:(a,b)\rightarrow\mathbb{R}$ is concave if $-f$ is convex</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%206.png" alt="convex"></p> <p>convex</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%207.png" alt="concave"></p> <p>concave</p> <ul> <li> <p><strong>Theorem</strong> $f\in C^2$ ($fâ€™,fâ€™â€™$ exist and are continuous) and $fâ€™â€˜(x)\ge0$ on $(a,b)$, then $f$ is convex</p> <p>pf: $f(x)=f(x_0)+fâ€™(x_0)(x-x_0)+(fâ€™â€˜(x^<em>)/2)(x-x_0)^2,\quad x^</em>\in[x_0,x]$</p> <p>$(fâ€™â€˜(x^*)/2)(x-x_0)^2$ì´ $y=ax^2+bx+c,a&gt;0$ ê¼´ë¡œ convex</p> <p>$x_1, x_2$ : arbitrarily given points</p> <p>Let $x_0:=\lambda x_1 + (1-\lambda)x_2\;\; (0\le\lambda\le1)$</p> <p>$f(x_1)=f(x_0)+fâ€™(x_0)(x_1-x_0)$ $\qquad\qquad+(fâ€™â€˜(x_1^*)/2)(x_1-x_0)^2$</p> <p>($x_1^*$ lines between $x_0, x_1$)</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%208.png" alt="Jenson"></p> <p>$f(x_2)=f(x_0)+fâ€™(x_0)(x_2-x_0)+(fâ€™â€˜(x_2^*)/2)(x_2-x_0)^2$</p> <p>($x_2^*$ lines between $x_0, x_1$)</p> <p>Since $fâ€™â€˜(x_1^<em>)\ge0,fâ€™â€˜(x_2^</em>)\ge0$,</p> <ol> <li>$f(x_1)\ge f(x_0)+fâ€™(x_0)(x_1-x_0)$</li> <li>$f(x_2)\ge f(x_0)+fâ€™(x_0)(x_2-x_0)$</li> </ol> <p>$\lambda f(x_1)+(1-\lambda)f(x_2)\ \quad \ge \lambda f(x_0)+\lambda fâ€™(x_0)(x_1-x_0) +(1-\lambda)f(x_2)+(1-\lambda)fâ€™(x_0)(x_2-x_0)\ \quad = f(x_0)fâ€™(x_0)[\lambda x_1 -\lambda x_0+(1-\lambda)x_2 -(1-\lambda)x_0]\ \quad = f(x_0) + fâ€™(x_0)[\lambda x_1+(1-\lambda)x_2-x_0] \ \quad =f(x_0)+fâ€™(x_0)[x_0-x_0] = f(x_0)\ \quad = f(\lambda x_1 + (1-\lambda)x_2)$</p> <p>$\therefore f$ is convex</p> </li> <li> <p><strong>Thm: Jensenâ€™s inequality</strong> $f$ : convex and $X$ : R.V. then $\mathbb{E}[f(X)]\ge f(\mathbb{E}[X])$</p> <p>pf: We use mathematical induction.</p> <table> <tbody> <tr> <td>Let $\chi_2 ={ x_1,x_2 },\ldots,\chi_k={ x_1,x_2,\ldots,x_n} \textrm{ where }</td> <td>\chi_i</td> <td>=i \textrm{ for all } 2\le i \le k$</td> </tr> </tbody> </table> <p>case 1) $\chi = {x_1, x_2}$</p> <p>$\;\mathbb{E}[f(X)] = p_1 f(x_1) + p_2 f(x_2)$</p> <p>$\qquad\qquad = p_1f(x_1)+(1-p_1)f(x_2)$</p> <table> <thead> <tr> <th>$x$</th> <th>$x_1$</th> <th>$x_2$</th> </tr> </thead> <tbody> <tr> <td>$p(x)$</td> <td>$p_1$</td> <td>$p_2$</td> </tr> </tbody> </table> <p>$\qquad\qquad\quad\; \ge f(p_1x_1 + (1-p_1)x_2)$ since $f$ is convex</p> <p>$\qquad\qquad =f(\mathbb{E}[X])$</p> <table> <tbody> <tr> <td>case 2) Suppose that Theorem is true for $</td> <td>\chi</td> <td>\le k-1$</td> </tr> </tbody> </table> <p>$X\sim{$ $\chi:={x_1,\ldots,x_k}$ and $p_i:=P[X=x_i] \;\ (i=1,\ldots,k)\ }$ are given.</p> <p>Define a distribution $Xâ€™$ with $\chi_{k-1}:={x_1,\ldots,{\color{red}x_{k-1}}}$ where $p_iâ€™:=P(X=x_i)=\frac{p_i}{1-p_k} \;\ (i=1,\ldots,k-1)$</p> <p>$\,\mathbb{E}[f(X)]=\sum p_i f(x_i) = p_k f(x_k)+\sum_{i=1}^{k-1}p_i f(x_i)$</p> <p>$\qquad\qquad=p_k f(x_k)+(1-p_k)\sum_{i=1}^{k-1}\frac{p_i}{1-p_k}f(x_i)$</p> <p>$\qquad\qquad=p_kf(x_k)+(1-p_k)\sum_{i=1}^{k-1}p_iâ€™ f(x_i)$</p> <p>$\qquad\qquad=p_k f(x_k) + (1-p_k)\mathbb{E}[f(Xâ€™)]$</p> <table> <tbody> <tr> <td>$\qquad\qquad\ge p_k f(x_k)+(1-p_k)f\left(\sum_{i=1}^{k-1} p_iâ€™ x_i\right)$ for $</td> <td>\chi</td> <td>\le k-1$</td> </tr> </tbody> </table> <p>$\qquad\qquad\ge f\left(p_k x_k + (1-p_k)\sum_{i=1}^{k-1} p_iâ€™ x_i\right)$ by $f$ : convex</p> <p>$\qquad\qquad=f\left(p_k x_k + (1-p_k)\sum_{i=1}^{k-1}\frac{p_i}{1-p_k}x_i\right)$</p> <p>$\qquad\qquad=f\left(\sum_{i=1}^k p_i x_i\right)$</p> <p>$\qquad\qquad=f(\mathbb{E}[X])$</p> </li> <li> <p>Theorem: Information inequality $D(p\parallel q)\ge 0$ , $I(X;Y)\ge0$ , $H(X)\le\log|\chi|$</p> <p>$p(x), q(x)$ are p.m.f. (probability mass function) on $x\in\chi$</p> <ol> <li>$D(p\parallel q)\ge 0$ (equality holds $\iff$ $p(x)\equiv q(x)$ <ul> <li> <p>pf</p> <p>$supp(f)$ : support of $f$</p> <table> <tbody> <tr> <td>$supp(f) := \overline{{x</td> <td>f(x)&gt;0}}={x</td> <td>f(x)&gt;0}$ where $x$ is discrete</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Let $A:={x</td> <td>p(x)&gt;0}$ (i.e. $A:= supp(p)$)</td> </tr> </tbody> </table> <p>$\;-D(p\parallel q) = -\sum_{x\in\chi} p(x) \log\frac{p(x)}{q(x)} = -\sum_{x\in A} p(x)\log\frac{p(x)}{q(x)}$</p> <p>$\qquad\qquad\quad= \sum_{x\in a} p(x) \log\frac{q(x)}{p(x)}\;\;\cdots\;\; \mathbb{E}\left[\log\frac{q(X)}{p(X)}\right]$</p> <p>$\qquad\qquad\quad\le\log\left[ \sum_{x\in A} p(x)\frac{q(x)}{p(x)} \right]$ by Jensenâ€™s inequality</p> <p>$\qquad\qquad\quad=\log\left[ \sum_{x\in A} q(x) \right]$</p> <p>$\qquad\qquad\quad\le\log\left[\sum_{x\in\chi} q(x)\right]=\log 1=0$</p> <p>$\therefore\ D(p\parallel q)\ge0$</p> </li> </ul> </li> <li>$I(X;Y)\ge0$ (equality holds $\iff$ $X,Y$ are independent <ul> <li> <p>pf</p> <p>$I(X;Y)=D\left(p(x,y)\parallel p(x)q(y)\right)\ge0$</p> </li> </ul> </li> <li> <table> <tbody> <tr> <td>$X$: R.V. $x\in\chi$, $H(X)\le\log</td> <td>\chi</td> <td>$ (entropyëŠ” distributionì´ uniformì¼ ë•Œ ìµœëŒ€)</td> </tr> </tbody> </table> <ul> <li> <p>pf</p> <table> <tbody> <tr> <td>Let $u(x):=\frac{1}{</td> <td>\chi</td> <td>}$ (constant function on $\chi$)</td> </tr> </tbody> </table> <p>$u(x)$ is uniform p.m.f. over $\chi$.</p> <p>Then for any R.V. $X\sim P$</p> <p>$D(p\parallel q)=\sum p(x)\log\frac{p(x)}{u(x)}$</p> <table> <tbody> <tr> <td>$\qquad\qquad=\sum p(x)\log</td> <td>\chi</td> <td>+\sum p(x)\log p(x)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\qquad\qquad=\log</td> <td>\chi</td> <td>-\sum p(x)\log\frac{1}{p(x)}$ since $\log</td> <td>\chi</td> <td>$ is constant and $\sum p(x)=1$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\qquad\qquad=\log</td> <td>\chi</td> <td>-H(X)\ge0$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\therefore H(X)\le\log</td> <td>\chi</td> <td>$</td> </tr> </tbody> </table> <p>â‡’ entropy of $X$ is smaller than Entropy of uniform disribution</p> </li> </ul> </li> </ol> </li> <li> <p>Theorem: Conditioning reduces entropy $H(X|Y)\le H(X)$</p> <table> <tbody> <tr> <td>pf: $0\le I(X;Y)=H(X)-H(X</td> <td>Y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\therefore\ H(X)\ge H(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>ex. $H(X) = H\left( \frac{1}{8},\frac{7}{8} \right)$ or</p> <p>$\qquad\quad = H(p) \textrm{\,\ where\,\ } p=\frac{1}{8}, q=1-p$</p> <table> <tbody> <tr> <td>$H(X</td> <td>Y=2)=H\left(\frac{1}{2},\frac{1}{2}\right)=1$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\therefore H(X)\ll H(X</td> <td>Y=2)$</td> </tr> </tbody> </table> <p>â‡’ ì •ë¦¬ì— ëª¨ìˆœì¸ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ,</p> <p>ì •ë¦¬ëŠ” averageì— ëŒ€í•œ ë‚´ìš©ì´ê³ </p> <p>ì˜ˆì œëŠ” single term $Y=2$ë¼ëŠ”</p> <p>íŠ¹ì • ìƒí™©ì— ëŒ€í•œ ê²°ê³¼ì„</p> <table> <thead> <tr> <th>$Y$ \ $X$</th> <th>1</th> <th>2</th> <th>$P(Y)$</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0</td> <td>3/4</td> <td>3/4</td> </tr> <tr> <td>2</td> <td>1/8</td> <td>1/8</td> <td>1/4</td> </tr> <tr> <td>$P(X)$</td> <td>1/8</td> <td>7/8</td> <td>Â </td> </tr> </tbody> </table> </li> <li> <p>Theorem: independence bound $H(X_1,\ldots, X_n)\le\sum_{i=1}^n H(X_i)$ (equality â‡” independence)</p> <p>pf: By the Chain rule,</p> <table> <tbody> <tr> <td>$\;H(X_1,\ldots,X_n)=\sum_{i=1}^n H(X_i</td> <td>X_{i-1},\ldots,X_1)$</td> </tr> </tbody> </table> <p>$\qquad\qquad\qquad\quad\le \sum_{i=1}^n H(X_i)$ by previous theorems</p> </li> </ul> <h2 id="log-sum-inequality-and-its-applications">Log-sum inequality and its Applications</h2> <h3 id="theorem-log-sum-inequality">Theorem: Log-sum inequality</h3> <ul> <li> <p>For non-negative numbers $a_1,\ldots,a_n$ and $b_1,\ldots, b_n$, $\sum_{i=1}^n a_i\left(\log\frac{a_i}{b_i}\right) \ge \left(\sum_{i=1}n a_i\right)\cdot\log\left( \sum_{i=1}^n a_i/\sum_{i=1}^n b_i \right)$ (equality â‡” $\frac{a_i}{b_i}$ is constant)</p> <p>pf: case 1) For $a_i\ge0, b_i&gt;0$.</p> <p>Let $f(t):= t\log t$, then $f(t)$ is convex.</p> <p>ğŸ’¡ $t\times a&gt;t\textrm{\;\; as\;\;} t\rightarrow\infin \textrm{\;\; where\;\;} a&gt;1$</p> <p>Since $(\log t)â€™=\left( \frac{\log_e t}{\log_e 2} \right)â€™=\frac{1}{\log_e 2}\cdot\frac{1}{t}$,</p> <p>$fâ€™(t)=\log t+\log e$</p> <p>$fâ€™â€˜(t)=\frac{1}{\log_e 2}\cdot\frac{1}{t}=\frac{\log e}{t} &gt; \frac{1}{t}&gt;0$</p> <p>$\therefore\ fâ€™â€˜(t)&gt;0 \;\Rightarrow \;f$ is convex</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%209.png" alt="Graph shape"></p> <p>By Jensenâ€™s inequality $\mathbb{E}[f(X)] \ge f(\mathbb{E}[X])$ for convex $f$,</p> <p>$\sum \alpha_i f(t_i) \ge f\left(\sum \alpha_i t_i\right)$</p> <p>for $X\leftarrow t_i, P_i\leftarrow \alpha_i$ where $\sum \alpha_i = \sum p_i =1, \alpha_i\ge0, p_i\ge0$</p> <p>Set $\alpha_i = \frac{b_i}{\sum_j b_j}$ because we need to property $\sum_i \alpha_i=\sum_i\frac{b_i}{\sum_j b_j} = \frac{1}{\sum_j b_j}\left(\sum_i b_i\right) = 1$.</p> <p>set $t_i=\frac{a_i}{b_i}$</p> <p>Then we obtain $\sum_i \alpha_i f(t_i)=\sum_i\frac{b_i}{\sum_j b_j}\,f!\left(\frac{a_i}{b_i}\right) = \sum_i\frac{b_i}{\sum_j b_j}\frac{a_i}{b_i}\log\frac{a_i}{b_i}$</p> <p>$\;\,\,\qquad\qquad\qquad\qquad\qquad = \sum_i\frac{a_i}{\sum_j b_j}\log\frac{a_i}{b_i} = \frac{1}{\sum_j b_j}\sum_i a_i\log\frac{a_i}{b_i}$</p> <p>$\;\,\,\qquad\qquad\qquad\qquad\qquad \ge f\left( \sum_i \alpha_i t_i \right)$ by Jensenâ€™s inequality</p> <p>$\;\,\,\qquad\qquad\qquad\qquad\qquad = \left[ \sum_i \left( \frac{b_i}{\sum_j b_j} \right)\frac{a_i}{b_i} \right] \log \left[ \sum_i \left( \frac{b_i}{\sum_j b_j} \right)\frac{a_i}{b_i} \right]$</p> <p>$\;\,\,\qquad\qquad\qquad\qquad\qquad = \frac{1}{\sum_j b_j} \sum_i a_i \log \left( \frac{1}{\sum_j b_j}\sum_i a_i \right)$</p> <p>In this inequality, $\frac{1}{\sum_j b_j}\sum_i a_i\log\frac{a_i}{b_i} \ge \frac{1}{\sum_j b_j} \sum_i a_i \log \left( \frac{1}{\sum_j b_j}\sum_i a_i \right)$</p> <p>$\therefore\; \sum_i a_i\log\frac{a_i}{b_i} \ge \sum_i a_i \log \left( \frac{\sum_i a_i}{\sum_i b_i}\right)$</p> </li> </ul> <h4 id="applying-log-sum-inequality">Applying Log-sum inequality</h4> <ul> <li> <p>$D(p\parallel q)\ge0$ where $p ,q$ are p.m.f.</p> <p>Relatibe entropy $D(p\parallel q):=\sum_x p(x)\log\frac{p(x)}{q(x)}\ge\sum_x p(x)\log\frac{\sum_x p(x)}{\sum_x q(x)} =\sum_x p(x)\log1=0$</p> <p>$\therefore \; D(p\parallel q)\ge0$ (equality â‡” $\frac{p(x)}{q(x)}=1$ for all $x$)</p> </li> <li> <p><strong>Thm: Convexity of relative entropy</strong> $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)\le \lambda D(p_1\parallel q_1) + (1-\lambda)D(p_2\parallel q_2)$</p> <p>$D(p\parallel q)$ is convex in the pair $(p,q)$ where $p,q$ are p.m.f. i.e. If $(p_1, q_1), (p_2,q_2)$ are two pairs of p.m.f. and $0\le \lambda\le1$, then $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)\le \lambda D(p_1\parallel q_1) + (1-\lambda)D(p_2\parallel q_2)$</p> <p>pf: First, we check $\lambda p_1+(1-\lambda) p_2$ and $\lambda p_1+(1-\lambda) p_2$ are p.m.f.</p> <p>Itâ€™s trivial!</p> <p>Next, $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)$</p> <p>$\qquad := \sum_x\left[ \lambda p_q(x)+(1-\lambda) p_2(x) \right]\log\frac{\lambda p_1+(1-\lambda)p_2}{\lambda q_1+(1-\lambda)q_2}$</p> <p>$\qquad\le \sum_x\left[\sum_i \left{ \lambda p_1+(1-\lambda)p_2\right} \log\frac{\lambda p_1+(1-\lambda)p_2}{\lambda q_1+(1-\lambda)q_2}\right]$</p> <p>$\qquad=\sum_x\left[ \lambda p_1(x)\log\frac{ \lambda p_1(x) }{ \lambda q_1(x) } + (1-\lambda)p_2(x)\log\frac{ (1-\lambda) p_2(x) }{ (1-\lambda) q_2(x) } \right]$</p> <p>$\qquad=\lambda\sum_xp_1(x)\log\frac{p_1(x)}{q_1(x)}+(1-\lambda)\sum_xp_2(x)\log\frac{p_2(x)}{q_2(x)}$</p> <p>$\qquad=\lambda D(p_1\parallel q_1)+(1-\lambda) D(p_2\parallel q_2)$</p> <p>ì¦‰, ë‘ p.m.f.ë¥¼ combinationí•œ ë’¤ relative entropyëŠ” ê°ê°ì˜ relative entropyì˜ í‰ê· ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìŒ</p> </li> <li> <p><strong>Thm. Concavity of entropy</strong> $H(p)$ is a concave function of p.m.f. $p$</p> <table> <tbody> <tr> <td>pf 1: Let $p(x)$ is p.m.f., $x\in \chi={x_1,\ldots,x_n}$ with $</td> <td>\chi</td> <td>=n$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>and $u(x):=\frac{1}{</td> <td>\chi</td> <td>}$ is uniform distribution</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$H(p)=\log</td> <td>\chi</td> <td>-D(p\parallel u)$</td> </tr> </tbody> </table> <p>ğŸ’¡ $D(p\parallel u)=\sum p(x)\log\frac{p(x)}{u(x)} = \sum p(x)\log\frac{1}{u(x)}-\sum p(x)\log\frac{1}{p(x)}$</p> <table> <tbody> <tr> <td>Since $\log\frac{1}{u(x)}=\log</td> <td>\chi</td> <td>$ is constant, $D(p\parallel u)=\log</td> <td>\chi</td> <td>-H(p)$.</td> </tr> </tbody> </table> <p>Thus $D(p\parallel u)$ is convex.</p> <p>â‡’ $-D(p\parallel u)$ is concave</p> <table> <tbody> <tr> <td>$H(p)=\log</td> <td>\chi</td> <td>-D(p\parallel u)$</td> </tr> </tbody> </table> <p>$\therefore\ H(p)$ is concave</p> <p>pf 2:</p> </li> </ul> <h2 id="channel-capacity">Channel Capacity</h2> <ul> <li>í•œë²ˆì— (ì¼ì • ì‹œê°„ ë‚´ì—) ìµœëŒ€ë¡œ ì „ì†¡í•  ìˆ˜ ìˆëŠ” ë°ì´í„°</li> </ul> <p>ğŸ’¡ Communication channel</p> <p>outputì´ í™•ë¥ ì ìœ¼ë¡œ inputì— ì˜ì¡´í•˜ëŠ” ì‹œìŠ¤í…œ</p> <ul> <li> <p>Ex (<em>noisy four-symbol channel )</em></p> <p>error rate $p=1/2$</p> <p>(case 1) $P(X=x)$ê°€ uniformí•œ ê²½ìš°</p> <table> <thead> <tr> <th>X \ Y</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>P(X)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>1/8</td> <td>1/8</td> <td>0</td> <td>0</td> <td>1/4</td> </tr> <tr> <td>2</td> <td>0</td> <td>1/8</td> <td>1/8</td> <td>0</td> <td>1/4</td> </tr> <tr> <td>3</td> <td>0</td> <td>0</td> <td>1/8</td> <td>1/8</td> <td>1/4</td> </tr> <tr> <td>4</td> <td>1/8</td> <td>0</td> <td>0</td> <td>1/8</td> <td>1/4</td> </tr> <tr> <td>P(Y)</td> <td>1/4</td> <td>1/4</td> <td>1/4</td> <td>1/4</td> <td>Â </td> </tr> </tbody> </table> <p>Yë¥¼ ë°›ì•˜ì„ ë•Œ Xë¥¼ í•˜ë‚˜ë¡œ ê²°ì •í•  ìˆ˜ ì—†ìŒ</p> <p>â†’ decoding ì‹¤íŒ¨. $I(X;Y)\neq1$</p> <p>(case 2) $P(X=x)$ê°€ uniformí•˜ì§€ ì•Šì€ ê²½ìš°</p> <table> <thead> <tr> <th>X \ Y</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>P(X)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>1/4</td> <td>1/4</td> <td>0</td> <td>0</td> <td>1/2</td> </tr> <tr> <td>2</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>3</td> <td>0</td> <td>0</td> <td>1/4</td> <td>1/4</td> <td>1/2</td> </tr> <tr> <td>4</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>P(Y)</td> <td>1/4</td> <td>1/4</td> <td>1/4</td> <td>1/4</td> <td>Â </td> </tr> </tbody> </table> <p>noisy n-symbol channel</p> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_183927.png" alt="n-symbol channel"></p> <p>Yë¥¼ ë°›ìœ¼ë©´ Xë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŒ</p> <p>â†’ decoding ì„±ê³µ. $I(X;Y)=1$</p> <p>4ê°œì˜ ì •ë³´ë¥¼ ì „í•  ìˆ˜ ìˆëŠ” 2ë¹„íŠ¸ ì±„ë„ì„ ê°€ì§€ê³  ìˆì§€ë§Œ,</p> <p>ì‹¤ì œë¡œëŠ” 1ê³¼ 3 ë‘ ì •ë³´ë§Œ ì£¼ê³ ë°›ëŠ” 1ë¹„íŠ¸ ì±„ë„ë¡œ ì‚¬ìš©í•¨</p> <p>â†’ ì±„ë„ ìš©ëŸ‰ $C=1$ bit</p> <p>ë” ì¢‹ì€ ë°©ë²•ì´ ìˆë‹¤ë©´ C ìˆ˜ì •</p> </li> <li> <p>$C:=\max I(X;Y)$</p> <p>The capacity is the maximum tate at which we can send information over the channel</p> <p>ì±„ë„ì˜ ê¸°ë³¸ì ì¸ ì„¤ì •(ì…ë ¥ ê°€ëŠ¥í•œ ê°’ë“¤ì˜ ë²”ìœ„, error rate ë“±)ì€ ì½”ë”©ì„ í†µí•´ ë°”ê¿€ ìˆ˜ ì—†ìŒ</p> <p>ì½”ë”©ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥í•œ ê°’ì€ $p(x)$</p> <p>ì´ë¥¼ ì¡°ì ˆí•˜ì—¬ ê°€ì¥ í° $C$ë¥¼ ì–»ëŠ” distributionìœ¼ë¡œ ì±„ë„ì„ êµ¬ì„±í•´ì•¼ íš¨ìœ¨ì´ ê°€ì¥ ì¢‹ìŒ</p> </li> </ul> <h3 id="data-processing-inequality">Data Processing inequality</h3> <h4 id="data-flow">Data flow</h4> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%2010.png" alt="noisy channel"></p> <ul> <li> <p><strong>Def: Markov Chain</strong> $X, Y, Z$ : R.V.s $X\rightarrow Y\rightarrow Z$ (i.e. Markov chain) $\Leftrightarrow p(x,y,z)=p(x)p(y|x)p(z|y)$ and we also can denote as $X\longleftrightarrow Y\longleftrightarrow Z$</p> <p>ğŸ’¡ For $X_1, X_2,\ldots,X_n$, $X_1\rightarrow X_2\rightarrow \cdots X_{n-1}\rightarrow X_n \rightarrow\cdots$ is Markov chain when $P(X_n|X{n-1},\ldots,X_1)=P(X_n|X_{n-1})$</p> <p>Def â‡” $Z$ depends only on $Y$ â€” (1)</p> <p>â‡” $Z$ is conditionally independent of $X$ given $Y$ â€” (2)</p> <table> <tbody> <tr> <td>i.e. $P(X,Z\</td> <td>\ Y)=P(X</td> <td>Y)P(Z</td> <td>Y)$</td> </tr> </tbody> </table> <p>ğŸ’¡ Conditional probability $p(x,y)=p(x)p(y|x)$ $p(x,y,z)=p(x)p(y,z\,|\,x)=p(x)p(y|z)p(z\,|\,y,x)$ â€¦ $p(x_1, x_2,\ldots,x_n)= \prod_{i=1}^n p(x_i\,|\,x_{i-1},\ldots,x_1)$</p> <p>(1) For Markov chain $X\rightarrow Y\rightarrow Z$,</p> <table> <tbody> <tr> <td>we need to show $p(x,y,z)=p(x)p(y</td> <td>x)p(z\,</td> <td>\,y,x)=p(x)p(y</td> <td>x)p(z</td> <td>y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>By the definition, $p(z</td> <td>\ y,x)=p(z</td> <td>y)$ is trivial.</td> </tr> </tbody> </table> <p>ğŸ’¡ $Z$ depends on $X$ only through $Y$</p> <p>ì¦‰, $X$ì˜ ë³€í™”ëŠ” $Y$ë¥¼ í†µí•´ì„œë§Œ $Z$ì— ë°˜ì˜ë¨</p> <p>â‡” $X$ì™€ $Z$ê°€ dependí•˜ì§€ë§Œ, $Y$ë¥¼ í†µí•´ì„œë§Œ dependí•˜ë¯€ë¡œ $Y$ì—ë§Œ dependí•˜ ë³´ì„</p> <p>â‡” $Z$ê°€ $X$ë¡œë¶€í„° dependí•˜ëŠ” ì •ë„ê°€ ì´ë¯¸ $Y$ì— ë°˜ì˜ë˜ì–´ ìˆìŒ</p> <table> <tbody> <tr> <td>Thus, $p(x,y,z)=p(x)p(y</td> <td>x)p(z</td> <td>y)$</td> </tr> </tbody> </table> <p>(2) Given $Y$,</p> <table> <tbody> <tr> <td>$p(x,z\</td> <td>y)=p(x,y,z)/p(y)$ and by the property of Markov chain,</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\qquad\qquad\ =[p(x)p(y</td> <td>x)p(z</td> <td>y)]/{p(y)}=[p(x,y)/p(y)]p(z</td> <td>y)=p(x</td> <td>y)p(z</td> <td>y)$</td> </tr> </tbody> </table> <p>Therefore, $X,Z$ are independent given $Y$</p> <p>By (1), (2), $X\rightarrow Y\rightarrow Z$ $\iff$ $X,Z$ are independent given $Y$</p> <p>and it also can be represented as $Z,X$ are independent given $Y$</p> <p>then, $\iff \ Z\rightarrow Y\rightarrow X$ holds.</p> <p>We may write $X\longleftrightarrow Y\longleftrightarrow Z$</p> <p>Then if $Z=f(Y)$, $X\rightarrow Y\rightarrow f(Y)$?</p> <p>ì§ê´€ì ìœ¼ë¡œ ìƒê°í•  ë•Œ, $Z$ëŠ” $Y$ê°€ ê²°ì •ë˜ëŠ” ìˆœê°„ ê°™ì´ ê²°ì •ë¨</p> <p>i.e., $Y$ determines $Z$ completely.</p> <p>It means $Z$ depends only $Y$ and is the definition of Markov chain.</p> <p>So, this chain is holds.</p> </li> </ul> <p>ğŸ’¡ R.V.s $X, Y, Z$ê°€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ Markov chainì„</p> <ul> <li> <table> <tbody> <tr> <td>$P(Z</td> <td>Y)=P(Z</td> <td>\ Y,X)$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(X,Y,Z)=P(X)P(Y</td> <td>X)P(Z</td> <td>Y)$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(Z</td> <td>Y)P(X</td> <td>Y)=P(X,Z\</td> <td>Y)$</td> </tr> </tbody> </table> </li> <li> <p>ex</p> <p>$X\xrightarrow[channel\,1]{BSC} Y\xrightarrow[channel\,2]{BSC} Z$,</p> <p>error rate of channel1 = $p_1$, error rate of channel2 = $p_2$</p> <p><strong>BSC channel 1</strong></p> <p>Input data</p> <p>given $X=0$</p> <p>given $X=1$</p> <p>Joint distribution</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>3/4</td> <td>1/4</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(Y</td> <td>X=0)$</td> <td>$1-p_1$</td> <td>$p_1$</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(Y</td> <td>X=1)$</td> <td>$p_1$</td> <td>$1-p_1$</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X$ \ $Y$</th> <th>0</th> <th>1</th> <th>$P(X)$</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>$\frac{3}{4}(1-p_1)$</td> <td>$\frac{3}{4}p_1$</td> <td>3/4</td> </tr> <tr> <td>1</td> <td>$\frac{1}{4}p_1$</td> <td>$\frac{1}{4}(1-p_1)$</td> <td>1/4</td> </tr> <tr> <td>$P(Y)$</td> <td>$\frac{3}{4}-\frac{1}{2}p_1$</td> <td>$\frac{1}{4}-\frac{1}{2}p_1$</td> <td>Â </td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Check: $P(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>given $Y$ (ë¹ˆ ì¹¸ ì±„ìš°ê¸°)</p> <p>given $Y=0$</p> <p>given $Y=1$</p> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(X</td> <td>Y=0)$</td> <td>Â </td> <td>Â </td> </tr> </tbody> </table> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(X</td> <td>Y=1)$</td> <td>Â </td> <td>Â </td> </tr> </tbody> </table> <p>$Z=f(Y):=2Y-1$ ë¼ë©´</p> <p>Joint distribution</p> <p>$p_1=1/4$ì¼ ë•Œ $Y$ì˜</p> <p>input data</p> <p>$P(Y=0)=5/8$</p> <p>$P(Y=1)=3/8$</p> <table> <thead> <tr> <th>$Y$ \ $Z$</th> <th>-1</th> <th>1</th> <th>$P(Y)$</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>5/8</td> <td>0</td> <td>5/8</td> </tr> <tr> <td>1</td> <td>0</td> <td>3/8</td> <td>3/8</td> </tr> <tr> <td>$P(Z)$</td> <td>5/8</td> <td>3/8</td> <td>Â </td> </tr> </tbody> </table> <p>$Y$ì˜ ë¶„í¬ë¥¼ ì•Œë©´ $Z$ì˜ ë¶„í¬ê°€ fixë¨</p> <p>($X$ì˜ ë¶„í¬ë¥¼ ëª¨ë¥´ê³  $Y$ ë¶„í¬ë§Œ ì•Œì•„ë„ ê³ ì •ë¨, ê°œì…í•  ì—¬ì§€ê°€ ì—†ì–´ì§)</p> </li> </ul> <h4 id="theorem-data-processing-inequality">Theorem: Data Processing Inequality</h4> <ul> <li> <p>$X\rightarrow Y\rightarrow Z \ \Longrightarrow\ I(X;Y)\ge I(X;Z)$</p> <p>ğŸ’¡ <strong>Probability</strong> $p(x_1,\ldots,x_n)=\prod_{i=1}^n p(x_i\,|\,x_{i-1},\ldots,x_1)$</p> <p><strong>Entropy</strong> $H(x_1,\ldots,x_n)=\sum_{i=1}^n H(x_i\,|\,x_{i-1},\ldots,x_1)$ $=H(x_1)+H(x_2|x_1)+H(x_3\,|\,x_2,x_1)+\cdots$</p> <p><strong>Mutual information</strong> $I(X;Y)=H(X)-H(X|Y)$ $I(X_1,\ldots,X_n\;;\,Y)=\sum_{i=1}^n I(X_i;Y\;|\;X_{i-1},\ldots,X_1)$</p> <p>pf: Since $I(X;\ Y,Z)=I(Y,Z\; ;X)=I(Z,Y\; ;X)$</p> <table> <tbody> <tr> <td>$\qquad\qquad\qquad\qquad\;\;\,=I(Z;X)+I(Y;X\;</td> <td>Z)=I(X;Z)+I(X;Y\;</td> <td>Z)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\qquad\qquad\qquad\qquad\;\;\,=I(X;\ Z,Y)=I(X;Y)+I(X;Z\;</td> <td>Y)$,</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X;Z)+I(X;Y\</td> <td>Z)=I(X;Y)+I(X;Z\</td> <td>Y)$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X;Z\</td> <td>Y)=0$ by property of Markov Chain</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>$X,Z$ are conditionally independent given $Y$ $\Rightarrow\ I(X;Z\</td> <td>Y)=0$</td> </tr> </tbody> </table> </li> </ul> <table> <tbody> <tr> <td>$I(X;Y)=I(X;Z)+I(X;Y\</td> <td>Z)\ge I(X;Z)$ $\because I(A;B)\ge0$</td> </tr> </tbody> </table> </li> </ul> <p>ğŸ’¡ ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³¼ ë•Œ, $X$ë¡œë¶€í„° $Y$ë¥¼ ì•„ëŠ” ê²ƒì´ $X$ë¡œë¶€í„° $Z$ë¥¼ ì•„ëŠ” ê²ƒë³´ë‹¤ ì‰¬ì›€ (ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ë” ë§ìŒ). ì´ë•Œ ë“±í˜¸ê°€ ì„±ë¦½í•œë‹¤ëŠ” ê²ƒì€ $Y$ë¥¼ ê±°ì³ $Z$ì—ì„œ $X$ê°€ ì˜í–¥ì„ ë¯¸ì§€ëŠ” ì •ë„ì— ì†ì‹¤ì´ ì—†ìŒì„ ì˜ë¯¸í•¨</p> <ul> <li> <p>Corollary $Z=g(Y)\ \Longrightarrow\ I(X;Y)\ge I\left(X;g(Y)\right)$</p> <p>pf: $X\longrightarrow Y\longrightarrow g(Y)$</p> <p>$Y\rightarrow g(Y)$ëŠ” ìµœëŒ€ 1-1 ëŒ€ì‘ ê´€ê³„ì´ë¯€ë¡œ $g(Y)$ì˜ ì •ë³´ëŠ” $Y$ë³´ë‹¤ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì˜ ë²”ìœ„ê°€ ì‘ìŒ</p> <p>â‡’ trivial!</p> </li> <li> <p>Corollary $X\longrightarrow Y\longrightarrow Z\;\Longrightarrow\; I(X;Y\ |Z)\le I(X;Y)$</p> <table> <tbody> <tr> <td>pf: Since $I(X;\ Y,Z)=I(X;Y)+I(X;Z\</td> <td>Y)=I(X;Y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>and $I(X;\ Y,Z)=I(X;Z)+I(X;Y\</td> <td>Z)$,</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\Rightarrow\ I(X;Y)=I(X;Z)+I(X;Y\</td> <td>Z)\ge I(X;Y\</td> <td>Z)$ $\because I(X;Z)\ge0$</td> </tr> </tbody> </table> </li> </ul> <p>ğŸ’¡ $X\longrightarrow Y\longrightarrow Z$ The dependency of $X$ and $Y$ is decreased (or unchanged) by the observation of a downstream (to receiver) R.V. $Z$ i.e., $I(X;Y)\ge I(X;Y\ |Z)$</p> <ul> <li> <p>ex</p> <p>case1) $X_1, X_2, X_3$ : independent R.V.s</p> <p>case2) $X_1$ : dice {1, 2, â€¦ , 6}, $X_2$ : biased coin {0, 1}, $X_3=2X_2-1$</p> <table> <thead> <tr> <th>$X_1$</th> <th>1</th> <th>â€¦</th> <th>6</th> </tr> </thead> <tbody> <tr> <td>$P(X_1)$</td> <td>1/6</td> <td>Â </td> <td>1/6</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X_2$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X_2)$</td> <td>$1-\theta$</td> <td>$\theta$</td> </tr> </tbody> </table> <p>$\theta:=\frac{1}{X_1}\in\left{ 1, \frac{1}{2},\frac{1}{3},\ldots,\frac{1}{6} \right}$</p> <p>$X_1=2$ â‡’ fair coin toss</p> <p>Which case is a Markov chain as $X\longrightarrow Y\longrightarrow Z$?</p> <table> <tbody> <tr> <td>Definition : $p(x,y,z)=p(x)p(y</td> <td>x)p(z</td> <td>y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>and we can lead $P(Z</td> <td>Y)P(X</td> <td>Y)=P(X,Z\</td> <td>Y)$</td> </tr> </tbody> </table> <p>â‡’ given $Y$, $X$and $Z$ are independent from Markov chain</p> <p>case1) and case2) are both satisfied the definition of Markov chain</p> <table> <tbody> <tr> <td>case1) $P(X_1)P(X_3)=P(X_1,X_3)$, $P(X_1,X_3\</td> <td>X_2)=P(X_1</td> <td>X_2)P(X_3</td> <td>X_2)$</td> </tr> </tbody> </table> <p>case2) $X\longrightarrow Y\longrightarrow g(Y)$ í˜•íƒœì´ë©´ Markov chain</p> <p>$X_2$ depends $X_1$, $X_3$ is determined by $X_2$.</p> <p>ë”°ë¼ì„œ $X_3$ëŠ” $X_1$ì— dependí•˜ì§€ë§Œ, $X_2$ë¥¼ í†µí•´ì„œ ì˜í–¥ì„ ë°›ìŒ</p> <p>($X_1$ì€ $X_2$ë¥¼ í†µí•´ì„œê°€ ì•„ë‹Œ ë°©ë²•ìœ¼ë¡œ $X_3$ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ìŒ)</p> </li> </ul> <p><strong>Caution!</strong> $I(X;Y\ |Z)&gt;I(X;Y)$ but $I(X;Y\ |Z)\neq I(X;Y)$ is possible</p> <ul> <li> <p>ex: $X,Y$ are fair coin tosses (independent)<br> $Z:=X+Y$ â€” Not Markov chain</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(Y)$</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X$ \ $Y$</th> <th>0</th> <th>1</th> <th>$P(X)$</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1/4</td> <td>1/4</td> <td>1/2</td> </tr> <tr> <td>1</td> <td>1/4</td> <td>1/4</td> <td>1/2</td> </tr> <tr> <td>$P(Y)$</td> <td>1/2</td> <td>1/2</td> <td>Â </td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X,Y$ \ $Z$</th> <th>0</th> <th>1</th> <th>2</th> <th>$P(X,Y)$</th> </tr> </thead> <tbody> <tr> <td>0, 0</td> <td>1/4</td> <td>0</td> <td>0</td> <td>1/2</td> </tr> <tr> <td>0, 1</td> <td>0</td> <td>1/4</td> <td>0</td> <td>1/2</td> </tr> <tr> <td>1, 0</td> <td>0</td> <td>1/4</td> <td>0</td> <td>1/4</td> </tr> <tr> <td>1, 1</td> <td>0</td> <td>0</td> <td>1/4</td> <td>1/4</td> </tr> <tr> <td>$P(Z)$</td> <td>1/4</td> <td>1/2</td> <td>1/4</td> <td>Â </td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(X</td> <td>Z=1)$</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <p>ğŸ’¡ - $I(X;Y)=0$ since $X,Y$ are independent</p> <ul> <li> <table> <tbody> <tr> <td>$I(X;Y\</td> <td>Z)=H(X</td> <td>Z)-H(X</td> <td>\; Y,Z)$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$H(X</td> <td>\;Y,Z)=0$ since given $Y,Z$, $X=Z-Y$ determined</td> </tr> </tbody> </table> </li> </ul> <table> <tbody> <tr> <td>$I(X;Y\</td> <td>Z)=H(X</td> <td>Z) \ \qquad\qquad\quad =P(Z=0)H(X</td> <td>Z=0) \quad\longrightarrow P(X=0)=1 \quad (X=0,Y=0)\ \qquad\qquad\qquad +P(Z=1)H(X</td> <td>Z=1)\ \qquad\qquad\qquad +P(Z=2)H(X</td> <td>Z=2) \quad\longrightarrow P(X=1)=1\quad (X=1,Y=1) \ \qquad\qquad\quad =P(Z=1)H(X</td> <td>Z=1)=\frac{1}{2}H\left(\frac{1}{2},\frac{1}{2}\right)=1/2$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>ì¦‰, $I(X;Y)=0&lt;I(X;Y\</td> <td>Z)=1/2$</td> </tr> </tbody> </table> <p>Markov chainì´ë©´ ë¶€ë“±í˜¸ê°€ ë°˜ëŒ€ì—¬ì•¼ í•˜ë¯€ë¡œ $X\longrightarrow Y\longrightarrow Z$ëŠ” Markov chainì´ ì•„ë‹˜</p> </li> </ul> <h3 id="sufficient-statistics">Sufficient Statistics</h3> <ul> <li>Given $\theta \longrightarrow X \longrightarrow T(X)$, if $I(\theta;T(X))=I(\theta;X)$, then $T(X)$ is called sufficient for $\theta$. <ul> <li> <p>${f_{\theta}(x)}$: family of p.mp.f. indexed by $\theta$ ; 1-parameter family</p> <p>$\theta$ê°€ ê²°ì •ë˜ë©´ $f$ê°€ ê²°ì •ë˜ê³  $x$ì— ì˜í•œ ê°’ì„ ì–»ì„ ìˆ˜ ìˆëŠ” í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜</p> <p>($\theta$ : index parameter, $\theta$ì— ëŒ€í•˜ì—¬ í™•ë¥ ì ì¸ ë¶€ë¶„ì´ ìˆë‹¤ë©´ R.V.)</p> <p>â†’ parameter í•˜ë‚˜ê°€ ê²°ì •ë˜ë©´ í•¨ìˆ˜ì˜ ë³€ìˆ˜ $X$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ìˆìŒ</p> <p>ex:</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>$1-\theta$</td> <td>$\theta$</td> </tr> </tbody> </table> <p>$P(X=1)=\theta$</p> </li> </ul> <p>$X\sim f_{\theta}(x)$ : $X$ is a R.V. from a distribution in this family ${f_{\theta}(x)}$</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%2011.png" alt="coin tossing"></p> <p>coinë§ˆë‹¤ $\theta$ê°€ ì •í•´ì ¸ ìˆìŒ</p> <p>${f_{\theta}(x)}\longrightarrow f_{\theta}(x)\longrightarrow X$</p> <p>family â†’ p.m.f. â†’ R.V.</p> <p>$X \longrightarrow T(X)$ : statistic, $X$ë¡œë¶€í„° ì–»ì€ í†µê³„ëŸ‰, funtion of $X$</p> <p>Then, $\theta, X, T(X)$ form a Markov chain. i.e., $\theta \longrightarrow X \longrightarrow T(X)$</p> <p>â‡’ $\theta \longrightarrow X \longrightarrow T(X) \;\Longrightarrow I(\theta;T(X))\le I(\theta;X)$</p> <p>If $I(\theta;T(X))=I(\theta;X)$, then $T(X)$ is called sufficient for $\theta$.</p> <p>ğŸ’¡ ì‹¤í—˜ ë°ì´í„°ë¥¼ í•´ì„í•  ë•Œ,</p> <ul> <li> <p>ì‹¤í—˜ ëŒ€ìƒì´ familyë¡œë¶€í„° ì‹¤í—˜ ëŒ€ìƒì„ ê±°ì³ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì–»ê¸°ê¹Œì§€</p> <p>ex: í˜„ì‹¤ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ë©´ familyì— ëŒ€í•œ ì •ë³´ëŠ” ì•Œ ìˆ˜ ì—†ê³  ì‹¤í—˜ ê²°ê³¼ë§Œ ì•Œ ìˆ˜ ìˆìŒ</p> <p>ë™ì „ì„ 10ë²ˆ ë˜ì ¸ì„œ 8ë²ˆì´ Hê°€ ë‚˜ì˜¤ë©´, Hê°€ ë‚˜ì˜¬ í™•ë¥ ì´ $\theta=4/5$ì¸ ë™ì „ì„ ì‚¬ìš©í–ˆë‹¤ê³  ì¶”ì •í•˜ëŠ” ê²ƒì´ íƒ€ë‹¹í•¨.</p> <p>ë¬¼ë¡  ì‹¤ì œë¡œ ì‚¬ìš©í•œ ë™ì „ì´ Hê°€ 4/5ì˜ í™•ë¥ ë¡œ ë‚˜ì˜¤ëŠ” ë™ì „ì¸ì§€ëŠ” ì•Œ ìˆ˜ ì—†ìŒ Hê°€ ë‚˜ì˜¬ í™•ë¥ ì´ 1/3ì¸ë° ë˜ì§€ëŠ” ë°©ì‹ì— ì˜í•´ Hë§Œ ë‚˜ì™”ì„ ìˆ˜ë„ ìˆìŒ</p> <p>ì´ë ‡ë“¯ $n$ë²ˆì˜ ì‹œí–‰ìœ¼ë¡œë¶€í„° ì–»ì€ ë°ì´í„°ì—ì„œ $T(X)$ë¥¼ êµ¬í•˜ê³  ê°œë³„ ì‹œí–‰ì˜ ê²°ê³¼ëŠ” ì‹œí–‰ í•  ë•Œë§ˆë‹¤ ë‹¤ë¥´ê² ì§€ë§Œ $T(X)$ë¥¼ í†µí•´ $n$ë²ˆ ì¤‘ Hê°€ ë‚˜ì˜¤ëŠ” íšŸìˆ˜ì˜ ê¸°ëŒ€ê°’ $\theta$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ê³  ìš”ì•½ ê°€ëŠ¥í•¨</p> <p>â‡’ sufficient statistics</p> <p>= í†µê³„ëŸ‰ì„ ì˜ ì¡ìœ¼ë©´ ëª‡ íšŒì˜ ì‹œí–‰ì´ ìˆì—ˆëŠ”ì§€, ëª‡ ë²ˆ ì›í•˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ì§€ ëª¨ë¥´ë”ë¼ë„ $\theta$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŒ</p> <p>information lossê°€ í•˜ë‚˜ë„ ì—†ìŒì„ ë³´ì´ê¸° ìœ„í•œ ê°€ì •ìœ¼ë¡œ ì“°ì„</p> </li> </ul> </li> <li>Def <ul> <li> <p>Informal version</p> <p>Informmaly, $T(X)$ is called sufficient for $\theta$ if it contains all information in $X$ about $\theta$.</p> <p>ì¦‰, $\theta$ì— ëŒ€í•´ì„œ $X$ê°€ ê°€ì§€ê³  ìˆëŠ” informationì„ $T(X)$ê°€ ëª¨ë‘ ê°€ì§€ê³  ìˆìŒ</p> </li> <li> <p>1st formal version</p> <p>$T(X)$ is said to be a sufficient statistic relative to ${f_{\theta}(x) }$</p> <p>if $X$ is independent of $\theta$ given $T(X)$ for any distributioni $f_{\theta}(x)$.</p> <p>ì¦‰, $T(X)$ë¥¼ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ indepentí•¨</p> <ul> <li> <p>$X$ì™€ $\theta$ì˜ dependí•œ ì •ë³´ëŠ” ëª¨ë‘ $T(X)$ì— ì†í•´ìˆìŒ</p> <p>$X$ì™€ $\theta$ëŠ” ë…ë¦½ì´ ì•„ë‹˜ â‡’ $X$ì™€ $\theta$ê°€ ë…ë¦½ì´ë©´ ì„œë¡œì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŒ (ì§ê´€ì ìœ¼ë¡œ ë…ë¦½ì´ë©´ ì„œë¡œ ë¬´ê´€í•˜ê¸° ë•Œë¬¸ì— ì˜í–¥ì„ ì£¼ê³  ë°›ê³ ë¥¼ ë”°ì§ˆ ì˜ë¯¸ê°€ ì—†ì–´ ë³´ì„)</p> <p>ì£¼ì–´ì§„ $T(X)$ì— ëŒ€í•´ì„œ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ ë…ë¦½ì´ë¼ëŠ” ë§ì€ $X$ì™€ $\theta$ê°€ ì£¼ê³ ë°›ëŠ” ì •ë³´ë¥¼ $T(X)$ê°€ ëª¨ë‘ ê°€ì ¸ê°”ìŒì„ ì˜ë¯¸í•¨</p> <p>â‡’ $T(X)$ì— ì†í•œ $X,\theta$ì˜ ì •ë³´ë¥¼ ì œì™¸í•˜ë©´ $X$ì™€ $\theta$ëŠ” ë…ë¦½ì„</p> </li> </ul> </li> <li> <p>2nd formal version</p> <p>If $\theta\longrightarrow X\longrightarrow T(X)$ is Markov chain then $\theta\longrightarrow T(X)\longrightarrow X$</p> <ul> <li> <p>$T(X)$ê°€ $\theta$ì™€ $X$ë¥¼ ì—°ê²°í•´ì£¼ëŠ” ëª¨ë“  ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ</p> <p>â‡” $\theta$ì™€ $X$ë¥¼ ì—°ê²°í•´ì£¼ëŠ” $T(X)$ê°€ ì£¼ì–´ì§€ë©´ $X$ì™€ $\theta$ê°€ indepentí•¨</p> <p>â‡” $T(X)$ë¥¼ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ indepentí•¨</p> </li> </ul> </li> </ul> <p>ğŸ’¡ If $T(X)$ is a sufficient statistic, $I(\theta\,;T(X))= I(\theta\,;X)$</p> <ul> <li> <p>i.e., No information loss</p> <p>2nd formal version definition shows that $I(\theta\,;T(X))\ge I(\theta\,;X)$.</p> <p>And in general, $\theta\longrightarrow X\longrightarrow T(X)$ then $I(\theta\,;T(X))\le I(\theta\,;X)$.</p> <p>Therefore, if $T(X)$ is a sufficient statistic, $I(\theta\,;T(X))= I(\theta\,;X)$.</p> </li> </ul> </li> </ul> <p>ğŸ’¡ Where $X_1, \ldots, X_n \sim f_{\theta}(x)$, $Y:=g(X_1, \ldots,X_n)$ is sufficient for $\theta$</p> <ul> <li> <table> <tbody> <tr> <td>if $P(X_1=x_1,\ldots,X_n=x_n\;</td> <td>Y=y)$ does not depend on $\theta$.</td> </tr> </tbody> </table> <hr> <p>Let $X_1, \ldots,X_n$ be R.V.s from a p.m.f. with parameter $\theta$</p> <p>i.e., $X_1, \ldots, X_n \sim f_{\theta}(x)$</p> <p>and $Y:=g(X_1, \ldots,X_n)$ is a funtion of $X_1, \ldots,X_n$</p> <p>i.e., $Y$ is deterministic for $X_1, \ldots,X_n$</p> <p>Then $Y$ is sufficient for $\theta$, if the conditional probability $P(X_1=x_1,\ldots,X_n=x_n\;|Y=y)$ does not depend on $\theta$.</p> <hr> <p>Since $Y=g(X_1,\ldots,X_n)$, $\theta\longrightarrow Y\longrightarrow (X_1,\ldots,X_n)$.</p> <table> <tbody> <tr> <td>â‡’ $P(X_1=x_1,\ldots,X_n=x_n\;</td> <td>Y=y)\ \quad = P(X_1=x_1,\ldots,X_n=x_n\;</td> <td>Y=y,\Theta=\theta)$</td> </tr> </tbody> </table> <p>ì¦‰, ì£¼ì–´ì§„ $Y$ì— ëŒ€í•˜ì—¬ $\theta$ì™€ $(X_1,\ldots,X_n)$ê°€ independent.</p> </li> <li> <p>ex. Bernoulli distribution</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>$1-\theta$</td> <td>$\theta$</td> </tr> </tbody> </table> <p>For sample space $\chi$, $x\in\chi={0,1}$.</p> <p>$P_\theta(X=x)= \left{\begin{matrix} \theta &amp; x=1 <br> 1-\theta &amp; x=0 <br> \end{matrix}\right.\quad\textrm{(p.m.f.)} \ \qquad\qquad\quad = \theta^x (1-\theta)^{1-x}$</p> <p>Let $X_1, \ldots, X_n$ be R.V.s sample from a Bernoulli distribution with $\theta$.</p> <p>Bernoulli : i.i.d.(independent and identically distributed)</p> <p>i.e., $P(X_1=x_1,\ldots,X_n=x_n)=P(X_1=x_1)\cdots P(X_n=x_n)$</p> <p>$\qquad\qquad\qquad\qquad\qquad\quad\;\, =\theta^{x_1}(1-\theta)^{1-x_1}\cdots \theta^{x_n}(1-\theta)^{1-x_n}$</p> <p>$\qquad\qquad\qquad\qquad\qquad\quad\;\, =\prod_{i=1}^n\theta^{x_i}(1-\theta)^{1-x_i}$</p> <p>Introduce a statistic $Y:=T(X_1,\ldots,X_n)=\sum_{i=1}^n X_i$.</p> <p>â‡’ counting 1â€™s out of $x_1,\ldots,x_n$ â‡” counting the number of Heads in $n$ tosses</p> <p>$Y\sim B(n,\theta)$ : Binomial ditribution</p> <p>â‡’ $P(Y=k)=\binom{n}{k}\theta^k(1-\theta)^{n-k}$</p> <p>Consider the conditional probabiltiy</p> <p>$P(X_1=x_1,\ldots,X_n=x_n)=\frac{P(X_1,=x_1\ ,\ldots,\ X_n=x_n \textrm{ and } Y=k)}{P(Y=k)}$</p> <p>$\qquad\qquad\qquad\qquad\qquad=\frac{\prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i}}{\binom{n}{k}\theta^k(1-\theta)^{n-k}}\textrm{\quad where }\sum_{i=1}^n x_i =k$</p> <p>$\qquad\qquad\qquad\qquad\qquad=\frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i} }{\binom{n}{k}\theta^k(1-\theta)^{n-k}}$</p> <p>$\qquad\qquad\qquad\qquad\qquad=\frac{\theta^{k}(1-\theta)^{n-k} }{\binom{n}{k}\theta^k(1-\theta)^{n-k}}$</p> <p>$\qquad\qquad\qquad\qquad\qquad=\frac{1}{\binom{n}{k}}$</p> <p>â‡’ $P(X_1=x_1,\ldots,X_n=x_n)=\frac{1}{\binom{n}{k}}$ does not depend on $\theta$,</p> <p>which means that given $Y=k\left( =\sum_{i=1}^n x_i \right)$,</p> <p>the individual values of the $x_i$â€™s cannot provide additional information about $\theta$.</p> <p>ì¦‰, $Y$ ê°’ì´ ì£¼ì–´ì§€ë©´ $x_i$ ê° ê°’ì€ (ê° $x_i$ê°€ 0ì¸ì§€ 1ì¸ì§€ëŠ”) $\theta$ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ</p> </li> <li> <p>ex. Poisson distribution</p> <p>p.m.f. $p(k)=\frac{\lambda^k e^{-\lambda}}{k!}$ for $k=0,1,2,\ldots$ ($\lambda$ë¥¼ ì•Œë©´ ëª¨ë“  parameterê°€ ê²°ì •ë¨)</p> <p>Let $X_1, \ldots, X_n$ be sample from a Poisson distribution with $\lambda$ and $Y=\sum_{i=1}^n X_i$ (=í†µê³„ëŸ‰)</p> <p>â€» $r$ : average rate of the event</p> <p>$t$ : time interval ($\lambda=rt$ : ì£¼ì–´ì§„ ì‹œê°„ $t$ë™ì•ˆ eventê°€ ë°œìƒí•˜ëŠ” íšŸìˆ˜ì˜ í‰ê· , ë¹ˆë„)</p> <p>â‡’ $P(k\textrm{ events in interval }t) = \frac{(rt)^k e^{-rt}}{k!}$</p> <p>$Y=\sum_{i=1}^n X_i \sim$ Poisson dist with $\lambda y=n\lambda$</p> <ul> <li> <p>$Y$ is sufficient for $\lambda$.</p> <p>Consider the conditional probability</p> <table> <tbody> <tr> <td>$P(X_1=x_1,\ldots, X_n=x_n\;</td> <td>Y=y)=\frac{P(X_1=x_1,\ldots, X_n=x_n\textrm{ and }Y=y)}{P(Y=y)}$</td> </tr> </tbody> </table> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{P(X_1=x_1,\ldots,X_n=x_n)}{P(Y=y)}\quad\textrm{ where }\sum x_i=y$</p> <p>(note that $\sum x_i\neq y\Rightarrow P(*)=0$)</p> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\prod_{i=1}^n\frac{\lambda^{x_i}e^{-\lambda}}{x_i!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$</p> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\frac{\lambda^{x_1+\cdots+x_n}e^{-\lambda}}{x_1!x_2!\cdots x_n!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$</p> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\frac{\lambda^y e^{-\lambda}}{x_1!\cdots x_n!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$</p> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{y!}{n^yx_1!\cdots x_n!}$ does not depend on $\lambda$.</p> </li> </ul> </li> <li> <p>Thm: Factorization Theorem $Y=g(X_1,\ldots,X_n)$ is sufficient for $\theta$ $\iff P(x_1,\ldots,x_n\;|\,\theta)=\phi(g(x_1,\ldots,x_n)\,|\,\theta)h(x_1,\ldots,x_n)$</p> <hr> <p>Let $X_1,\ldots,X_n$ be i.i.d. R.V.s from p.m.f. $f_\theta(x)\in {f_\theta(x)}$,</p> <p>$Y=g(X_1,\ldots,X_n)$ is sufficient for $\theta$</p> <table> <tbody> <tr> <td>$\iff P(x_1,\ldots,x_n\;</td> <td>\,\theta)=\phi(g(x_1,\ldots,x_n)\,</td> <td>\,\theta)h(x_1,\ldots,x_n)$</td> </tr> </tbody> </table> <p>where $\phi$ depends on $x_i$â€™s only through $g$ (i.e., $Y$) and $h$ does not depend on $\theta$.</p> <hr> <p>$\theta$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê° $X_i$ê°€ ê°’ $x_i$ë¥¼ ê°€ì§ˆ í™•ë¥ ì— ëŒ€í•˜ì—¬</p> <p>$\theta$ì— ì˜ì¡´í•˜ëŠ” ê°’ë“¤ì˜ í™•ë¥ ì€ $\phi$, ì¦‰ $g(x_1,\ldots,x_n)=Y$ì— ëª°ë ¤ìˆìŒ ($h$ëŠ” $\theta$ì— independent)</p> <p>â‡’ $Y$ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ” ê°’ë“¤ì€ $\theta$ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ</p> <ul> <li> <p>Remark</p> <p>ğŸ’¡ $P(x_1,\ldots,x_n):=P(X_1=x_1,\ldots,X_n=x_n)$</p> <p>$P(X_1,\ldots,X_n)$ê°€ $x_1,\ldots,x_n$ì„ ì–´ë–»ê²Œ ê²°ì •í•˜ëŠëƒì— ë”°ë¼ ë‹¬ë¼ì§ì„ ë°˜ì˜í•œ í‘œê¸°ì„</p> <table> <tbody> <tr> <td>ğŸ’¡ $P(<em>;\theta)=P(</em> </td> <td>\theta)$ì´ê¸° ìœ„í•´ì„œëŠ” $\theta$ê°€ R.V.ì—¬ì•¼ í•¨</td> </tr> </tbody> </table> <p>ìš°í•­ì˜ í‘œê¸°ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ì§€ë§Œ, $\theta$ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•˜ê³  ì‹¶ì§€ ì•ŠìŒ</p> <p>ì¢Œí•­ì˜ $\theta$ ëŠ” determined $\theta$ ë˜ëŠ” fixed $\theta$, indexed by $\theta$ì´ê³  ìš°í•­ì˜ $\theta$ëŠ” p.m.f.ë¥¼ ê°–ëŠ” R.V.ì„</p> </li> </ul> </li> <li> <p>ex: For Bernoulli distribution,</p> <p>$P(x_1,\ldots,x_n\ ;\theta)=\prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i})=\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}$</p> <p>Let $y=\sum_{i=1}^n x_i$,</p> <p>$\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}=\theta^{y}(1-\theta)^{n-y}\times1:=\phi(y\,;\theta)\times h(x_1,\ldots,x_n)$</p> <p>â‡’ Bernoulli distributionì— ëŒ€í•˜ì—¬</p> <p>$P(x_1,\ldots,x_n\ ;\theta)=\phi(y\,;\theta)\times h(x_1,\ldots,x_n)$ë¥¼ ë§Œì¡±í•˜ë¯€ë¡œ</p> <p>$Y=\sum X_i$ëŠ” $\theta$ì— ëŒ€í•˜ì—¬ sufficient statisticì„</p> </li> <li> <p>ex: For Poisson distribution,</p> <p>$p(x_1, \ldots,x_n\,;\lambda)=\frac{\lambda^{x_1+\cdots+x_n}e^{-n\lambda} }{x_1!\cdots x_n!}=\lambda^y e^{-n\lambda}\times\frac{1}{x_1!\cdots x_n!}$</p> <p>â‡’ Poisson distributionì— ëŒ€í•˜ì—¬</p> <p>$P(x_1,\ldots,x_n\ ;\lambda)=\phi(y\,;\lambda)\times h(x_1,\ldots,x_n)$ë¥¼ ë§Œì¡±í•˜ë¯€ë¡œ</p> <p>$Y=\sum X_i$ëŠ” $\lambda$ì— ëŒ€í•˜ì—¬ sufficient statisticì„</p> </li> </ul> <h4 id="minimal-sufficient-statistic">Minimal sufficient statistic</h4> <p>Def: $T(X)$ is a minimal sufficient statistic relative to ${ f_\theta(x) }$ if it is a function of every other sufficient statistic $U$.</p> <p>ì¦‰, sufficient statisticì´ë©´ì„œ ë‹¤ë¥¸ sufficient statisticë¡œ í‘œì‹œë˜ëŠ” í•¨ìˆ˜</p> <p>$\theta \longrightarrow T(X) \longrightarrow U(X) \longrightarrow X$</p> <p>ğŸ’¡ A minimal sufficient statistic maximally compresses the information about $\theta$ in the sample.</p> <p>Other sufficient statistics may contain additional irrelevant information.</p> <ul> <li>ex <ul> <li> <p>$Y=X_1+\cdots+X_n$ is minimal sufficient statistic for $\theta$ in the independent coin toss example.</p> <p>Let $Y_{odd}=X_1+X_3+\cdots+X_{2n-1}$, $Y_{even}=X_2+X_4+\cdots+X_{2n}$ and $\widetilde{Y}=(Y_{odd}, Y_{even})$, then $\widetilde{Y}$ is a sufficient statistic</p> <p>$Y=g(\widetilde{Y})=g(Y_{odd},Y_{even})=Y_{odd}+Y_{even}$</p> <p>â‡’ $Y$ì™€ ë¹„êµí•  ë•Œ, $Y_{odd}+Y_{even}$ëŠ” ë¶ˆí•„ìš”í•˜ê²Œ ì„¸ë¶€ì ìœ¼ë¡œ ë‚˜ë‰œ í•¨ìˆ˜ì„</p> </li> <li> <p>í•™êµì—ì„œ ì „êµ ìˆ˜í•™ì ìˆ˜ í‰ê· ì„ ì•Œê¸¸ ë°”ë„ ë•Œ,</p> <ul> <li>ì „ì²´ ë°ì´í„°ë¡œë¶€í„° ê° ë°˜ì˜ í‰ê·  ì ìˆ˜ë¥¼ êµ¬í•˜ê³  ì´ê²ƒë“¤ì„ ëª¨ì•„ì„œ ë‹¤ì‹œ ì „ì²´ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒë³´ë‹¤</li> <li>ì „ì²´ ë°ì´í„°ë¡œë¶€í„° í•œ ë²ˆì— ì „ì²´ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ í›¨ì”¬ íš¨ìœ¨ì ì„ (ë°˜ í‰ê· ì´ë¼ëŠ” ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ)</li> </ul> </li> </ul> </li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2026 Jieun Ryu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 20, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/june.github.io/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/june.github.io/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/june.github.io/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/june.github.io/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/june.github.io/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/june.github.io/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/june.github.io/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/june.github.io/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/june.github.io/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/june.github.io/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/june.github.io/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/june.github.io/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/june.github.io/assets/js/search-data.js"></script> <script src="/june.github.io/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> <script type="text/javascript">
      // ìš°í´ë¦­ ë°©ì§€
      document.oncontextmenu = function() {
        return false;
      };

      // ë“œë˜ê·¸ ë° ì„ íƒ ë°©ì§€
      document.onselectstart = function() {
        return false;
      };

      // í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ ë°©ì§€ (F12, Ctrl+Shift+I ë“± ê°œë°œì ë„êµ¬ ë° ë³µì‚¬ ë°©ì§€)
      document.onkeydown = function(e) {
        if (e.ctrlKey && (e.keyCode === 67 || e.keyCode === 86 || e.keyCode === 85 || e.keyCode === 83)) {
          return false; // Ctrl+C, Ctrl+V, Ctrl+U(ì†ŒìŠ¤ë³´ê¸°), Ctrl+S ë°©ì§€
        }
      };
    </script> </body> </html>