---
layout: post
title: "Information Theory"
date: 2024-01-02
description: "Information theory: Probability and Entropy"
tags: [MATH]
categories: [Study]
---


ğŸ“– Cover, Thomas M.Â *Elements of information theory*. John Wiley & Sons, 1999.


![ì¼ë°˜ì ì¸ í†µì‹  ì‹œìŠ¤í…œì˜ ë„ì‹]({{'/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_153653.png' | relative_url }}){: width="90%" }

ì´ìƒì ì¸ channelì€ transmitterë¡œë¶€í„° receiverê¹Œì§€ ì˜¤ë¥˜ ì—†ì´ ì •ë³´ê°€ ì „ë‹¬ë˜ëŠ” channelì´ì§€ë§Œ, ì¼ë°˜ì ì¸ channelì€ noise channelì„

### Probability

**Event** : ì¼ì–´ë‚  ìˆ˜ ìˆëŠ” ì–´ë–¤ ì‚¬ê±´, í™•ë¥  ë³€ìˆ˜ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’, $X=a$

- í™•ë¥  ë³€ìˆ˜ê°€ ì£¼ì–´ì§ â‡” í™•ë¥  ë³€ìˆ˜ë¥¼ ì• â‡” í™•ë¥  ë¶„í¬ë¥¼ ì•
    
    ex. ë™ì „ì„ ë˜ì¡Œì„ ë•Œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ë©´ì— ëŒ€í•œ í™•ë¥  ë³€ìˆ˜ X
    
    í™•ë¥  ë³€ìˆ˜ Xâˆˆ{ì•ë©´, ë’·ë©´}
    
    í™•ë¥  ë¶„í¬ëŠ” P(X=ì•ë©´)=1/2, P(X=ë’·ë©´)=1/2
    

ê²°ê³¼ë¡œ ë„ì¶œë  ìˆ˜ ìˆëŠ” ëª¨ë“  eventë¥¼ ëª¨ì€ ì§‘í•©ì´ sample spaceì´ê³ , â€˜ì–´ë–¤ eventê°€ ì–¼ë§ˆë‚˜ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ”ê°€?â€™ì— ëŒ€í•œ í™•ë¥ ì´ ì£¼ì–´ì¡Œì„ ë•Œ eventì— ëŒ€í•œ ë³€ìˆ˜ê°€ í™•ë¥  ë³€ìˆ˜ì´ê³  ì´ í™•ë¥  ë³€ìˆ˜ì˜ eventsì— ëŒ€í•œ í™•ë¥ ì´ í™•ë¥  ë¶„í¬ë¥¼ ì´ë£¸

â‡’ í™•ë¥  ë³€ìˆ˜ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì€ ì–´ë–¤ eventì˜ í™•ë¥ ì„ ë‹¤ë£¨ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë³€ìˆ˜ì˜ í™•ë¥  ë¶„í¬ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒ

í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜(probability mass function) : ì´ì‚° í™•ë¥  ë³€ìˆ˜ì˜ í™•ë¥  ë¶„í¬ì— ë”°ë¥¸ í•¨ìˆ˜

- í™•ë¥  ë³€ìˆ˜ $X$ë¥¼ ì‚¬ìš©í•œ í•¨ìˆ˜ $f(X)$ë„ ë³€ìˆ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê°–ëŠ” í™•ë¥  ë³€ìˆ˜ì„
    
    **í™•ë¥  ë³€ìˆ˜**
    
    ì‚¬ê±´ ê³µê°„ì—ì„œ ê°€ì¸¡ ê³µê°„ìœ¼ë¡œì˜ ê°€ì¸¡ í•¨ìˆ˜
    
    ex. $f(X)= \begin{cases}
    1 & \text{ if } X= \textrm{Head}\\
    0 & \text{ if } X= \textrm{Tail}
    \end{cases}$  ì´ë•Œ, $X$ëŠ” eventë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜
    

í™•ë¥  ë³€ìˆ˜ $f(X)$ì˜ í‰ê·  $\mathbb{E}[f(X)]:=\sum P(X=x)f(X=x)$

#### **Joint distribution**

 P(X,Y) : joint distribution 

 P(X), P(Y) : marginal distribution 

â†’ joint distributionì´ ì£¼ì–´ì§€ë©´
    marginal distributionì„ êµ¬í•  ìˆ˜ ìˆìœ¼ë‚˜
    ì—­ì€ ì„±ë¦½í•˜ì§€ ì•ŠìŒ

 P(Y|X=x) : conditional distribution 

YëŠ” R.V.ì´ê³ , ì¡°ê±´ë¶€ì˜ xëŠ” value

X=2ì¸ í–‰ì— ëŒ€í•œ í™•ë¥ ì€
X=2ì¼ ë•Œì˜ Yì— ëŒ€í•œ ì¡°ê±´ë¶€ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ„

![marginal dist. and joint dist.]({{'/assets/img/post_information_theory/Untitled.png' | relative_url }})

- ex
    
    
    | Y   \   X | 0 | 1 | 2 | P(Y) |
    | --- | --- | --- | --- | --- |
    | 0 | 1/4 | 1/8 | 1/8 | 1/2 |
    | 1 | 1/4 | 0 | 1/4 | 1/2 |
    | P(X) | 1/2 | 1/8 | 3/8 |  |
    

### Information and Uncertainty

- **Information : level of surprise**
    
    â†’  ë‚®ì€ í™•ë¥  = ë†’ì€ entropy
    
    ì–¼ë§ˆë‚˜ ì ì€ í™•ë¥ ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì•Œê²Œ ë˜ì—ˆì„ ë•Œ ì–¼ë§ˆë‚˜ ë†€ë¼ì›€ì„ ì£¼ëŠ”ê°€
    

    $X$: random variable, $X\in \mathcal{X}= \left\{ x_1, \ldots, x_n \right\}$

- The information of an event $X=x_i$ is defiend by $I(x_i) := \log\frac{1}{P(X=x_i)} = -\log p(x_i)$
    
    ì •ë³´ëŠ” $P(X=x_i)$ì—ë§Œ dependí•œë‹¤. ì¦‰, $I(x_i)=I(p(x_i))=-\log p(x_i)$
    
    
    ğŸ’¡ ì™œ $\log$ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ê°€?
     
    
    ì •ë³´ëŠ” ë‹¤ìŒì„ ë§Œì¡±í•¨
    
    1. í™•ë¥ ì´ ë‚®ìœ¼ë©´ ì •ë³´ê°€ ë§ê³ , í™•ë¥ ì´ ë†’ìœ¼ë©´ ì •ë³´ê°€ ì ìŒ
    2. $0 \le P(X=x_i) \le 1$
        
        $I(x_i)\ge0\quad\because0<p(x)\le1$
        
        (í™•ë¥ ì´ 0ì¸ eventëŠ” $\mathcal{X}$ì— ì†í•˜ì§€ ì•ŠìŒ)
        
    3. Fundamental Axioms (axiomatic apprach)
        
        $I(p)$ : information measure
        
        1. $I(p)\ge0$
        2. $p=1\quad \Rightarrow \quad I(p)=0$
        3. For tow independent events $P(X=x_i), P(Y=y_i)$,
            
            $I(X=x_i, Y=y_j)=I(X=x_i)+I(Y=y_j)$
            
            because, $P(X=x_i ,\ Y=y_i)=P(X=x_i)(Y=y_i)$
            
    4. The information measure $I(p)$ is continuous
    
    ![log_a function]({{'/assets/img/post_information_theory/PZLMYNWXnYn2pWQVXLCAm7rDpopTP36K_477OXTN8SD2LDdPCXXy-K3JFuJlMHfQRXRSGf7cP9WIUFRtU1-FcMDwuJdBazKIHPQ8psEu_w0CKdiMDVe82MpHK_qxUIluLzc-X_Zfm_S-NWXNkA9GVA.webp' | relative_url }})
    
    logì˜ ë°‘ì„ 2ë¡œ ì‚¬ìš©í•˜ë©´ ì •ë³´ëŸ‰ì˜ ë¹„íŠ¸ ìˆ˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŒ
    
- ë¶„í¬ì˜ information = $\mathbb{E}[I(X)] = \sum_x P(X=x)I(X=x) = -\sum_x p(x)\log p(x)$
    
    â€œí™•ë¥  ë³€ìˆ˜ë¥¼ ì• â‡” í™•ë¥  ë¶„í¬ë¥¼ ì•â€ì´ê¸° ë•Œë¬¸ì— R.V.ì˜ informationì„ ì•„ëŠ” ê²ƒì€ ë¶„í¬ì˜ informationì„ ì•„ëŠ” ê²ƒê³¼ ê°™ìŒ
    
    ë¶„í¬ì— ëŒ€í•œ informationì€ ê°ê°ì˜ $X=x_i$ì— ëŒ€í•œ informationì„ ë¬¼ì–´ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ëª¨ë“  $x$ì— ëŒ€í•œ informationì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•˜ì—¬ ë¶„í¬ì˜ informationì„ ì •ì˜í•¨
    

#### Mutual information

- For two R.V. $X\leftarrow\{x_1,x_2,\ldots,x_m\},\ Y\leftarrow\{y_1,y_2,\ldots,y_m\}$,
    
    
    (case 1) $X, Y$ are independent
    
    $Y=y_j$ì˜ ë°œìƒì´ $X=x_i$ì— ëŒ€í•œ
    ì–´ë– í•œ ì •ë³´ë„ ì œê³µí•˜ì§€ ì•ŠìŒ
    
    (case 2) $X,Y$ are fully dependent
    
    $Y=y_j$ì˜ ë°œìƒì´ $X=x_i$ì˜ ë°œìƒì„ ê²°ì •
    
    â†’ í†µì‹  ê³¼ì •ì—ì„œ Y=yë¥¼ ì „ì†¡ ë°›ìœ¼ë©´
        ì‹¤ì œë¡œ ë³´ë‚´ì§„ ê°’ Xê°€ xì˜€ìŒì„ ë³´ì¥í•  ìˆ˜ ìˆìŒ
    
    *(case 2)ì˜ ê·¸ë¦¼*
    
    ![(case 2)ì˜ ê·¸ë¦¼]({{'/assets/img/post_information_theory/Untitled%201.png' | relative_url }})
    
- $I(X;Y)$ : mutual information (average of mutual information $X=x_i, Y=y_j$) â€” for R.V.s
    
    $I(X;Y)=\sum_{x,y}p(x,y)I(x,y)$
    
    I(x,y)ëŠ” X=x,Y=yì¼ ë•Œì˜ ìƒí˜¸ ì •ë³´ëŸ‰ì„ ë‚˜íƒ€ëƒ„
    
    ì¼ë°˜ì ìœ¼ë¡œ joint informationì„ ì •ì˜í•œ ê¸°í˜¸ê°€ ì—†ê¸° ë•Œë¬¸ì—
    ê³ ì •ëœ $x_i, y_j$ì— ëŒ€í•œ mutual information í‘œê¸°ë¥¼ ;ì´ ì•„ë‹Œ ,ë¡œ ì‚¬ìš©í•¨
    
- $I(X;Y):=\sum_{x,y}p(x,y)[I(x)-I(x|y)]=\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$
    
    (ìœ„ ì‹ì˜ ì²« ë“±ì‹ì— ë‚˜ì˜¤ëŠ” $p(x,y)$ëŠ” $x=x_i,y=y_j$ì— ëŒ€í•œ $p(x_i,y_j)$ì„.
     $I(x_i;y_j)=I(x_i)-I(x_i|y_j)$) â€” for event
    
    Y=yì„ì„ ì•Œê²Œ ë¨ìœ¼ë¡œì¨ ì¤„ì–´ë“œëŠ” xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„(ì •ë³´ëŸ‰)
    
    ìƒí˜¸ ì •ë³´ëŸ‰ì´ ë†’ìœ¼ë©´ Yë¥¼ ì•Œ ë•Œ Xë¥¼ ì•Œ í™•ë¥ ì´ ë†’ì•„ì§
    
    $I(x)-I(x|y) = \log\frac{1}{p(x)}-\log\frac{1}{p(x|y)} = \log\frac{p(x|y)}{p(x)} = \log\frac{p(x,y)}{p(x)p(y)}$
    
    $I(X;Y)=I(Y;X)$
    
- $I(X;Y)= H(X)-H(X|Y)$
    
    pf) $I(X;Y)=\sum_{x,y} p(x,y)\log\frac{p(x,y)}{p(x)p(y)} \\ \qquad\qquad\quad = \sum_{x,y} p(x,y)\log \frac{1}{p(x)} - \sum_{x,y}p(x,y)\log\frac{p(y)}{p(x,y)} \\ \qquad\qquad\quad = \sum_x p(x) \log\frac{1}{p(x)} - \sum_{x,y}p(x,y)\log\frac{1}{p(x|y)} \\ \qquad\qquad\quad = H(X)-H(X|Y)$
    
    ë„¤ ë²ˆì§¸ ë“±í˜¸ ë„˜ì–´ê°ˆ ë•Œ ì•„ë˜ ì°¸ê³ 
    
    $H(X|Y)=\sum_y p(y)H(X|Y=y) \\ \qquad\qquad = \sum_y p(y)\sum_x p(x|y)\log\frac{1}{p(x|y)} \\ \qquad\qquad = \sum_{x,y}p(x|y)p(y)\log\frac{1}{p(x|y)} = \sum_{x,y} p(x,y)\log\frac{1}{p(x|y)}$
    
    - ë³€í˜•
        
        $H(X)=H(X|Y)+I(X;Y)$ ; Xì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì€ Yë¥¼ ì•Œ ë•Œ Xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„ì™€ Yë¥¼ ì•Œ ë•Œ Xë¥¼ ì•„ëŠ” ì •ë„ì˜ í•©
        

ì¢‹ì€ ì±„ë„ = ìƒí˜¸ ì •ë³´ëŸ‰ì´ ë†’ì€ ì±„ë„

- Ex 1
    1. independent : $p(x|y)=p(x) \Rightarrow I(x;y)=0$
    2. fully dependent : $p(x|y)=1 \Rightarrow I(x;y)=I(x)$
- Ex 2 (*binary symmetric channel* )
    
    
    $p$ : error rate
    
    (case 1)
    
    | X | 0 | 1 |
    | --- | --- | --- |
    | P(X) | 1 | 0 |
    
    â†’
    
    | Y | 0 | 1 |
    | --- | --- | --- |
    | P(Y) | 1-p | p |
    
    (case 2)
    
    | X | 0 | 1 |
    | --- | --- | --- |
    | P(X) | 1/2 | 1/2 |
    
    â†’
    
    | Y | 0 | 1 |
    | --- | --- | --- |
    | P(Y) | 1/2 | 1/2 |
    
    ![ìŠ¤í¬ë¦°ìƒ· 2023-09-21 183918.png]({{'/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_183918.png' | relative_url }})
    
    ![ìŠ¤í¬ë¦°ìƒ· 2023-09-21 184010.png]({{'/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_184010.png' | relative_url }})
    
    $P(Y=0)=P(Y=0|X=0)P(X=0)+P(Y=0|X=1)P(X=1)\\ \qquad\qquad\ \ =(1-p)(1/2)+p(1/2)=1/2$
    
    $P(Y=1)=P(Y=1|X=0)P(X=0)+P(Y=1|X=1)P(X=1)\\ \qquad\qquad\ \ =p(1/2)+(1-p)(1/2)=1/2$
    
    $I(X=0;Y=0)=\log\frac{p(x|y)}{p(x)}=\log\frac{p(y|x)}{p(y)}=\log\frac{1-p}{1/2}=\log 2(1-p)=\log(1-p)$
    
    $I(X=0;Y=1)=\log\frac{p(x|y)}{p(x)}=\log\frac{p(y|x)}{p(y)}=\log\frac{p}{1/2}=\log 2p=\log p$
    
    â†’ $pâ†’0$ì¼ ë•Œ $\log p\rightarrow -\infin$ ; ê°œë³„ ìƒí˜¸ ì •ë³´ëŸ‰ì€ ìŒìˆ˜ê°€ ë‚˜ì˜¤ëŠ” ê²½ìš°ë„ ìˆìœ¼ë‚˜, í‰ê· ì„ êµ¬í•˜ë©´ 0ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ë³´ì •ë¨
    

#### Conditional Mutual Information

$I(X;Y\ |\ Z)=H(X|Z)-H(X\,|\,\ Y,Z)$

- **Theorem (Chain rule for mutual information)**
$I(X_1,\ldots, X_n;Y)=\sum_{i=1}^n I(X_i;Y\ |\ X_{i-1},\ldots,X_1)$
    
    pf) $I(X_1,\ldots,X_n;Y) \\ \qquad =H(X_1,\ldots,X_n)-H(X_1,\ldots,X_n\ |\ Y) \\ \qquad = \sum_{i=1}^n H(X_i\ |\ X_{i-1},\ldots,X_1) -\sum_{i=1}^n H(X_i\ |\ X_{i-1},\ldots,X_1,Y) \\ \qquad = \sum_{i=1}^n [H(X_i\ |\ X_{i-1},\ldots,X_1) - H(X_i\ |\ {\color{red}Y},X_{i-1},\ldots,X_1)] \\ \qquad = \sum_{i=1}^n H(X_i;Y\ |\ X_{i-1},\ldots,X_1)$
    

### Entropy

The Shannon entropy of R.V. $X\sim p(x)$ is defined by

 

$H(X):=-\sum_x p(x)\log p(x)$        ; $\log\frac{1}{p(x)}$ì˜ ê¸°ëŒ€ê°’(í‰ê· )

- í™•ë¥  ë³€ìˆ˜ì˜ (í™•ë¥  ë¶„í¬ì˜) í‰ê·  information
    
    $H(X):=\mathbb{E}[I(X)]$
    
    - ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥  ë³€ìˆ˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë¹„íŠ¸ ìˆ˜
    - ë°ì´í„°ë¥¼ ìµœëŒ€ë¡œ ì••ì¶• ë•Œì˜ í¬ê¸° (ë°”ë¥¸ ë³µì›ì„ ë³´ì¥í•  ìˆ˜ ìˆì–´ì•¼ í•¨)
- Example 1.1.2 (*xì˜ í™•ë¥ ì´ uniformí•˜ì§€ ì•Šì€ ê²½ìš° code wordì˜ ê¸¸ì´ê°€ ì¤„ì–´ë“œëŠ” ìƒí™©* )
    
    ![example]({{'/assets/img/post_information_theory/Untitled%202.png' | relative_url }})
    
    | $x_i$ | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |
    | $p(x_i)$ | 1/2 | 1/4 | 1/8 | 1/16 | 1/64 | 1/64 | 1/64 | 1/64 |
    | code | 0 | 10 | 110 | 1110 | 111100 | 111101 | 111110 | 111111 |
    
    í‰ê·  ì½”ë“œ ê¸¸ì´ = entropy ; $\mathbb{E}[\textrm{code len}] = \mathbb{E}[f(X)]$
    
- Example 1.1.3
    
    
    | $X$ | 0 | 1 |
    | --- | --- | --- |
    | $P(X)$ | $p$ | $1-p$ |
    
    $H(X)=H(p)$
    
    ![1-bit Shannon entropy]({{'/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_184032.png' | relative_url }})
    

#### Joint entropy

R.V. $Z:= (X,Y) \ \sim\ p(z)=p(x,y):=P[X=x \textrm{ and } Y=y]$

$H(Z) = H(X,Y) =\sum_x\sum_y p(x,y)\log\frac{1}{p(x,y)}=\mathbb{E}_{x,y}[-\log(X,Y)]$

#### Conditional entropy

$H(Y|X)$ ; $X=x_i$ì˜ $x_i$ê°€ ë³€í™”í•  ë•Œ $Y$ì˜ ì—”íŠ¸ë¡œí”¼

ğŸ’¡ Shannon entropy : ì •ë³´ëŸ‰ì˜ í‰ê· 


$H(Y|X) := \sum P(X=x_i)H(Y|X=x_i)$

#### Mutual entropy

- $H(X;Y):=\sum_x\sum_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$
    
    Y=yì„ì„ ì•Œê²Œ ë¨ìœ¼ë¡œì¨ ì¤„ì–´ë“œëŠ” xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„(ì •ë³´ëŸ‰)
    
    $I(x)-I(x|y) = \log\frac{1}{p(x)}-\log\frac{1}{p(x|y)} = \log\frac{p(x|y)}{p(x)} = \log\frac{p(x,y)}{p(x)p(y)}$
    

#### Theorem (Chain Rule)

$H(X,Y)=H(Y)+H(X|Y)$

joint dist (X,Y)ì˜ ë¶ˆí™•ì‹¤ì„±
= Yì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± + Yë¥¼ ì•Œ ë•Œ Xì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±

- ì¦ëª…
    
    $H(X,Y) \\ =-\sum_{x,y}\log p(x,y) \\ = -\sum_{x,y} p(x,y)\log p(x)p(y|x) \\ = -\sum_{x,y} p(x,y)\log p(x) - \sum_{x,y} p(x,y)\log p(y|x) \\ = -\sum_x p(x)\log p(x) - \sum_{x,y} p(x,y)\log p(y|x) \\ = H(X)+H(Y|X)$
    

![conditional entropy]({{'/assets/img/post_information_theory/Untitled%203.png' | relative_url }})

- **Corollary (Diagram about Entropy & M.I.)**
    
    
    $H(X,Y\,|\,Z)=H(X|Z)+H(X\,|\,Y,Z)$
    
    $I(X;Y)=H(X)-H(X|Y)\\ =I(Y;X)=H(Y)-H(Y|X)$
    
    $H(X,Y)=H(X)+H(Y|X)\\ =H(Y,X)=H(Y)+H(X|Y)$
    
    $I(X;X)=H(X)$
    
    ![entropy diagram]({{'/assets/img/post_information_theory/Untitled%204.png' | relative_url }})
    

**Chain rule for entropy**

$X_1, X_2, \ldots, X_n$  are R.V.s  $\sim\ p(x_1, \ldots, x_n)$

$H(X_1, \ldots, X_n)=\sum_{i=1}^n H(X_i|X{i-1},\ldots, X_1)$

- pf 1
    
    $H(X_1,X_2)=H(X_1)+H(X_2|X_1)$
    
    $H(X_1,X_2,X_3)=H(X_1)+H(X_2,X_3|X_1)\\ \qquad\qquad\qquad\ \ = H(X_1) + H(X_2|X_1) + H(X_3|X_2,X_1)$ (by corollary)
    
    â€¦
    
    $H(X_1,\ldots,X_n)\\ \quad =H(X_1)+H(X_2,\ldots,X_n|X_1) \\ \quad = H(X_1)+H(X_2|X_1)+ H(X_3|X_2,X_1) + \cdots + H(X_n|X_{n-1},\ldots,X_1) \\ \quad = \sum_{i=1}^n H(X_i|X_{i-1},\ldots,X_1)$
    
- pf 2
    
    $p(x_1,x_2) = p(x_1)p(x_2|x_1)$
    
    $p(x_1,x_2,x_3)=p(x_1,x_2)p(x_3|x_1,x_2)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)$
    
    â€¦
    
    $p(x_1,x_2,\ldots,x_n)=\prod_{i=1}^n p(x_i|x_{i-1},\ldots,x_1)$
    
    Then,
    
    $H(X_1,\ldots,X_n)=-\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\log p(x_1,\ldots,x_n) \\ \qquad\qquad\qquad\ \ \ = -\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\log \prod_{i=1}^n p(x_i|x_{i-1},\ldots,x_1) \\ \qquad\qquad\qquad\ \ \ = -\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\sum_{i=1}^n \log p(x_i|x_{i-1},\ldots,x_1) \\ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n) \log p(x_i|x_{i-1},\ldots,x_1) \\ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n\sum_{x_1,\ldots,{\color{red} x_i}}p(x_1,\ldots,{\color{red} x_i}) \log p(x_i|x_{i-1},\ldots,x_1) \\ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n H(X_i|X_{i-1},\ldots,X_1)$
    

#### Relative entropy

- $D(p\parallel q) = \sum_i p(x_i)\log\frac{p(x_i)}{q(x_i)}$  â€” pì™€ qì˜ ì°¨ì´
    
    $\neq D(q\parallel p)$   (in general)
    
    - A measure of the distance between two distributions $p$ and $q$
    - $p$ê°€ ì¤‘ì‹¬ì´ ë˜ì–´ ë°”ë¼ë³¼ ë•Œ $q$ì˜ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ê°€
    
    R.V. distribution
    
    - $X\sim p  \; \wedge \; \widetilde{X}\sim q$
    - $\chi=\{ x_1, \ldots, x_n \}$ (the same sample space)
    â‡’ ì„œë¡œ ë‹¤ë¥¸ sample spaceì—ì„œ ì •ì˜ëœ ë³€ìˆ˜ëŠ”
        ë¹„êµ ëŒ€ìƒì´ ë  ìˆ˜ ì—†ìŒ
    
    $q(x_i):=P(\widetilde{X}=x_i)$
    
    $p(x_i):=P({X}=x_i)$
    
    ![prob dist.]({{'/assets/img/post_information_theory/Untitled%205.png' | relative_url }})
    
- $\sum p(x)\log\frac{1}{p(x)} + \sum p(x)\log\frac{p(x)}{q(x)}=\sum p(x)\log\frac{1}{q(x)}$
    
    $p$ : true distribution (unknown) $\wedge$ $q$ : approximation (guessing) of $p$
    
    If we can construct a â€œgood codeâ€ for $X\sim p$, then the average cod length = $H(p)$
    
    Instead, we use (of mis-use) a code for $X\sim q$,
    then the average code length = $\sum p(x)\log\frac{1}{q(x)}$ > $H(p)$
    
    â‡’ We need $H(p)+D(p\parallel q)$ bits on the average.
    
    í˜„ì‹¤ì—ì„œ code wordë¥¼ ë¶€ì—¬í•˜ì—¬ ì½”ë”©í•  ë•Œ í•„ìš”í•œ í‰ê·  ë¹„íŠ¸ ìˆ˜ëŠ” 
    ìµœì†Œ ìš”êµ¬ ë¹„íŠ¸ ìˆ˜ + ì˜ëª» ë¶€ì—¬í•¨ì— ì˜í•œ ì¶”ê°€ ë¹„íŠ¸ ìˆ˜
    
- ex. Code word
    
    
    | $x_i$ | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |
    | $p(x_i)$ | 1/2 | 1/4 | 1/8 | 1/16 | 1/64 | 1/64 | 1/64 | 1/64 |
    | code | 0 | 10 | 110 | 1110 | 111100 | 111101 | 111110 | 111111 |
    
    sequence of R.V.s: $X_1, X_2,\ldots,X_n \textrm{ where } X_i\sim P(X)$
    
    â‡’ ë³€ìˆ˜ì˜ ì ˆë°˜ì€ 1, ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ 2, â€¦ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê°’ì´ ë‚˜ì˜¬ ê²ƒì„ ì˜ˆìƒí•  ìˆ˜ ìˆìŒ
    
    ì£¼ì–´ì§„ ìƒí™©ì—ì„œ $H(X) = - \sum p(x)\log p(x)$ì´ê³ , ì—”íŠ¸ë¡œí”¼ëŠ” eventì— ëŒ€í•œ informationì˜ í‰ê· ì´ë©° ë™ì‹œì— ìµœì í™”ëœ (minimized) code lengthì˜ í‰ê· ì„
    
    ê²°êµ­ informationì´ì code lengthì¸ $\log\frac{1}{p(x)}$ê°€ ë³€ìˆ˜ë¥¼ ë½‘ì„ ë•Œë§ˆë‹¤ ê³„ì‚°ë¨
    â‡’ $X_1,\ldots,X_n$ ì¤‘ ì ˆë°˜ì€ code lengthê°€ 1, ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ 2, â€¦ ê°€ ë¨
        $H(X)=2$ì´ë¯€ë¡œ $X_1$ë¶€í„° $X_n$ê¹Œì§€ í‘œí˜„í•˜ê¸° ìœ„í•œ ë¹„íŠ¸ëŠ” í‰ê· ì ìœ¼ë¡œ $2\cdot n$ bitsê°€ í•„ìš”í•¨ 
    
    R.V.ì˜ sequenceë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©´ ì´ sequenceë¥¼ ì–´ëŠ ì •ë„ì˜ ê¸¸ì´ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ê°€ëŠ” ë¶„í¬ì— ì˜í•´ ì •í•´ì§€ê³ , ìµœì í™”ëœ code lengthì¸ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒì´ Shannonâ€™s entropy ê°’ì„
    
- ì„¤ëª…
    
    known $X_1, X_2,\ldots,X_n \sim p(x) \Rightarrow \log\frac{1}{p(x)}$; code length $\Rightarrow \sum p(x)\log{1}{p(x)}$
    
    í™•ë¥  ë¶„í¬ë¥¼ ì•Œë©´ ìµœì í™”ëœ code wordì˜ ê¸¸ì´ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ
    
    ë°˜ëŒ€ë¡œ fixed $p(x)$ë¥¼ ëª¨ë¥´ëŠ” ê²½ìš°, $q(x)$ë¡œì¨ code wordì˜ ê¸¸ì´ë¥¼ guessingí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•¨
    
    $q(x)\Rightarrow \log\frac{1}{q(x)}$
    
    â‡’   ìµœì í™” code word ê¸¸ì´ = ì‹¤ì œ ë‚˜ì˜¬ í™•ë¥  $\times$ ë¶€ì—¬í•œ ì½”ë“œ ê¸¸ì´ $\Rightarrow\sum q(x)\log\frac{1}{q(x)}$
    
    $p(x)$ë¥¼ ëª¨ë¥´ëŠ” ì±„ $X_1, X_2,\ldots,X_n$ì„ ë½‘ì•„ì„œ average lengthë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŒ
    
    ì‹¤ì œë¡œ êµ¬í•´ì„œ ê³„ì‚°í•˜ê²Œ ë˜ëŠ” ê°’ì€ $\sum p(x)\log\frac{1}{p(x)}$ì´ë¯€ë¡œ ë‘ ê°’ì˜ ì°¨ì´ë¥¼ ì•Œ ìˆ˜ ìˆìŒ
    
    $D(p\parallel q) :=\sum p(x)\log\frac{1}{q(x)}-\sum p(x)\log\frac{1}{p(x)}=\sum p(x)\log\frac{p(x)}{q(x)}$
    
- Mutual informationê³¼ì˜ ê´€ê³„
    
    **Mutual inforamtion**
    
    $I(x_i\,;y_j)=I(x_i)-I(x_i|y_j)=\log\frac{1}{p(x_i)}-\log\frac{1}{p(x_i|y_i)}=\log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}$
    
    $I(X;Y)\;=\sum p(x_i,y_j)I(x_i\,;y_j)\\ \qquad\qquad =\sum p(x_i,y_j)\log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}=D(p(x,y)\parallel p(x)p(y))$ â€” joint || product
    
    $\therefore I(X;Y)=0 \Leftrightarrow p(x,y)=p(x)p(y) \textrm{ for } \forall x,y$  â€” independent
    
    â‡’ joint ë¶„í¬ì™€ productì˜ ë¶„í¬ì˜ ì°¨ì´ê°€ 0
    


ğŸ’¡ $0\log\frac{0}{0} \rightarrow 0$      $0\log\frac{0}{g}\rightarrow 0$       $p\log\frac{p}{0}\rightarrow\infin$ 

define $p(x), q(x)$ for $\exist \, x\in\chi$ s.t. $p(x)>0, q(X)=0$
then $D(p\parallel q)=\infin$


- ex. $D(p\parallel q) \neq D(q\parallel q)$
    
    
    $\chi=\{0,1\}$
    
    $H(p)=-\sum p(x)\log p(x) \\ \qquad\;\;= -(1-r)\log(1-r) \\ \qquad\qquad-r\log r$
    
    $H(p)\leftarrow$
    
    $H(q)\leftarrow$
    
    $H(q) = -(1-s)\log(1-s)=s\log s$
    
    | x | 0 | 1 |
    | --- | --- | --- |
    | p(x) | 1-r | r |
    | q(x) | 1-s | s |
    
    For $r=\frac{1}{2},s=\frac{1}{4}$, 
    
    $D(p\parallel q)=\sum p(X)\log\frac{p(x)}{q(x)}=\frac{1}{2}\log\frac{1/2}{3/4}+\frac{1}{2}\log\frac{1/2}{1/4}\approx 0.2075$
    
    and $D(q\parallel q)=\frac{3}{4}\log\frac{3/4}{1/2}+\frac{1}{4}\log\frac{1/4}{1/2}\approx 0.1887$
    

#### Jensenâ€™s inequality

convex ë˜ëŠ” concaveë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œëŠ” ì¡°ê±´ì„ ë§Œì¡±í•˜ë„ë¡ êµ¬ê°„ì„ ì„¤ì •í•´ ì£¼ì–´ì•¼ í•¨

**Def: Convex**

$f:(a,b)\rightarrow\mathbb{R}$ is convex if for every $x_1, x_2\in (a,b),$

$f(\lambda x_1+(1-\lambda)x_2)\le \lambda f(x_1)+(1-\lambda)f(x_2)$ where $0\le \forall\lambda \le 1$

(êµ¬ê°„ ë‚´ x=aì¼ ë•Œ, ê³¡ì„  ìœ„ point â‰¤ êµ¬ê°„ ë‚´ x=aì¼ ë•Œ, ì§ì„  ìœ„ì˜ point)

**NOTE:** $\lambda x_1 + (1-\lambda)x_2 \Rightarrow$ $x_1$,$x_2$ë¥¼ $\lambda:1-\lambda$ë¡œ ë‚´ë¶„í•˜ëŠ” ì 

**Def: Concave**

$f:(a,b)\rightarrow\mathbb{R}$ is concave if $-f$ is convex

![convex]({{'/assets/img/post_information_theory/Untitled%206.png' | relative_url }})

convex

![concave]({{'/assets/img/post_information_theory/Untitled%207.png' | relative_url }})

concave

- **Theorem**
$f\in C^2$ ($f',f''$ exist and are continuous) and $f''(x)\ge0$ on $(a,b)$, then $f$ is convex
    
    pf: $f(x)=f(x_0)+f'(x_0)(x-x_0)+(f''(x^*)/2)(x-x_0)^2,\quad x^*\in[x_0,x]$
    
    $(f''(x^*)/2)(x-x_0)^2$ì´ $y=ax^2+bx+c,a>0$ ê¼´ë¡œ convex
    
    $x_1, x_2$ : arbitrarily given points
    
    Let $x_0:=\lambda x_1 + (1-\lambda)x_2\;\; (0\le\lambda\le1)$
    
    $f(x_1)=f(x_0)+f'(x_0)(x_1-x_0)$
    $\qquad\qquad+(f''(x_1^*)/2)(x_1-x_0)^2$
    
    ($x_1^*$ lines between $x_0, x_1$)
    
    ![Jenson]({{'/assets/img/post_information_theory/Untitled%208.png' | relative_url }})
    
    $f(x_2)=f(x_0)+f'(x_0)(x_2-x_0)+(f''(x_2^*)/2)(x_2-x_0)^2$
    
    ($x_2^*$ lines between $x_0, x_1$)
    
    Since $f''(x_1^*)\ge0,f''(x_2^*)\ge0$,
    
    1. $f(x_1)\ge f(x_0)+fâ€™(x_0)(x_1-x_0)$
    2. $f(x_2)\ge f(x_0)+f'(x_0)(x_2-x_0)$
    
    $\lambda f(x_1)+(1-\lambda)f(x_2)\\ \quad \ge \lambda f(x_0)+\lambda f'(x_0)(x_1-x_0) +(1-\lambda)f(x_2)+(1-\lambda)f'(x_0)(x_2-x_0)\\ \quad = f(x_0)f'(x_0)[\lambda x_1 -\lambda x_0+(1-\lambda)x_2 -(1-\lambda)x_0]\\ \quad = f(x_0) + f'(x_0)[\lambda x_1+(1-\lambda)x_2-x_0] \\ \quad =f(x_0)+f'(x_0)[x_0-x_0] = f(x_0)\\ \quad = f(\lambda x_1 + (1-\lambda)x_2)$
    
    $\therefore f$ is convex
    
- **Thm: Jensenâ€™s inequality**
$f$ : convex and $X$ : R.V. then $\mathbb{E}[f(X)]\ge f(\mathbb{E}[X])$
    
    pf: We use mathematical induction.
    
    Let $\chi_2 =\{ x_1,x_2 \},\ldots,\chi_k=\{ x_1,x_2,\ldots,x_n\} \textrm{ where } |\chi_i|=i \textrm{ for all } 2\le i \le k$
    
    case 1)  $\chi = \{x_1, x_2\}$
    
    $\;\mathbb{E}[f(X)] = p_1 f(x_1) + p_2 f(x_2)$
    
    $\qquad\qquad = p_1f(x_1)+(1-p_1)f(x_2)$
    
    | $x$ | $x_1$ | $x_2$ |
    | --- | --- | --- |
    | $p(x)$ | $p_1$ | $p_2$ |
    
    $\qquad\qquad\quad\; \ge f(p_1x_1 + (1-p_1)x_2)$ since $f$ is convex
    
    $\qquad\qquad =f(\mathbb{E}[X])$
    
    case 2) Suppose that Theorem is true for $|\chi|\le k-1$
    
    $X\sim\{$ $\chi:=\{x_1,\ldots,x_k\}$ and $p_i:=P[X=x_i] \;\ (i=1,\ldots,k)\ \}$ are given.
    
    Define a distribution $X'$ with $\chi_{k-1}:=\{x_1,\ldots,{\color{red}x_{k-1}}\}$ 
    where $p_i':=P(X=x_i)=\frac{p_i}{1-p_k} \;\ (i=1,\ldots,k-1)$
    
    $\,\mathbb{E}[f(X)]=\sum p_i f(x_i) = p_k f(x_k)+\sum_{i=1}^{k-1}p_i f(x_i)$
    
    $\qquad\qquad=p_k f(x_k)+(1-p_k)\sum_{i=1}^{k-1}\frac{p_i}{1-p_k}f(x_i)$
    
    $\qquad\qquad=p_kf(x_k)+(1-p_k)\sum_{i=1}^{k-1}p_i' f(x_i)$
    
    $\qquad\qquad=p_k f(x_k) + (1-p_k)\mathbb{E}[f(X')]$
    
    $\qquad\qquad\ge p_k f(x_k)+(1-p_k)f\left(\sum_{i=1}^{k-1} p_i' x_i\right)$ for $|\chi|\le k-1$
    
    $\qquad\qquad\ge f\left(p_k x_k + (1-p_k)\sum_{i=1}^{k-1} p_i' x_i\right)$ by $f$ : convex
    
    $\qquad\qquad=f\left(p_k x_k + (1-p_k)\sum_{i=1}^{k-1}\frac{p_i}{1-p_k}x_i\right)$
    
    $\qquad\qquad=f\left(\sum_{i=1}^k p_i x_i\right)$
    
    $\qquad\qquad=f(\mathbb{E}[X])$
    
- Theorem: Information inequality
$D(p\parallel q)\ge 0$    ,    $I(X;Y)\ge0$    ,   $H(X)\le\log|\chi|$
    
    $p(x), q(x)$ are p.m.f. (probability mass function) on $x\in\chi$
    
    1. $D(p\parallel q)\ge 0$  (equality holds $\iff$ $p(x)\equiv q(x)$
        - pf
            
            $supp(f)$ : support of $f$
            
            $supp(f) := \overline{\{x|f(x)>0\}}=\{x|f(x)>0\}$ where $x$ is discrete
            
            Let $A:=\{x|p(x)>0\}$  (i.e. $A:= supp(p)$)
            
            $\;-D(p\parallel q) = -\sum_{x\in\chi} p(x) \log\frac{p(x)}{q(x)} = -\sum_{x\in A} p(x)\log\frac{p(x)}{q(x)}$ 
            
            $\qquad\qquad\quad= \sum_{x\in a} p(x) \log\frac{q(x)}{p(x)}\;\;\cdots\;\; \mathbb{E}\left[\log\frac{q(X)}{p(X)}\right]$
            
            $\qquad\qquad\quad\le\log\left[ \sum_{x\in A} p(x)\frac{q(x)}{p(x)} \right]$ by Jensenâ€™s inequality
            
            $\qquad\qquad\quad=\log\left[ \sum_{x\in A} q(x) \right]$
            
            $\qquad\qquad\quad\le\log\left[\sum_{x\in\chi} q(x)\right]=\log 1=0$
            
            $\therefore\ D(p\parallel q)\ge0$
            
    2. $I(X;Y)\ge0$  (equality holds $\iff$ $X,Y$ are independent
        - pf
            
            $I(X;Y)=D\left(p(x,y)\parallel p(x)q(y)\right)\ge0$
            
    3. $X$: R.V. $x\in\chi$,  $H(X)\le\log|\chi|$    (entropyëŠ” distributionì´ uniformì¼ ë•Œ ìµœëŒ€)
        - pf
            
            Let $u(x):=\frac{1}{|\chi|}$  (constant function on $\chi$)
            
            $u(x)$ is uniform p.m.f. over $\chi$.
            
            Then for any R.V. $X\sim P$
            
             $D(p\parallel q)=\sum p(x)\log\frac{p(x)}{u(x)}$
            
            $\qquad\qquad=\sum p(x)\log|\chi|+\sum p(x)\log p(x)$
            
            $\qquad\qquad=\log|\chi|-\sum p(x)\log\frac{1}{p(x)}$     since $\log|\chi|$ is constant and $\sum p(x)=1$
            
            $\qquad\qquad=\log|\chi|-H(X)\ge0$
            
            $\therefore H(X)\le\log|\chi|$
            
            â‡’ entropy of $X$ is smaller than Entropy of uniform disribution
            
- Theorem: Conditioning reduces entropy
$H(X|Y)\le H(X)$
    
    pf: $0\le I(X;Y)=H(X)-H(X|Y)$
    
    $\therefore\ H(X)\ge H(X|Y)$
    
    ex. $H(X) = H\left( \frac{1}{8},\frac{7}{8} \right)$  or 
    
    $\qquad\quad = H(p) \textrm{\,\ where\,\ } p=\frac{1}{8}, q=1-p$
    
    $H(X|Y=2)=H\left(\frac{1}{2},\frac{1}{2}\right)=1$
    
    $\therefore H(X)\ll H(X|Y=2)$ 
    
    â‡’ ì •ë¦¬ì— ëª¨ìˆœì¸ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ,
    
    ì •ë¦¬ëŠ” averageì— ëŒ€í•œ ë‚´ìš©ì´ê³ 
    
    ì˜ˆì œëŠ” single term $Y=2$ë¼ëŠ” 
    
    íŠ¹ì • ìƒí™©ì— ëŒ€í•œ ê²°ê³¼ì„
    
    | $Y$ \ $X$ | 1 | 2 | $P(Y)$ |
    | --- | --- | --- | --- |
    | 1 | 0 | 3/4 | 3/4 |
    | 2 | 1/8 | 1/8 | 1/4 |
    | $P(X)$ | 1/8 | 7/8 |  |
- Theorem: independence bound
$H(X_1,\ldots, X_n)\le\sum_{i=1}^n H(X_i)$    (equality â‡” independence)
    
    pf: By the Chain rule,
    
    $\;H(X_1,\ldots,X_n)=\sum_{i=1}^n H(X_i|X_{i-1},\ldots,X_1)$
    
    $\qquad\qquad\qquad\quad\le \sum_{i=1}^n H(X_i)$  by previous theorems
    

## Log-sum inequality and its Applications

### Theorem: Log-sum inequality

- For non-negative numbers $a_1,\ldots,a_n$ and $b_1,\ldots, b_n$,
$\sum_{i=1}^n a_i\left(\log\frac{a_i}{b_i}\right) \ge \left(\sum_{i=1}n a_i\right)\cdot\log\left( \sum_{i=1}^n a_i/\sum_{i=1}^n b_i \right)$      (equality â‡” $\frac{a_i}{b_i}$ is constant)
    
    pf: case 1) For $a_i\ge0, b_i>0$.
    
    Let $f(t):= t\log t$, then $f(t)$ is convex.
    
    
    ğŸ’¡ $t\times a>t\textrm{\;\; as\;\;} t\rightarrow\infin \textrm{\;\; where\;\;} a>1$
    
    
    Since $(\log t)'=\left( \frac{\log_e t}{\log_e 2} \right)'=\frac{1}{\log_e 2}\cdot\frac{1}{t}$,
    
    $f'(t)=\log t+\log e$
    
    $f''(t)=\frac{1}{\log_e 2}\cdot\frac{1}{t}=\frac{\log e}{t} > \frac{1}{t}>0$
    
    $\therefore\ f''(t)>0 \;\Rightarrow \;f$ is convex
    
    ![Graph shape]({{'/assets/img/post_information_theory/Untitled%209.png' | relative_url }})
    
    By Jensenâ€™s inequality $\mathbb{E}[f(X)] \ge f(\mathbb{E}[X])$ for convex $f$,
    
    $\sum \alpha_i f(t_i) \ge f\left(\sum \alpha_i t_i\right)$
    
    for $X\leftarrow t_i, P_i\leftarrow \alpha_i$ where $\sum \alpha_i = \sum p_i =1, \alpha_i\ge0, p_i\ge0$
    
    Set $\alpha_i = \frac{b_i}{\sum_j b_j}$ because we need to property $\sum_i \alpha_i=\sum_i\frac{b_i}{\sum_j b_j} = \frac{1}{\sum_j b_j}\left(\sum_i b_i\right) = 1$.
    
    set $t_i=\frac{a_i}{b_i}$
    
    Then we obtain $\sum_i \alpha_i f(t_i)=\sum_i\frac{b_i}{\sum_j b_j}\,f\!\left(\frac{a_i}{b_i}\right) = \sum_i\frac{b_i}{\sum_j b_j}\frac{a_i}{b_i}\log\frac{a_i}{b_i}$
    
    $\;\,\,\qquad\qquad\qquad\qquad\qquad = \sum_i\frac{a_i}{\sum_j b_j}\log\frac{a_i}{b_i} = \frac{1}{\sum_j b_j}\sum_i a_i\log\frac{a_i}{b_i}$ 
    
    $\;\,\,\qquad\qquad\qquad\qquad\qquad \ge f\left( \sum_i \alpha_i t_i \right)$          by Jensenâ€™s inequality
    
    $\;\,\,\qquad\qquad\qquad\qquad\qquad = \left[ \sum_i \left( \frac{b_i}{\sum_j b_j} \right)\frac{a_i}{b_i} \right] \log \left[ \sum_i \left( \frac{b_i}{\sum_j b_j} \right)\frac{a_i}{b_i} \right]$ 
    
    $\;\,\,\qquad\qquad\qquad\qquad\qquad = \frac{1}{\sum_j b_j} \sum_i a_i \log \left( \frac{1}{\sum_j b_j}\sum_i a_i \right)$ 
    
    In this inequality, $\frac{1}{\sum_j b_j}\sum_i a_i\log\frac{a_i}{b_i}  \ge  \frac{1}{\sum_j b_j} \sum_i a_i \log \left( \frac{1}{\sum_j b_j}\sum_i a_i \right)$
    
    $\therefore\; \sum_i a_i\log\frac{a_i}{b_i}  \ge  \sum_i a_i \log \left( \frac{\sum_i a_i}{\sum_i b_i}\right)$
    

#### Applying Log-sum inequality

- $D(p\parallel q)\ge0$  where $p ,q$ are p.m.f.
    
    Relatibe entropy $D(p\parallel q):=\sum_x p(x)\log\frac{p(x)}{q(x)}\ge\sum_x p(x)\log\frac{\sum_x p(x)}{\sum_x q(x)} =\sum_x p(x)\log1=0$
    
    $\therefore \; D(p\parallel q)\ge0$  (equality â‡” $\frac{p(x)}{q(x)}=1$ for all $x$)
    
- **Thm: Convexity of relative entropy**
 $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)\le \lambda D(p_1\parallel q_1) + (1-\lambda)D(p_2\parallel q_2)$
    
    $D(p\parallel q)$  is convex in the pair $(p,q)$ where $p,q$ are p.m.f.
    i.e. If $(p_1, q_1), (p_2,q_2)$ are two pairs of p.m.f. and $0\le \lambda\le1$, 
    then $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)\le \lambda D(p_1\parallel q_1) + (1-\lambda)D(p_2\parallel q_2)$
    
    pf:  First, we check $\lambda p_1+(1-\lambda) p_2$ and $\lambda p_1+(1-\lambda) p_2$ are p.m.f.
    
    Itâ€™s trivial!
    
    Next, $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)$
    
    $\qquad := \sum_x\left[ \lambda p_q(x)+(1-\lambda) p_2(x) \right]\log\frac{\lambda p_1+(1-\lambda)p_2}{\lambda q_1+(1-\lambda)q_2}$
    
    $\qquad\le \sum_x\left[\sum_i \left\{ \lambda p_1+(1-\lambda)p_2\right\} \log\frac{\lambda p_1+(1-\lambda)p_2}{\lambda q_1+(1-\lambda)q_2}\right]$
    
    $\qquad=\sum_x\left[  \lambda p_1(x)\log\frac{ \lambda p_1(x) }{ \lambda q_1(x) } + (1-\lambda)p_2(x)\log\frac{ (1-\lambda) p_2(x) }{ (1-\lambda) q_2(x) }  \right]$
    
    $\qquad=\lambda\sum_xp_1(x)\log\frac{p_1(x)}{q_1(x)}+(1-\lambda)\sum_xp_2(x)\log\frac{p_2(x)}{q_2(x)}$
    
    $\qquad=\lambda D(p_1\parallel q_1)+(1-\lambda) D(p_2\parallel q_2)$ 
    
    ì¦‰, ë‘ p.m.f.ë¥¼ combinationí•œ ë’¤ relative entropyëŠ” ê°ê°ì˜ relative entropyì˜ í‰ê· ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìŒ
    
- **Thm. Concavity of entropy**
$H(p)$ is a concave function of p.m.f. $p$
    
    pf 1: Let $p(x)$ is p.m.f., $x\in \chi=\{x_1,\ldots,x_n\}$  with  $|\chi|=n$
    
    and $u(x):=\frac{1}{|\chi|}$ is uniform distribution
    
    $H(p)=\log|\chi|-D(p\parallel u)$
    
    
    ğŸ’¡ $D(p\parallel u)=\sum p(x)\log\frac{p(x)}{u(x)} = \sum p(x)\log\frac{1}{u(x)}-\sum p(x)\log\frac{1}{p(x)}$
    
    
    Since $\log\frac{1}{u(x)}=\log|\chi|$ is constant, $D(p\parallel u)=\log|\chi|-H(p)$.
    
    Thus $D(p\parallel u)$ is convex.
    
    â‡’ $-D(p\parallel u)$ is concave
    
    $H(p)=\log|\chi|-D(p\parallel u)$
    
    $\therefore\ H(p)$ is concave
    
    pf 2: 
    

## Channel Capacity

- í•œë²ˆì— (ì¼ì • ì‹œê°„ ë‚´ì—) ìµœëŒ€ë¡œ ì „ì†¡í•  ìˆ˜ ìˆëŠ” ë°ì´í„°


ğŸ’¡ Communication channel

outputì´ í™•ë¥ ì ìœ¼ë¡œ inputì— ì˜ì¡´í•˜ëŠ” ì‹œìŠ¤í…œ


- Ex (*noisy four-symbol channel )*
    
    
    error rate $p=1/2$
    
    (case 1) $P(X=x)$ê°€ uniformí•œ ê²½ìš°
    
    | X  \  Y | 1 | 2 | 3 | 4 | P(X) |
    | --- | --- | --- | --- | --- | --- |
    | 1 | 1/8 | 1/8 | 0 | 0 | 1/4 |
    | 2 | 0 | 1/8 | 1/8 | 0 | 1/4 |
    | 3 | 0 | 0 | 1/8 | 1/8 | 1/4 |
    | 4 | 1/8 | 0 | 0 | 1/8 | 1/4 |
    | P(Y) | 1/4 | 1/4 | 1/4 | 1/4 |  |
    
    Yë¥¼ ë°›ì•˜ì„ ë•Œ Xë¥¼ í•˜ë‚˜ë¡œ ê²°ì •í•  ìˆ˜ ì—†ìŒ
    
    â†’ decoding ì‹¤íŒ¨. $I(X;Y)\neq1$
    
    (case 2) $P(X=x)$ê°€ uniformí•˜ì§€ ì•Šì€ ê²½ìš°
    
    | X  \  Y | 1 | 2 | 3 | 4 | P(X) |
    | --- | --- | --- | --- | --- | --- |
    | 1 | 1/4 | 1/4 | 0 | 0 | 1/2 |
    | 2 | 0 | 0 | 0 | 0 | 0 |
    | 3 | 0 | 0 | 1/4 | 1/4 | 1/2 |
    | 4 | 0 | 0 | 0 | 0 | 0 |
    | P(Y) | 1/4 | 1/4 | 1/4 | 1/4 |  |
    
    noisy n-symbol channel
    
    ![n-symbol channel]({{'/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_183927.png' | relative_url }})
    
    Yë¥¼ ë°›ìœ¼ë©´ Xë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŒ
    
    â†’ decoding ì„±ê³µ. $I(X;Y)=1$
    
    4ê°œì˜ ì •ë³´ë¥¼ ì „í•  ìˆ˜ ìˆëŠ”
    2ë¹„íŠ¸ ì±„ë„ì„ ê°€ì§€ê³  ìˆì§€ë§Œ,
    
    ì‹¤ì œë¡œëŠ” 1ê³¼ 3 ë‘ ì •ë³´ë§Œ
    ì£¼ê³ ë°›ëŠ” 1ë¹„íŠ¸ ì±„ë„ë¡œ ì‚¬ìš©í•¨
    
    â†’ ì±„ë„ ìš©ëŸ‰ $C=1$ bit
    
    ë” ì¢‹ì€ ë°©ë²•ì´ ìˆë‹¤ë©´ C ìˆ˜ì •
    
- $C:=\max I(X;Y)$
    
    The capacity is the maximum tate at which we can send information over the channel
    
    ì±„ë„ì˜ ê¸°ë³¸ì ì¸ ì„¤ì •(ì…ë ¥ ê°€ëŠ¥í•œ ê°’ë“¤ì˜ ë²”ìœ„, error rate ë“±)ì€ ì½”ë”©ì„ í†µí•´ ë°”ê¿€ ìˆ˜ ì—†ìŒ
    
    ì½”ë”©ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥í•œ ê°’ì€ $p(x)$
    
    ì´ë¥¼ ì¡°ì ˆí•˜ì—¬ ê°€ì¥ í° $C$ë¥¼ ì–»ëŠ” distributionìœ¼ë¡œ ì±„ë„ì„ êµ¬ì„±í•´ì•¼ íš¨ìœ¨ì´ ê°€ì¥ ì¢‹ìŒ
    

### Data Processing inequality

#### Data flow

![noisy channel]({{'/assets/img/post_information_theory/Untitled%2010.png' | relative_url }})

- **Def: Markov Chain**
$X, Y, Z$ : R.V.s 
$X\rightarrow Y\rightarrow Z$ (i.e. Markov chain) $\Leftrightarrow p(x,y,z)=p(x)p(y|x)p(z|y)$
and we also can denote as $X\longleftrightarrow Y\longleftrightarrow Z$
    
    
    ğŸ’¡ For $X_1, X_2,\ldots,X_n$,
    $X_1\rightarrow X_2\rightarrow \cdots X_{n-1}\rightarrow X_n \rightarrow\cdots$ is Markov chain when 
    $P(X_n|X{n-1},\ldots,X_1)=P(X_n|X_{n-1})$
    
    
    Def â‡” $Z$ depends only on $Y$ â€” (1) 
    
     â‡” $Z$ is conditionally independent of $X$ given $Y$  â€” (2)
    
    i.e. $P(X,Z\ |\ Y)=P(X|Y)P(Z|Y)$
    
    
    ğŸ’¡ Conditional probability
    $p(x,y)=p(x)p(y|x)$
    $p(x,y,z)=p(x)p(y,z\,|\,x)=p(x)p(y|z)p(z\,|\,y,x)$
    â€¦
    $p(x_1, x_2,\ldots,x_n)= \prod_{i=1}^n p(x_i\,|\,x_{i-1},\ldots,x_1)$
    
    
    (1) For Markov chain $X\rightarrow Y\rightarrow Z$,
    
    we need to show $p(x,y,z)=p(x)p(y|x)p(z\,|\,y,x)=p(x)p(y|x)p(z|y)$
    
    By the definition, $p(z|\ y,x)=p(z|y)$ is trivial.
    
    
    ğŸ’¡ $Z$ depends on $X$ only through $Y$
    
    ì¦‰, $X$ì˜ ë³€í™”ëŠ” $Y$ë¥¼ í†µí•´ì„œë§Œ $Z$ì— ë°˜ì˜ë¨
    
    â‡” $X$ì™€ $Z$ê°€ dependí•˜ì§€ë§Œ, $Y$ë¥¼ í†µí•´ì„œë§Œ dependí•˜ë¯€ë¡œ $Y$ì—ë§Œ dependí•˜ ë³´ì„
    
    â‡” $Z$ê°€ $X$ë¡œë¶€í„° dependí•˜ëŠ” ì •ë„ê°€ ì´ë¯¸ $Y$ì— ë°˜ì˜ë˜ì–´ ìˆìŒ
    
    
    
    Thus, $p(x,y,z)=p(x)p(y|x)p(z|y)$
    
    (2) Given $Y$, 
    
    $p(x,z\ |y)=p(x,y,z)/p(y)$  and by the property of Markov chain,
    
    $\qquad\qquad\ =[p(x)p(y|x)p(z|y)]/{p(y)}=[p(x,y)/p(y)]p(z|y)=p(x|y)p(z|y)$
    
    Therefore, $X,Z$ are independent given $Y$
    
    By (1), (2),    $X\rightarrow Y\rightarrow Z$  $\iff$  $X,Z$ are independent given $Y$
    
    and it also can be represented as $Z,X$ are independent given $Y$
    
    then, $\iff \ Z\rightarrow Y\rightarrow X$ holds.
    
    We may write $X\longleftrightarrow Y\longleftrightarrow Z$
    
    Then if $Z=f(Y)$, $X\rightarrow Y\rightarrow f(Y)$?
    
    ì§ê´€ì ìœ¼ë¡œ ìƒê°í•  ë•Œ, $Z$ëŠ” $Y$ê°€ ê²°ì •ë˜ëŠ” ìˆœê°„ ê°™ì´ ê²°ì •ë¨
    
    i.e., $Y$ determines $Z$  completely.
    
    It means $Z$ depends only $Y$ and is the definition of Markov chain.
    
    So, this chain is holds.
    


ğŸ’¡ R.V.s $X, Y, Z$ê°€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ Markov chainì„

- $P(Z|Y)=P(Z|\ Y,X)$
- $P(X,Y,Z)=P(X)P(Y|X)P(Z|Y)$
- $P(Z|Y)P(X|Y)=P(X,Z\ |Y)$


- ex
    
    $X\xrightarrow[channel\,1]{BSC} Y\xrightarrow[channel\,2]{BSC} Z$,
    
    error rate of channel1 = $p_1$, error rate of channel2 = $p_2$
    
    **BSC channel 1**
    
    Input data
    
    given $X=0$
    
    given $X=1$
    
    Joint distribution
    
    | $X$ | 0 | 1 |
    | --- | --- | --- |
    | $P(X)$ | 3/4 | 1/4 |
    
    | $Y$ | 0 | 1 |
    | --- | --- | --- |
    | $P(Y|X=0)$ | $1-p_1$ | $p_1$ |
    
    | $Y$ | 0 | 1 |
    | --- | --- | --- |
    | $P(Y|X=1)$ | $p_1$ | $1-p_1$ |
    
    | $X$ \ $Y$ | 0 | 1 | $P(X)$ |
    | --- | --- | --- | --- |
    | 0 | $\frac{3}{4}(1-p_1)$ | $\frac{3}{4}p_1$ | 3/4 |
    | 1 | $\frac{1}{4}p_1$ | $\frac{1}{4}(1-p_1)$ | 1/4 |
    | $P(Y)$ | $\frac{3}{4}-\frac{1}{2}p_1$ | $\frac{1}{4}-\frac{1}{2}p_1$ |  |
    
    Check: $P(X|Y)$
    
    given $Y$     (ë¹ˆ ì¹¸ ì±„ìš°ê¸°)
    
    given $Y=0$
    
    given $Y=1$
    
    | $Y$ | 0 | 1 |
    | --- | --- | --- |
    | $P(X|Y=0)$ |  |  |
    
    | $Y$ | 0 | 1 |
    | --- | --- | --- |
    | $P(X|Y=1)$ |  |  |
    
    $Z=f(Y):=2Y-1$ ë¼ë©´
    
    Joint distribution
    
    $p_1=1/4$ì¼ ë•Œ $Y$ì˜
    
    input data
    
    $P(Y=0)=5/8$
    
    $P(Y=1)=3/8$
    
    | $Y$ \ $Z$ | -1 | 1 | $P(Y)$ |
    | --- | --- | --- | --- |
    | 0 | 5/8 | 0 | 5/8 |
    | 1 | 0 | 3/8 | 3/8 |
    | $P(Z)$ | 5/8 | 3/8 |  |
    
    $Y$ì˜ ë¶„í¬ë¥¼ ì•Œë©´ $Z$ì˜ ë¶„í¬ê°€ fixë¨
    
    ($X$ì˜ ë¶„í¬ë¥¼ ëª¨ë¥´ê³  $Y$ ë¶„í¬ë§Œ ì•Œì•„ë„ ê³ ì •ë¨, ê°œì…í•  ì—¬ì§€ê°€ ì—†ì–´ì§)
    

#### Theorem: Data Processing Inequality

- $X\rightarrow Y\rightarrow Z \ \Longrightarrow\ I(X;Y)\ge I(X;Z)$
    
    
    ğŸ’¡ **Probability**
        $p(x_1,\ldots,x_n)=\prod_{i=1}^n p(x_i\,|\,x_{i-1},\ldots,x_1)$

    **Entropy**
        $H(x_1,\ldots,x_n)=\sum_{i=1}^n H(x_i\,|\,x_{i-1},\ldots,x_1)$
                                   $=H(x_1)+H(x_2|x_1)+H(x_3\,|\,x_2,x_1)+\cdots$
    
    **Mutual information**
        $I(X;Y)=H(X)-H(X|Y)$
        $I(X_1,\ldots,X_n\;;\,Y)=\sum_{i=1}^n I(X_i;Y\;|\;X_{i-1},\ldots,X_1)$
    
    
    
    pf: Since $I(X;\ Y,Z)=I(Y,Z\; ;X)=I(Z,Y\; ;X)$
    
    $\qquad\qquad\qquad\qquad\;\;\,=I(Z;X)+I(Y;X\;|Z)=I(X;Z)+I(X;Y\;|Z)$
    
    $\qquad\qquad\qquad\qquad\;\;\,=I(X;\ Z,Y)=I(X;Y)+I(X;Z\;|Y)$,
    
    $I(X;Z)+I(X;Y\ |Z)=I(X;Y)+I(X;Z\ |Y)$.
    
    $I(X;Z\ |Y)=0$  by property of Markov Chain
    
    - $X,Z$ are conditionally independent given $Y$  $\Rightarrow\ I(X;Z\ |Y)=0$
    
    $I(X;Y)=I(X;Z)+I(X;Y\ |Z)\ge I(X;Z)$                            $\because I(A;B)\ge0$
    

ğŸ’¡ ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³¼ ë•Œ, $X$ë¡œë¶€í„° $Y$ë¥¼ ì•„ëŠ” ê²ƒì´ $X$ë¡œë¶€í„° $Z$ë¥¼ ì•„ëŠ” ê²ƒë³´ë‹¤ ì‰¬ì›€ (ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ë” ë§ìŒ). ì´ë•Œ ë“±í˜¸ê°€ ì„±ë¦½í•œë‹¤ëŠ” ê²ƒì€ $Y$ë¥¼ ê±°ì³ $Z$ì—ì„œ $X$ê°€ ì˜í–¥ì„ ë¯¸ì§€ëŠ” ì •ë„ì— ì†ì‹¤ì´ ì—†ìŒì„ ì˜ë¯¸í•¨


- Corollary
$Z=g(Y)\ \Longrightarrow\ I(X;Y)\ge I\left(X;g(Y)\right)$
    
    pf: $X\longrightarrow Y\longrightarrow g(Y)$
    
    $Y\rightarrow g(Y)$ëŠ” ìµœëŒ€ 1-1 ëŒ€ì‘ ê´€ê³„ì´ë¯€ë¡œ $g(Y)$ì˜ ì •ë³´ëŠ” $Y$ë³´ë‹¤ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì˜ ë²”ìœ„ê°€ ì‘ìŒ
    
    â‡’ trivial!
    
- Corollary
$X\longrightarrow Y\longrightarrow Z\;\Longrightarrow\; I(X;Y\ |Z)\le I(X;Y)$
    
    pf: Since $I(X;\ Y,Z)=I(X;Y)+I(X;Z\ |Y)=I(X;Y)$
    
     and  $I(X;\ Y,Z)=I(X;Z)+I(X;Y\ |Z)$,
    
    $\Rightarrow\ I(X;Y)=I(X;Z)+I(X;Y\ |Z)\ge I(X;Y\ |Z)$      $\because I(X;Z)\ge0$
    


ğŸ’¡ $X\longrightarrow Y\longrightarrow Z$
The dependency of $X$ and $Y$ is decreased (or unchanged) by the observation of a downstream (to receiver) R.V. $Z$
i.e., $I(X;Y)\ge I(X;Y\ |Z)$


- ex
    
    case1) $X_1, X_2, X_3$ : independent R.V.s
    
    case2) $X_1$ : dice {1, 2, â€¦ , 6}, $X_2$ : biased coin {0, 1}, $X_3=2X_2-1$
    
    | $X_1$ | 1 | â€¦ | 6 |
    | --- | --- | --- | --- |
    | $P(X_1)$ | 1/6 |  | 1/6 |
    
    | $X_2$ | 0 | 1 |
    | --- | --- | --- |
    | $P(X_2)$ | $1-\theta$ | $\theta$ |
    
    $\theta:=\frac{1}{X_1}\in\left\{ 1, \frac{1}{2},\frac{1}{3},\ldots,\frac{1}{6} \right\}$
    
    $X_1=2$    â‡’ fair coin toss
    
    Which case is a Markov chain as $X\longrightarrow Y\longrightarrow Z$?
    
    Definition :  $p(x,y,z)=p(x)p(y|x)p(z|y)$
    
    and we can lead $P(Z|Y)P(X|Y)=P(X,Z\ |Y)$
    
    â‡’ given $Y$, $X$and $Z$ are independent from Markov chain
    
    case1) and case2) are both satisfied the definition of Markov chain
    
    case1) $P(X_1)P(X_3)=P(X_1,X_3)$, $P(X_1,X_3\ |X_2)=P(X_1|X_2)P(X_3|X_2)$
    
    case2) $X\longrightarrow Y\longrightarrow g(Y)$ í˜•íƒœì´ë©´ Markov chain
    
    $X_2$ depends $X_1$, $X_3$ is determined by $X_2$.
    
    ë”°ë¼ì„œ $X_3$ëŠ” $X_1$ì— dependí•˜ì§€ë§Œ, $X_2$ë¥¼ í†µí•´ì„œ ì˜í–¥ì„ ë°›ìŒ
    
    ($X_1$ì€ $X_2$ë¥¼ í†µí•´ì„œê°€ ì•„ë‹Œ ë°©ë²•ìœ¼ë¡œ $X_3$ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ìŒ)
    

**Caution!**
$I(X;Y\ |Z)>I(X;Y)$ but $I(X;Y\ |Z)\neq I(X;Y)$ is possible

- ex: $X,Y$ are fair coin tosses (independent)  
     $Z:=X+Y$    â€” Not Markov chain
    
    
    | $X$ | 0 | 1 |
    | --- | --- | --- |
    | $P(X)$ | 1/2 | 1/2 |
    
    | $Y$ | 0 | 1 |
    | --- | --- | --- |
    | $P(Y)$ | 1/2 | 1/2 |
    
    | $X$ \ $Y$ | 0 | 1 | $P(X)$ |
    | --- | --- | --- | --- |
    | 0 | 1/4 | 1/4 | 1/2 |
    | 1 | 1/4 | 1/4 | 1/2 |
    | $P(Y)$ | 1/2 | 1/2 |  |
    
    | $X,Y$ \ $Z$ | 0 | 1 | 2 | $P(X,Y)$ |
    | --- | --- | --- | --- | --- |
    | 0, 0 | 1/4 | 0 | 0 | 1/2 |
    | 0, 1 | 0 | 1/4 | 0 | 1/2 |
    | 1, 0 | 0 | 1/4 | 0 | 1/4 |
    | 1, 1 | 0 | 0 | 1/4 | 1/4 |
    | $P(Z)$ | 1/4 | 1/2 | 1/4 |  |
    
    | $X$ | 0 | 1 |
    | --- | --- | --- |
    | $P(X|Z=1)$ | 1/2 | 1/2 |
    
    
    ğŸ’¡ - $I(X;Y)=0$   since $X,Y$ are independent
    
    - $I(X;Y\ |Z)=H(X|Z)-H(X|\; Y,Z)$
    
    - $H(X|\;Y,Z)=0$  since given $Y,Z$, $X=Z-Y$ determined
    
    
    
    $I(X;Y\ |Z)=H(X|Z) \\ \qquad\qquad\quad =P(Z=0)H(X|Z=0) \quad\longrightarrow P(X=0)=1 \quad (X=0,Y=0)\\ \qquad\qquad\qquad +P(Z=1)H(X|Z=1)\\ \qquad\qquad\qquad +P(Z=2)H(X|Z=2) \quad\longrightarrow P(X=1)=1\quad (X=1,Y=1) \\ \qquad\qquad\quad =P(Z=1)H(X|Z=1)=\frac{1}{2}H\left(\frac{1}{2},\frac{1}{2}\right)=1/2$
    
    ì¦‰, $I(X;Y)=0<I(X;Y\ |Z)=1/2$
    
    Markov chainì´ë©´ ë¶€ë“±í˜¸ê°€ ë°˜ëŒ€ì—¬ì•¼ í•˜ë¯€ë¡œ $X\longrightarrow Y\longrightarrow Z$ëŠ” Markov chainì´ ì•„ë‹˜
    

### Sufficient Statistics

- Given $\theta \longrightarrow X \longrightarrow T(X)$, if $I(\theta;T(X))=I(\theta;X)$, then $T(X)$ is called sufficient for $\theta$.
    - $\{f_{\theta}(x)\}$: family of p.mp.f. indexed by $\theta$  ; 1-parameter family
        
        $\theta$ê°€ ê²°ì •ë˜ë©´ $f$ê°€ ê²°ì •ë˜ê³  $x$ì— ì˜í•œ ê°’ì„ ì–»ì„ ìˆ˜ ìˆëŠ” í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜
        
        ($\theta$ : index parameter, $\theta$ì— ëŒ€í•˜ì—¬ í™•ë¥ ì ì¸ ë¶€ë¶„ì´ ìˆë‹¤ë©´ R.V.)
        
        â†’ parameter í•˜ë‚˜ê°€ ê²°ì •ë˜ë©´ í•¨ìˆ˜ì˜ ë³€ìˆ˜ $X$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ìˆìŒ
        
        ex: 
        
        | $X$ | 0 | 1 |
        | --- | --- | --- |
        | $P(X)$ | $1-\theta$ | $\theta$ |
        
        $P(X=1)=\theta$
        
    
    $X\sim f_{\theta}(x)$ : $X$ is a R.V. from a distribution in this family $\{f_{\theta}(x)\}$
    
    ![coin tossing]({{'/assets/img/post_information_theory/Untitled%2011.png' | relative_url }})
    
    coinë§ˆë‹¤ $\theta$ê°€ ì •í•´ì ¸ ìˆìŒ
    
    $\{f_{\theta}(x)\}\longrightarrow f_{\theta}(x)\longrightarrow X$
    
    family     â†’    p.m.f.    â†’   R.V.
    
    $X \longrightarrow T(X)$ : statistic, $X$ë¡œë¶€í„° ì–»ì€ í†µê³„ëŸ‰, funtion of $X$
    
    Then,  $\theta, X, T(X)$ form a Markov chain. i.e., $\theta \longrightarrow X \longrightarrow T(X)$
    
    â‡’ $\theta \longrightarrow X \longrightarrow T(X) \;\Longrightarrow I(\theta;T(X))\le I(\theta;X)$
    
    If $I(\theta;T(X))=I(\theta;X)$, then $T(X)$ is called sufficient for $\theta$.
    
    
    ğŸ’¡ ì‹¤í—˜ ë°ì´í„°ë¥¼ í•´ì„í•  ë•Œ,
    
    - ì‹¤í—˜ ëŒ€ìƒì´ familyë¡œë¶€í„° ì‹¤í—˜ ëŒ€ìƒì„ ê±°ì³ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì–»ê¸°ê¹Œì§€
        
        ex: í˜„ì‹¤ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ë©´ familyì— ëŒ€í•œ ì •ë³´ëŠ” ì•Œ ìˆ˜ ì—†ê³  ì‹¤í—˜ ê²°ê³¼ë§Œ ì•Œ ìˆ˜ ìˆìŒ
        
        ë™ì „ì„ 10ë²ˆ ë˜ì ¸ì„œ 8ë²ˆì´ Hê°€ ë‚˜ì˜¤ë©´,
        Hê°€ ë‚˜ì˜¬ í™•ë¥ ì´ $\theta=4/5$ì¸ ë™ì „ì„ ì‚¬ìš©í–ˆë‹¤ê³  ì¶”ì •í•˜ëŠ” ê²ƒì´ íƒ€ë‹¹í•¨.
        
        ë¬¼ë¡  ì‹¤ì œë¡œ ì‚¬ìš©í•œ ë™ì „ì´ Hê°€ 4/5ì˜ í™•ë¥ ë¡œ ë‚˜ì˜¤ëŠ” ë™ì „ì¸ì§€ëŠ” ì•Œ ìˆ˜ ì—†ìŒ
        Hê°€ ë‚˜ì˜¬ í™•ë¥ ì´ 1/3ì¸ë° ë˜ì§€ëŠ” ë°©ì‹ì— ì˜í•´ Hë§Œ ë‚˜ì™”ì„ ìˆ˜ë„ ìˆìŒ
        
        ì´ë ‡ë“¯ $n$ë²ˆì˜ ì‹œí–‰ìœ¼ë¡œë¶€í„° ì–»ì€ ë°ì´í„°ì—ì„œ $T(X)$ë¥¼ êµ¬í•˜ê³ 
        ê°œë³„ ì‹œí–‰ì˜ ê²°ê³¼ëŠ” ì‹œí–‰ í•  ë•Œë§ˆë‹¤ ë‹¤ë¥´ê² ì§€ë§Œ $T(X)$ë¥¼ í†µí•´ $n$ë²ˆ ì¤‘ Hê°€ ë‚˜ì˜¤ëŠ” íšŸìˆ˜ì˜ ê¸°ëŒ€ê°’ $\theta$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ê³  ìš”ì•½ ê°€ëŠ¥í•¨ 
        
        â‡’ sufficient statistics
        
        =  í†µê³„ëŸ‰ì„ ì˜ ì¡ìœ¼ë©´ ëª‡ íšŒì˜ ì‹œí–‰ì´ ìˆì—ˆëŠ”ì§€,
            ëª‡ ë²ˆ ì›í•˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ì§€ ëª¨ë¥´ë”ë¼ë„ $\theta$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŒ
        
    
        information lossê°€ í•˜ë‚˜ë„ ì—†ìŒì„ ë³´ì´ê¸° ìœ„í•œ ê°€ì •ìœ¼ë¡œ ì“°ì„
    
    
    
- Def
    - Informal version
        
        Informmaly, $T(X)$ is called sufficient for $\theta$ if it contains all information in $X$ about $\theta$.
        
        ì¦‰, $\theta$ì— ëŒ€í•´ì„œ $X$ê°€ ê°€ì§€ê³  ìˆëŠ” informationì„ $T(X)$ê°€ ëª¨ë‘ ê°€ì§€ê³  ìˆìŒ 
        
    - 1st formal version
        
        $T(X)$ is said to be a sufficient statistic relative to $\{f_{\theta}(x) \}$
        
        if $X$ is independent of $\theta$ given $T(X)$ for any distributioni $f_{\theta}(x)$.
        
        ì¦‰, $T(X)$ë¥¼ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ indepentí•¨
        
        - $X$ì™€ $\theta$ì˜ dependí•œ ì •ë³´ëŠ” ëª¨ë‘ $T(X)$ì— ì†í•´ìˆìŒ
            
            $X$ì™€ $\theta$ëŠ” ë…ë¦½ì´ ì•„ë‹˜  â‡’ $X$ì™€ $\theta$ê°€ ë…ë¦½ì´ë©´ ì„œë¡œì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŒ
            (ì§ê´€ì ìœ¼ë¡œ ë…ë¦½ì´ë©´ ì„œë¡œ ë¬´ê´€í•˜ê¸° ë•Œë¬¸ì— ì˜í–¥ì„ ì£¼ê³  ë°›ê³ ë¥¼ ë”°ì§ˆ ì˜ë¯¸ê°€ ì—†ì–´ ë³´ì„)
            
            ì£¼ì–´ì§„ $T(X)$ì— ëŒ€í•´ì„œ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ ë…ë¦½ì´ë¼ëŠ” ë§ì€
            $X$ì™€ $\theta$ê°€ ì£¼ê³ ë°›ëŠ” ì •ë³´ë¥¼ $T(X)$ê°€ ëª¨ë‘ ê°€ì ¸ê°”ìŒì„ ì˜ë¯¸í•¨
            
            â‡’ $T(X)$ì— ì†í•œ $X,\theta$ì˜ ì •ë³´ë¥¼ ì œì™¸í•˜ë©´ $X$ì™€ $\theta$ëŠ” ë…ë¦½ì„
            
    - 2nd formal version
        
        If $\theta\longrightarrow X\longrightarrow T(X)$ is Markov chain then  $\theta\longrightarrow T(X)\longrightarrow X$ 
        
        - $T(X)$ê°€ $\theta$ì™€ $X$ë¥¼ ì—°ê²°í•´ì£¼ëŠ” ëª¨ë“  ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ
            
            â‡” $\theta$ì™€ $X$ë¥¼ ì—°ê²°í•´ì£¼ëŠ” $T(X)$ê°€ ì£¼ì–´ì§€ë©´ $X$ì™€ $\theta$ê°€ indepentí•¨
            
            â‡” $T(X)$ë¥¼ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ indepentí•¨
            
    
    
    ğŸ’¡ If $T(X)$ is a sufficient statistic, $I(\theta\,;T(X))= I(\theta\,;X)$
    
    - i.e., No information loss
        
        2nd formal version definition shows that $I(\theta\,;T(X))\ge I(\theta\,;X)$.
        
        And in general, $\theta\longrightarrow X\longrightarrow T(X)$ then $I(\theta\,;T(X))\le I(\theta\,;X)$.
        
        Therefore, if $T(X)$ is a sufficient statistic, $I(\theta\,;T(X))= I(\theta\,;X)$.
        
        
    


ğŸ’¡ Where $X_1, \ldots, X_n \sim f_{\theta}(x)$, $Y:=g(X_1, \ldots,X_n)$ is sufficient for $\theta$

- if $P(X_1=x_1,\ldots,X_n=x_n\;|Y=y)$ does not depend on $\theta$.
    
    ---
    
    Let $X_1, \ldots,X_n$ be R.V.s from a p.m.f. with parameter $\theta$
    
    i.e., $X_1, \ldots, X_n \sim f_{\theta}(x)$
    
    and $Y:=g(X_1, \ldots,X_n)$ is a funtion of $X_1, \ldots,X_n$ 
    
    i.e., $Y$ is deterministic for  $X_1, \ldots,X_n$
    
    Then $Y$ is sufficient for $\theta$,
    if the conditional probability $P(X_1=x_1,\ldots,X_n=x_n\;|Y=y)$ 
    does not depend on $\theta$.
    
    ---
    
    Since $Y=g(X_1,\ldots,X_n)$,   $\theta\longrightarrow Y\longrightarrow (X_1,\ldots,X_n)$.
    
    â‡’ $P(X_1=x_1,\ldots,X_n=x_n\;|Y=y)\\ \quad = P(X_1=x_1,\ldots,X_n=x_n\;|Y=y,\Theta=\theta)$ 
    
    ì¦‰, ì£¼ì–´ì§„ $Y$ì— ëŒ€í•˜ì—¬ $\theta$ì™€ $(X_1,\ldots,X_n)$ê°€ independent.
    
    

- ex. Bernoulli distribution
    
    
    | $X$ | 0 | 1 |
    | --- | --- | --- |
    | $P(X)$ | $1-\theta$ | $\theta$ |
    
    For sample space $\chi$, $x\in\chi=\{0,1\}$.
    
    $P_\theta(X=x)= 
    \left\{\begin{matrix}
    \theta   & x=1 \\
    1-\theta & x=0 \\
    \end{matrix}\right.\quad\textrm{(p.m.f.)} \\ \qquad\qquad\quad
    = \theta^x (1-\theta)^{1-x}$
    
    Let $X_1, \ldots, X_n$ be R.V.s sample from a Bernoulli distribution with $\theta$.
    
    Bernoulli : i.i.d.(independent and identically distributed)
    
    i.e., $P(X_1=x_1,\ldots,X_n=x_n)=P(X_1=x_1)\cdots P(X_n=x_n)$
    
    $\qquad\qquad\qquad\qquad\qquad\quad\;\, =\theta^{x_1}(1-\theta)^{1-x_1}\cdots \theta^{x_n}(1-\theta)^{1-x_n}$ 
    
    $\qquad\qquad\qquad\qquad\qquad\quad\;\, =\prod_{i=1}^n\theta^{x_i}(1-\theta)^{1-x_i}$ 
    
    Introduce a statistic $Y:=T(X_1,\ldots,X_n)=\sum_{i=1}^n X_i$.
    
    â‡’ counting 1â€™s out of $x_1,\ldots,x_n$ â‡” counting the number of Heads in $n$ tosses
    
    $Y\sim B(n,\theta)$ : Binomial ditribution
    
    â‡’ $P(Y=k)=\binom{n}{k}\theta^k(1-\theta)^{n-k}$ 
    
    Consider the conditional probabiltiy
    
    $P(X_1=x_1,\ldots,X_n=x_n)=\frac{P(X_1,=x_1\ ,\ldots,\ X_n=x_n \textrm{  and } Y=k)}{P(Y=k)}$ 
    
    $\qquad\qquad\qquad\qquad\qquad=\frac{\prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i}}{\binom{n}{k}\theta^k(1-\theta)^{n-k}}\textrm{\quad where }\sum_{i=1}^n x_i =k$
    
    $\qquad\qquad\qquad\qquad\qquad=\frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i} }{\binom{n}{k}\theta^k(1-\theta)^{n-k}}$
    
    $\qquad\qquad\qquad\qquad\qquad=\frac{\theta^{k}(1-\theta)^{n-k} }{\binom{n}{k}\theta^k(1-\theta)^{n-k}}$
    
    $\qquad\qquad\qquad\qquad\qquad=\frac{1}{\binom{n}{k}}$
    
    â‡’ $P(X_1=x_1,\ldots,X_n=x_n)=\frac{1}{\binom{n}{k}}$ does not depend on $\theta$,
    
    which means that given $Y=k\left( =\sum_{i=1}^n x_i \right)$,
    
    the individual values of the $x_i$â€™s cannot provide additional information about $\theta$.
    
    ì¦‰, $Y$ ê°’ì´ ì£¼ì–´ì§€ë©´ $x_i$ ê° ê°’ì€ (ê° $x_i$ê°€ 0ì¸ì§€ 1ì¸ì§€ëŠ”) $\theta$ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
    
- ex. Poisson distribution
    
    p.m.f. $p(k)=\frac{\lambda^k e^{-\lambda}}{k!}$   for $k=0,1,2,\ldots$  ($\lambda$ë¥¼ ì•Œë©´ ëª¨ë“  parameterê°€ ê²°ì •ë¨)
    
    Let $X_1, \ldots, X_n$ be sample from a Poisson distribution with $\lambda$ and $Y=\sum_{i=1}^n X_i$ (=í†µê³„ëŸ‰)
    
    â€»  $r$ : average rate of the event
    
    $t$ : time interval  ($\lambda=rt$ : ì£¼ì–´ì§„ ì‹œê°„ $t$ë™ì•ˆ eventê°€ ë°œìƒí•˜ëŠ” íšŸìˆ˜ì˜ í‰ê· , ë¹ˆë„)
    
    â‡’ $P(k\textrm{ events in interval }t) = \frac{(rt)^k e^{-rt}}{k!}$
    
    $Y=\sum_{i=1}^n X_i \sim$  Poisson dist with $\lambda y=n\lambda$
    
    - $Y$ is sufficient for $\lambda$.
        
        Consider the conditional probability
        
        $P(X_1=x_1,\ldots, X_n=x_n\;|Y=y)=\frac{P(X_1=x_1,\ldots, X_n=x_n\textrm{ and }Y=y)}{P(Y=y)}$
        
        $\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{P(X_1=x_1,\ldots,X_n=x_n)}{P(Y=y)}\quad\textrm{ where }\sum x_i=y$ 
        
        (note that $\sum x_i\neq y\Rightarrow P(*)=0$)
        
        $\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\prod_{i=1}^n\frac{\lambda^{x_i}e^{-\lambda}}{x_i!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$
        
        $\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\frac{\lambda^{x_1+\cdots+x_n}e^{-\lambda}}{x_1!x_2!\cdots x_n!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$
        
        $\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\frac{\lambda^y e^{-\lambda}}{x_1!\cdots x_n!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$
        
        $\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{y!}{n^yx_1!\cdots x_n!}$    does not depend on $\lambda$.
        
- Thm: Factorization Theorem
$Y=g(X_1,\ldots,X_n)$ is sufficient for $\theta$ 
$\iff P(x_1,\ldots,x_n\;|\,\theta)=\phi(g(x_1,\ldots,x_n)\,|\,\theta)h(x_1,\ldots,x_n)$
    
    ---
    
    Let $X_1,\ldots,X_n$ be i.i.d. R.V.s from p.m.f. $f_\theta(x)\in \{f_\theta(x)\}$,
    
    $Y=g(X_1,\ldots,X_n)$ is sufficient for $\theta$
    
    $\iff P(x_1,\ldots,x_n\;|\,\theta)=\phi(g(x_1,\ldots,x_n)\,|\,\theta)h(x_1,\ldots,x_n)$
    
    where $\phi$ depends on $x_i$â€™s only through $g$ (i.e., $Y$) and $h$ does not depend on $\theta$.
    
    ---
    
    $\theta$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê° $X_i$ê°€ ê°’ $x_i$ë¥¼ ê°€ì§ˆ í™•ë¥ ì— ëŒ€í•˜ì—¬ 
    
    $\theta$ì— ì˜ì¡´í•˜ëŠ” ê°’ë“¤ì˜ í™•ë¥ ì€ $\phi$, ì¦‰ $g(x_1,\ldots,x_n)=Y$ì— ëª°ë ¤ìˆìŒ ($h$ëŠ” $\theta$ì— independent)
    
    â‡’ $Y$ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ” ê°’ë“¤ì€ $\theta$ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ
    
    - Remark
        
        
        ğŸ’¡ $P(x_1,\ldots,x_n):=P(X_1=x_1,\ldots,X_n=x_n)$
        
        $P(X_1,\ldots,X_n)$ê°€ $x_1,\ldots,x_n$ì„ ì–´ë–»ê²Œ ê²°ì •í•˜ëŠëƒì— ë”°ë¼ ë‹¬ë¼ì§ì„ ë°˜ì˜í•œ í‘œê¸°ì„ 
        
        

        ğŸ’¡ $P(*;\theta)=P(*|\theta)$ì´ê¸° ìœ„í•´ì„œëŠ” $\theta$ê°€ R.V.ì—¬ì•¼ í•¨
        
        ìš°í•­ì˜ í‘œê¸°ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ì§€ë§Œ, $\theta$ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•˜ê³  ì‹¶ì§€ ì•ŠìŒ
        
        ì¢Œí•­ì˜ $\theta$ ëŠ” determined $\theta$ ë˜ëŠ” fixed $\theta$, indexed by $\theta$ì´ê³ 
        ìš°í•­ì˜ $\theta$ëŠ” p.m.f.ë¥¼ ê°–ëŠ” R.V.ì„ 
        
        
- ex: For Bernoulli distribution,
    
    $P(x_1,\ldots,x_n\ ;\theta)=\prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i})=\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}$
    
    Let $y=\sum_{i=1}^n x_i$,
    
    $\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}=\theta^{y}(1-\theta)^{n-y}\times1:=\phi(y\,;\theta)\times h(x_1,\ldots,x_n)$
    
    â‡’  Bernoulli distributionì— ëŒ€í•˜ì—¬
    
    $P(x_1,\ldots,x_n\ ;\theta)=\phi(y\,;\theta)\times h(x_1,\ldots,x_n)$ë¥¼ ë§Œì¡±í•˜ë¯€ë¡œ
    
    $Y=\sum X_i$ëŠ” $\theta$ì— ëŒ€í•˜ì—¬ sufficient statisticì„ 
    
- ex: For Poisson distribution,
    
    $p(x_1, \ldots,x_n\,;\lambda)=\frac{\lambda^{x_1+\cdots+x_n}e^{-n\lambda} }{x_1!\cdots x_n!}=\lambda^y e^{-n\lambda}\times\frac{1}{x_1!\cdots x_n!}$
    
    â‡’  Poisson distributionì— ëŒ€í•˜ì—¬
    
    $P(x_1,\ldots,x_n\ ;\lambda)=\phi(y\,;\lambda)\times h(x_1,\ldots,x_n)$ë¥¼ ë§Œì¡±í•˜ë¯€ë¡œ
    
    $Y=\sum X_i$ëŠ” $\lambda$ì— ëŒ€í•˜ì—¬ sufficient statisticì„ 
    

#### Minimal sufficient statistic

Def: $T(X)$ is a minimal sufficient statistic relative to $\{ f_\theta(x) \}$
       if it is a function of every other sufficient statistic $U$.

ì¦‰, sufficient statisticì´ë©´ì„œ ë‹¤ë¥¸ sufficient statisticë¡œ í‘œì‹œë˜ëŠ” í•¨ìˆ˜

$\theta \longrightarrow 
T(X) \longrightarrow  U(X) \longrightarrow X$


ğŸ’¡ A minimal sufficient statistic maximally compresses the information about $\theta$ in the sample.

Other sufficient statistics may contain additional irrelevant information.



- ex
    - $Y=X_1+\cdots+X_n$ is minimal sufficient statistic for $\theta$ in the independent coin toss example.
        
        Let $Y_{odd}=X_1+X_3+\cdots+X_{2n-1}$, $Y_{even}=X_2+X_4+\cdots+X_{2n}$ and
        $\widetilde{Y}=(Y_{odd}, Y_{even})$, then $\widetilde{Y}$ is a sufficient statistic
        
        $Y=g(\widetilde{Y})=g(Y_{odd},Y_{even})=Y_{odd}+Y_{even}$
        
        â‡’  $Y$ì™€ ë¹„êµí•  ë•Œ, $Y_{odd}+Y_{even}$ëŠ” ë¶ˆí•„ìš”í•˜ê²Œ ì„¸ë¶€ì ìœ¼ë¡œ ë‚˜ë‰œ í•¨ìˆ˜ì„
        
    - í•™êµì—ì„œ ì „êµ ìˆ˜í•™ì ìˆ˜ í‰ê· ì„ ì•Œê¸¸ ë°”ë„ ë•Œ,
        - ì „ì²´ ë°ì´í„°ë¡œë¶€í„° ê° ë°˜ì˜ í‰ê·  ì ìˆ˜ë¥¼ êµ¬í•˜ê³  ì´ê²ƒë“¤ì„ ëª¨ì•„ì„œ ë‹¤ì‹œ ì „ì²´ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒë³´ë‹¤
        - ì „ì²´ ë°ì´í„°ë¡œë¶€í„° í•œ ë²ˆì— ì „ì²´ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ í›¨ì”¬ íš¨ìœ¨ì ì„
        (ë°˜ í‰ê· ì´ë¼ëŠ” ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ)