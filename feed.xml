<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ryuj1eun.github.io/june.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ryuj1eun.github.io/june.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-20T11:24:55+00:00</updated><id>https://ryuj1eun.github.io/june.github.io/feed.xml</id><title type="html">June</title><subtitle>PhD student&apos;s blog in Cryptography focusing on RNG, PQC, and WBC. </subtitle><entry><title type="html">KpqC íŠ¹ê°• ì •ë¦¬</title><link href="https://ryuj1eun.github.io/june.github.io/blog/2026/pqc/" rel="alternate" type="text/html" title="KpqC íŠ¹ê°• ì •ë¦¬"/><published>2026-02-19T00:00:00+00:00</published><updated>2026-02-19T00:00:00+00:00</updated><id>https://ryuj1eun.github.io/june.github.io/blog/2026/pqc</id><content type="html" xml:base="https://ryuj1eun.github.io/june.github.io/blog/2026/pqc/"><![CDATA[<p>ğŸ·ï¸(2025ë…„ë„ ìƒë°˜ê¸°) ì •ë³´ë³´í˜¸ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ì•”í˜¸êµìœ¡, 2025.05.<br/> ìµœí˜•ë¯¼, â€œê³µê°œí‚¤ ì•”í˜¸ ê¸°ì´ˆâ€, CryptoLab, inc. </p> <h4 id="game-based-security">Game-based Security</h4> <p>ë¹„ë°€ ì •ë³´ë¥¼ ì§€ë‹Œ Challengerì™€ ê·¸ ë¹„ë°€ì„ íƒˆì·¨í•˜ë ¤ëŠ” Attacker ê°„ì˜ ì‹¸ì›€ì„ ëª¨ë¸ë§í•œ ê²ƒ</p> <h5 id="attacker-adversary">Attacker (Adversary)</h5> <ul> <li> <p>ê³µê°œ ì •ë³´ íšë“ ê°€ëŠ¥</p> </li> <li>ê³µê²©ìì˜ ëŠ¥ë ¥ì´ íŠ¹ì • ì˜¤ë¼í´ í˜•íƒœë¡œ ì œí•œë¨</li> <li>ê³µê²©ìì˜ ëª¨ë“  í–‰ë™ì€ ë‹¤í•­ì‹ ì‹œê°„ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í‘œí˜„ë˜ì–´ì•¼ í•¨</li> </ul> <h5 id="pkekem-ì•ˆì „ì„±-ì •ì˜">PKE/KEM ì•ˆì „ì„± ì •ì˜</h5> <ul> <li>ê³µê²©ìëŠ” íŠ¹ì • ë©”ì‹œì§€ì— ëŒ€í•œ ìº¡ìŠí™”ë¥¼ í•  ìˆ˜ ìˆìŒ</li> <li>ê³µê²©ìëŠ” íŠ¹ì • ì•”í˜¸ë¬¸ì— ëŒ€í•œ ë³µí˜¸í™”ë¥¼ <ul> <li>í•  ìˆ˜ ì—†ìŒ: IND-CPA</li> <li>í•  ìˆ˜ ìˆìŒ: IND-CCA1</li> <li>ì•”í˜¸ë¬¸ì„ ì¶”ì¸¡í•˜ì—¬ ì•”í˜¸í™”, ë³µí˜¸í™”ë¥¼ í•  ìˆ˜ ìˆìŒ: IND-CCA2</li> </ul> </li> <li>$\text{Adv}^{\text{IND-win}}({\cal A}) \ge \frac{1}{2}$ì¼ ë•Œ ì•ˆì „í•˜ë‹¤ê³  í•¨ <ul> <li>$\text{Adv}^{\text{IND}}({\cal A}) = \Pr[\text{Exp}^{\text{IND-1}}(A)=1]-\Pr[\text{Exp}^{\text{IND-0}}({\cal A})=1]$</li> <li>$\Pr[\text{Exp}^{\text{IND-1}}({\cal A})=1]-\Pr[\text{Exp}^{\text{IND-0}}({\cal A})=1] &lt; \epsilon$ (ê³µê²©ìê°€ 0,1ì„ êµ¬ë³„í•  ìˆ˜ ì—†ìŒ)</li> <li>$\text{Adv}^{\text{IND-win}}({\cal A}) = \frac{1}{2}\left( \text{Adv}^{\text{IND}}({\cal A}) +1\right)$</li> </ul> </li> </ul> <h5 id="ì „ìì„œëª…ì˜-ì•ˆì „ì„±-ì •ì˜">ì „ìì„œëª…ì˜ ì•ˆì „ì„± ì •ì˜</h5> <ul> <li>ê³µê²©ìëŠ” íŠ¹ì • ì„œëª…ì— ëŒ€í•œ ê²€ì¦ì„ ìš”ì²­í•  ìˆ˜ ìˆìŒ</li> <li>ê³µê²©ìëŠ” íŠ¹ì • ë©”ì‹œì§€ì— ëŒ€í•œ ì„œëª…ì„ <ul> <li>ìš”ì²­í•  ìˆ˜ ì—†ìŒ: UF-NMA</li> <li>ìš”ì²­í•  ìˆ˜ ìˆìŒ: UF-CMA</li> <li>ë™ì¼ ë©”ì‹œì§€ì— ëŒ€í•œ ë°˜ë³µ ì„œëª…ì„ ìš”ì²­í•  ìˆ˜ ìˆìŒ: SUF-CMA</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="Study"/><category term="Cryptography"/><category term="PQC"/><summary type="html"><![CDATA['2025ë…„ë„ ìƒë°˜ê¸° ì •ë³´ë³´í˜¸ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ì•”í˜¸êµìœ¡' íŠ¹ê°• ë‚´ìš© ì •ë¦¬]]></summary></entry><entry><title type="html">Secret Sharing</title><link href="https://ryuj1eun.github.io/june.github.io/blog/2026/ss/" rel="alternate" type="text/html" title="Secret Sharing"/><published>2026-02-13T00:00:00+00:00</published><updated>2026-02-13T00:00:00+00:00</updated><id>https://ryuj1eun.github.io/june.github.io/blog/2026/ss</id><content type="html" xml:base="https://ryuj1eun.github.io/june.github.io/blog/2026/ss/"><![CDATA[<blockquote> <p>Adi Shamir, â€œHow to Share a Secretâ€, Communications of the ACM, Vol.22, No.11, 1979.</p> </blockquote> <p>This technique enables the construction of robust key management schemes for cryptographic systems that can function securely and reliably even when misfortunes destroy half the pieces and security breaches expose all but one of the remaining pieces.</p> <ol> <li> <p>knowledge of any $k$ or more $D_i$ pieces makes $D$ easily computable;</p> </li> <li> <p>knowledge of any $k-1$ or fewer $D_i$ pieces leaves $D$ completely undetermined (in the sense that all its possible values are equally likely)</p> </li> </ol> <p>By using a $(k,n)$ threshold scheme with $n=2k-1$ we get a very robust key management scheme:</p> <p>our opponents cannot reconstruct the key even when security breaches expose $\lfloor n/2 \rfloor = k-1$ of the remaining $k$ pieces.</p> <p>Our scheme is based on polynomial interpolation:</p> <p>Knowledge of just $k-1$ of these values, on the other hand, does not suffice in order to calculate $D$.</p> <p>Let us now assume that $k-1$ of these $n$ pieces are revealed to an opponent. For each candidate value $Dâ€™$ in $[0, p)$ he can construct one and only one polynomial $qâ€™(x)$ of degree $k- 1$ such that $qâ€™(0) =Dâ€™$ and $qâ€™(i) =D_i$ for the $k- 1$ given arguments. By construction, these $p$ possible polynomials are equally likely, and thus there is absolutely nothing the opponent can deduce about the real value of $D$.</p> <p>the smallest usable value of $p$ is $n + 1$ (there must be at least $n + 1$ distinct arguments in $[0, p)$ to evaluate $q(x)$ at).</p>]]></content><author><name></name></author><category term="Study"/><category term="Cryptography"/><category term="SS"/><summary type="html"><![CDATA[Paper review regarding the secret sharing]]></summary></entry><entry><title type="html">AIS 31 (version 3.0)</title><link href="https://ryuj1eun.github.io/june.github.io/blog/2026/ais31/" rel="alternate" type="text/html" title="AIS 31 (version 3.0)"/><published>2026-02-12T00:00:00+00:00</published><updated>2026-02-12T00:00:00+00:00</updated><id>https://ryuj1eun.github.io/june.github.io/blog/2026/ais31</id><content type="html" xml:base="https://ryuj1eun.github.io/june.github.io/blog/2026/ais31/"><![CDATA[<p>AIS 31 (version 3.0) 4.2.1.</p> <iframe src="/june.github.io/assets/pdf/AIS31.pdf" width="100%" height="800px" style="border: none;"> PDF ë¯¸ë¦¬ë³´ê¸°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. <a href="/june.github.io/assets/pdf/AIS31.pdf">ì—¬ê¸°</a>ë¥¼ í´ë¦­í•´ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”. </iframe>]]></content><author><name></name></author><category term="Study"/><category term="Cryptography"/><category term="RNG"/><category term="MATH"/><summary type="html"><![CDATA[Study note: AIS 31 (version 3.0)]]></summary></entry><entry><title type="html">PQC ë™í–¥ (~ â€˜24.10)</title><link href="https://ryuj1eun.github.io/june.github.io/blog/2025/pqc-updates/" rel="alternate" type="text/html" title="PQC ë™í–¥ (~ â€˜24.10)"/><published>2025-09-18T00:00:00+00:00</published><updated>2025-09-18T00:00:00+00:00</updated><id>https://ryuj1eun.github.io/june.github.io/blog/2025/pqc-updates</id><content type="html" xml:base="https://ryuj1eun.github.io/june.github.io/blog/2025/pqc-updates/"><![CDATA[<h3 id="250924-26-6th-pqc-standardization-conference">25.09.24-26 <a href="https://csrc.nist.gov/events/2025/6th-pqc-standardization-conference">6th PQC Standardization Conference</a></h3> <h4 id="nist-pqc-standardization-project-dustin-moody-september-24-2025"><a href="https://csrc.nist.gov/Presentations/2025/nist-pqc-project">NIST PQC Standardization Project</a> Dustin Moody (September 24, 2025)</h4> <h4 id="sp-800-227-recommendations-for-key-encapsulation-mechanisms-gorjan-alagic-september-25-2025"><a href="https://csrc.nist.gov/Presentations/2025/sp-800-227-recommendations-for-key-encapsulation-m">SP 800-227: Recommendations for Key-Encapsulation Mechanisms</a> Gorjan Alagic (September 25, 2025)</h4> <h3 id="250918-nist-publishes-sp-800-227">25.09.18. <a href="https://csrc.nist.gov/News/2025/nist-publishes-sp-800-227">NIST Publishes SP 800-227</a></h3> <p>â€œRecommendations for Key-Encapsulation Mechanismsâ€</p> <h4 id="250729-pqc-hybrid-ê²€ì¦-ì•ˆë‚´-ë°œí‘œ">25.07.29 PQC Hybrid ê²€ì¦ ì•ˆë‚´ <a href="https://www.ncsc.go.kr/main/cop/bbs/selectBoardArticle.do?bbsId=CryptoNotice_main&amp;nttId=213309&amp;pageIndex=1&amp;searchCnd2=">ë°œí‘œ</a></h4> <ul> <li><a href="https://www.ncsc.go.kr/main/cop/bbs/selectBoardArticle.do?bbsId=CryptoNotice_main&amp;nttId=221869&amp;pageIndex=1&amp;searchCnd2=">ì•”í˜¸ëª¨ë“ˆ êµ¬í˜„ ì•ˆë‚´ì„œ</a> C.5 ì–‘ìë‚´ì„± ì•”í˜¸ë¥¼ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹</li> </ul> <h4 id="800-227-46-multi-algorithm-kems-and-pqt-hybrids">800-227 4.6. Multi-Algorithm KEMs and PQ/T Hybrids</h4> <ul> <li><a href="https://www.etsi.org/deliver/etsi_tr/103900_103999/103966/01.01.01_60/tr_103966v010101p.pdf">ETSI TR 103 966 v1.1.1.</a> ë¶€ë¡ A.3 êµì°¨ í™•ì¸</li> </ul> <h3 id="250311-nistir-8545">25.03.11. <a href="https://csrc.nist.gov/pubs/ir/8545/final">NISTIR 8545</a></h3> <p>â€œStatus Report on the Fourth Round of the NIST Post-Quantum Cryptography Standardization Processâ€</p> <h4 id="ê³µì‹-íƒ€ì„ë¼ì¸">ê³µì‹ íƒ€ì„ë¼ì¸</h4> <p><img src="/june.github.io/assets/img/post_pqc_updates/250311_nistir_8545.png" alt="timeline"/> </p> <h4 id="hqc-ì„ ì •">HQC ì„ ì •</h4> <ul> <li>4ë¼ìš´ë“œ í›„ë³´ë¡œ ì„ ì •ëœ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ NISTì—ì„œ íŒë‹¨í•˜ê¸°ì— ëª¨ë‘ ë°”ë¡œ í‘œì¤€ìœ¼ë¡œì¨ ì‚¬ìš© ê°€ëŠ¥í•œ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ì—ˆìŒ</li> <li>ì£¼ìš” í‰ê°€ ê¸°ì¤€ì€ (1) ì•ˆì „ì„± (2) ë¹„ìš© ë° ì„±ëŠ¥ (3) ì•Œê³ ë¦¬ì¦˜ ë° êµ¬í˜„ íŠ¹ì„±ìœ¼ë¡œ í•´ë‹¹ ê¸°ì¤€ì— ë”°ë¥¸ ìì„¸í•œ í‰ê°€ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŒ</li> <li>(1) ì•ˆì „ì„±: ê° ì•Œê³ ë¦¬ì¦˜ì€ ì„œë¡œ ë‹¤ë¥¸ ì•ˆì „ì„± ìˆ˜ì¤€ì„ ê°€ì§€ë©°, NISTëŠ” HQCì˜ ì•ˆì „ì„±ì´ BIKEì˜ ì•ˆì „ì„±ë³´ë‹¤ ë” ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ë‹¤ê³  ë´„</li> <li>(2) ë¹„ìš© ë° ì„±ëŠ¥: ìº¡ìŠí™” í‚¤ì™€ ì•”í˜¸ë¬¸ í¬ê¸° &amp; ê° ì•Œê³ ë¦¬ì¦˜ì˜ ê³„ì‚° íš¨ìœ¨ì„±(ì†ë„). HQCê°€ BIKEë³´ë‹¤ ë” ë¹ ë¥´ê²Œ ë™ì‘í•˜ë©°, McEileceëŠ” êµ¬ì„± ì•Œê³ ë¦¬ì¦˜ê°„ ì†ë„ì˜ ì°¨ì´ê°€ ë§¤ìš° í¬ê¸° ë•Œë¬¸ì— ë²”ìš©ì„±ì´ ë–¨ì–´ì§. í‚¤ì™€ ì•”í˜¸ë¬¸ì˜ í¬ê¸°ëŠ” HQCë³´ë‹¤ BIKEê°€ íš¨ìœ¨ì ì´ë©°, ì´ëŠ” McEileceë³´ë‹¤ë„ ë‚˜ì€ ì„±ëŠ¥ì„. McElieceëŠ” ì•”í˜¸ë¬¸ì€ ì‘ì§€ë§Œ ìº¡ìŠí™” í‚¤ê°€ ë§¤ìš° ì»¤ TLS 1.3ì— ì ì ˆí•˜ì§€ ì•ŠìŒ</li> <li>(3) êµ¬í˜„ íŠ¹ì„±: McElieceë¥¼ ì†”ë£¨ì…˜ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•œ í™˜ê²½ì— ëŒ€í•˜ì—¬ ìš”ì²­í–ˆìœ¼ë‚˜, ê²°ê³¼ ë³´ê³ ê°€ ë§ì§€ ì•Šì•˜ìœ¼ë©°, í•´ë‹¹ í™˜ê²½ì„ ìœ„í•œ ë³„ë„ì˜ ì•”í˜¸ ìì‚° ëª©ë¡ì„ êµ¬ì„±í•˜ëŠ” ë°©ì‹ì€ PQC ì „í™˜ ì¸¡ë©´ì—ì„œ ì•”í˜¸í™” ë¯¼ì²©ì„±ì„ ì œê³µí•˜ê¸° ì–´ë µê²Œ ë§Œë“¦</li> <li>NISTì˜ í‘œì¤€í™” ëª©ì ì€ ì¼ë°˜ì ì¸ í™˜ê²½ì—ì„œ í™œìš© ê°€ëŠ¥í•œ KEMì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°•ë ¥í•œ ì•ˆì „ì„±ê³¼ ë²”ìš©ì„±ì„ ê³ ë ¤í•˜ì—¬ HQCë¥¼ í‘œì¤€ìœ¼ë¡œ ì„ ì •í•¨</li> </ul> <p>ê²°ë¡ </p> <ul> <li>NISTëŠ” HQCê°€ ML-KEMê³¼ ì¶©ë¶„í•œ ìƒí˜¸ ë³´ì™„ì„±ì„ ê°€ì§„ë‹¤ê³  íŒë‹¨í•¨. HQCëŠ” ML-KEMê³¼ ë‹¤ë¥¸ ë¬¸ì œì— ì•ˆì „ì„±ì„ ê¸°ë°˜í•˜ëŠ” ë™ì‹œì— ë²”ìš© ì•±ì— ëŒ€í•œ í•©ë¦¬ì  ì„±ëŠ¥ì„ ìœ ì§€í•¨</li> <li>BIKE ì—­ì‹œ ë¹„ìŠ·í•œ ì„±ì§ˆì„ ê³ ë ¤í•˜ì—¬ í‘œì¤€í™” í›„ë³´ë¡œ ì„ ì •ë˜ì—ˆìœ¼ë‚˜, HQCì™€ ìœ ì‚¬í•œ ê¸°ë°˜ ë¬¸ì œë¥¼ ë‹¤ë£¨ë©´ì„œë„ ì•ˆì „ì„±ì€ ë” ë‚®ë‹¤ê³  íŒë‹¨ë˜ì–´ í‘œì¤€ìœ¼ë¡œ í•¨ê»˜ ì„ ì •í•˜ì§€ ì•ŠìŒ</li> <li>HQCì˜ í‘œì¤€í™”ëŠ” ì•½ 2ë…„ì •ë„ ì†Œìš”ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•¨</li> </ul> <h3 id="241112-nistir-8547">24.11.12. <a href="https://csrc.nist.gov/pubs/ir/8547/ipd">NISTIR 8547</a></h3> <p>â€œTransition to Post-Quantum Cryptography Standardsâ€</p> <h4 id="41-nist-cryptographic-algorithm-standards-and-guidelines">4.1. NIST Cryptographic Algorithm Standards and Guidelines</h4> <ul> <li>Acceptable: FIPS ë˜ëŠ” SPì— ì •ì˜ëœ ì•Œê³ ë¦¬ì¦˜ ë° í‚¤ ê¸¸ì´/ê°•ë„ê°€ ê´€ë ¨ ì§€ì¹¨ì— ë”°ë¼ ì‚¬ìš©ì´ ìŠ¹ì¸ë¨</li> <li>Deprecated: ì•Œê³ ë¦¬ì¦˜ê³¼ í‚¤ ê¸¸ì´/ê°•ë„ë¥¼ ì‚¬ìš©í•  ìˆ˜ëŠ” ìˆìœ¼ë‚˜ ì¼ë¶€ ë³´ì•ˆ ìœ„í—˜ì´ ì¡´ì¬í•¨ (ë°ì´í„° ì¤‘ìš”ë„ì— ë”°ë¼ ì‚¬ìš© ê²°ì •)</li> <li>Disallowed: í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜, í‚¤ ê¸¸ì´/ê°•ë„, ë§¤ê°œë³€ìˆ˜ ì§‘í•© ë˜ëŠ” ì²´ê³„ê°€ ëª…ì‹œëœ ëª©ì ì— ë” ì´ìƒ í—ˆìš©ë˜ì§€ ì•ŠìŒ</li> <li>Legacy use: ì•Œê³ ë¦¬ì¦˜, ì²´ê³„ ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ ì§‘í•©ì´ ì´ë¯¸ ë³´í˜¸ëœ ì •ë³´(ì˜ˆ: ì•”í˜¸ë¬¸ ë°ì´í„°ì˜ ë³µí˜¸í™” ë˜ëŠ” ë””ì§€í„¸ ì„œëª…ì˜ ê²€ì¦)ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°ì—ë§Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ</li> </ul> <p>Table 1: Post-Quantum Security Categories</p> <table> <thead> <tr> <th><strong>Security Category</strong></th> <th><strong>Attack Type</strong></th> <th><strong>Example</strong></th> </tr> </thead> <tbody> <tr> <td>1</td> <td>Key search on a block cipher with a 128-bit key</td> <td>AES-128</td> </tr> <tr> <td>2</td> <td>Collision search on a 256-bit hash function</td> <td>SHA-256</td> </tr> <tr> <td>3</td> <td>Key search on a block cipher with a 192-bit key</td> <td>AES-192</td> </tr> <tr> <td>4</td> <td>Collision search on a 384-bit hash function</td> <td>SHA3-384</td> </tr> <tr> <td>5</td> <td>Key search on a block cipher with a 256-bit key</td> <td>AES-256</td> </tr> </tbody> </table> <p>Table 2: Quantum-vulnerable digital signature algorithms</p> <table> <thead> <tr> <th><strong>Digital Signature Algorithm Family</strong></th> <th><strong>Parameters</strong></th> <th><strong>Transition</strong></th> </tr> </thead> <tbody> <tr> <td><strong>ECDSA</strong> [FIPS186]</td> <td>112 bits of security strength</td> <td><em>Deprecated</em> after 2030 <em>Disallowed</em> after 2035</td> </tr> <tr> <td>Â </td> <td>â‰¥ 128 bits of security strength</td> <td><em>Disallowed</em> after 2035</td> </tr> <tr> <td><strong>EdDSA</strong> [FIPS186]</td> <td>â‰¥ 128 bits of security strength</td> <td><em>Disallowed</em> after 2035</td> </tr> <tr> <td><strong>RSA</strong> [FIPS186]</td> <td>112 bits of security strength</td> <td><em>Deprecated</em> after 2030 <em>Disallowed</em> after 2035</td> </tr> <tr> <td>Â </td> <td>â‰¥ 128 bits of security strength</td> <td><em>Disallowed</em> after 2035</td> </tr> </tbody> </table> <p>Table 3: Post-quantum digital signature algorithms</p> <table> <thead> <tr> <th><strong>Digital Signature Algorithm Family</strong></th> <th><strong>Parameter Sets</strong></th> <th><strong>Security Strength</strong></th> <th><strong>Security Category</strong></th> </tr> </thead> <tbody> <tr> <td><strong>ML-DSA</strong> [FIPS204]</td> <td>ML-DSA-44</td> <td>128 bits</td> <td>2</td> </tr> <tr> <td>Â </td> <td>ML-DSA-65</td> <td>192 bits</td> <td>3</td> </tr> <tr> <td>Â </td> <td>ML-DSA-87</td> <td>256 bits</td> <td>5</td> </tr> <tr> <td><strong>SLH-DSA</strong> [FIPS205]</td> <td>SLH-DSA-SHA2-128[s/f], SLH-DSA-SHAKE-128[s/f]</td> <td>128 bits</td> <td>1</td> </tr> <tr> <td>Â </td> <td>SLH-DSA-SHA2-192[s/f], SLH-DSA-SHAKE-192[s/f]</td> <td>192 bits</td> <td>3</td> </tr> <tr> <td>Â </td> <td>SLH-DSA-SHA2-256[s/f], SLH-DSA-SHAKE-256[s/f]</td> <td>256 bits</td> <td>5</td> </tr> <tr> <td><strong>LMS, HSS</strong> [SP800208]</td> <td>With SHA-256/192, With SHAKE256/192</td> <td>192 bits</td> <td>3</td> </tr> <tr> <td>Â </td> <td>With SHA-256, With SHAKE256</td> <td>256 bits</td> <td>5</td> </tr> <tr> <td><strong>XMSS, XMSS^MT</strong> [SP800208]</td> <td>With SHA-256/192, With SHAKE256/192</td> <td>192 bits</td> <td>3</td> </tr> <tr> <td>Â </td> <td>With SHA-256, With SHAKE256</td> <td>256 bits</td> <td>5</td> </tr> </tbody> </table> <p>Table 4: Quantum-vulnerable key-establishment schemes</p> <table> <thead> <tr> <th><strong>Key Establishment Scheme</strong></th> <th><strong>Parameters</strong></th> <th><strong>Transition</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Finite Field DH and MQV</strong> [SP80056A]</td> <td>112 bits of security strength</td> <td><em>Deprecated</em> after 2030 <em>Disallowed</em> after 2035</td> </tr> <tr> <td>Â </td> <td>â‰¥ 128 bits of security strength</td> <td><em>Disallowed</em> after 2035</td> </tr> <tr> <td><strong>Elliptic Curve DH and MQC</strong> [SP80056A]</td> <td>112 bits of security strength</td> <td><em>Deprecated</em> after 2030 <em>Disallowed</em> after 2035</td> </tr> <tr> <td>Â </td> <td>â‰¥ 128 bits of security strength</td> <td><em>Disallowed</em> after 2035</td> </tr> <tr> <td><strong>RSA</strong> [SP80056B]</td> <td>112 bits of security strength</td> <td><em>Deprecated</em> after 2030 <em>Disallowed</em> after 2035</td> </tr> <tr> <td>Â </td> <td>â‰¥ 128 bits of security strength</td> <td><em>Disallowed</em> after 2035</td> </tr> </tbody> </table> <p>Table 5: Post-quantum key-establishment schemes</p> <table> <thead> <tr> <th><strong>Key Establishment Scheme</strong></th> <th><strong>Parameter Sets</strong></th> <th><strong>Security Strength</strong></th> <th><strong>Security Category</strong></th> </tr> </thead> <tbody> <tr> <td><strong>ML-KEM</strong> [FIPS203]</td> <td>ML-KEM-512</td> <td>128 bits</td> <td>1</td> </tr> <tr> <td>Â </td> <td>ML-DSA-768</td> <td>192 bits</td> <td>3</td> </tr> <tr> <td>Â </td> <td>ML-DSA-1024</td> <td>256 bits</td> <td>5</td> </tr> </tbody> </table> <p>Table 6: Block ciphers</p> <table> <thead> <tr> <th><strong>Block Cipher</strong></th> <th><strong>Parameter Sets</strong></th> <th><strong>Security Strength</strong></th> <th><strong>Security Category</strong></th> </tr> </thead> <tbody> <tr> <td><strong>AES</strong> [FIPS197]</td> <td>AES-128</td> <td>128 bits</td> <td>1</td> </tr> <tr> <td>Â </td> <td>AES-192</td> <td>192 bits</td> <td>3</td> </tr> <tr> <td>Â </td> <td>AES-256</td> <td>256 bits</td> <td>5</td> </tr> </tbody> </table> <p>Table 7: Hash functions and XOFs</p> <table> <thead> <tr> <th><strong>Hash/XOF Algorithm Family</strong></th> <th><strong>Variants</strong></th> <th><strong>Collision Security Strength</strong></th> <th><strong>Collision Security Category</strong></th> <th><strong>Preimage Security Strength</strong></th> <th><strong>Preimage Security Category</strong></th> </tr> </thead> <tbody> <tr> <td><strong>SHA-1</strong> [FIPS180]</td> <td>SHA-1</td> <td>80 bits</td> <td>&lt; 1</td> <td>160 bits</td> <td>1</td> </tr> <tr> <td><strong>SHA-2</strong> [FIPS180]</td> <td>SHA-224, SHA-512/224</td> <td>112 bits</td> <td>&lt; 1</td> <td>224 bits</td> <td>3</td> </tr> <tr> <td>Â </td> <td>SHA-256, SHA-512/256</td> <td>128 bits</td> <td>2</td> <td>256 bits</td> <td>5</td> </tr> <tr> <td>Â </td> <td>SHA-384</td> <td>192 bits</td> <td>4</td> <td>384 bits</td> <td>5</td> </tr> <tr> <td>Â </td> <td>SHA-512</td> <td>256 bits</td> <td>5</td> <td>512 bits</td> <td>5</td> </tr> <tr> <td><strong>SHA-3</strong> [FIPS202]</td> <td>SHA3-224</td> <td>112 bits</td> <td>&lt; 1</td> <td>224 bits</td> <td>3</td> </tr> <tr> <td>Â </td> <td>SHA3-256</td> <td>128 bits</td> <td>2</td> <td>256 bits</td> <td>5</td> </tr> <tr> <td>Â </td> <td>SHAKE128</td> <td>128 bits</td> <td>2</td> <td>128 bits</td> <td>2</td> </tr> <tr> <td>Â </td> <td>SHA3-384</td> <td>192 bits</td> <td>4</td> <td>384 bits</td> <td>5</td> </tr> <tr> <td>Â </td> <td>SHA3-512</td> <td>256 bits</td> <td>5</td> <td>512 bits</td> <td>5</td> </tr> <tr> <td>Â </td> <td>SHAKE256</td> <td>256 bits</td> <td>5</td> <td>512 bits</td> <td>5</td> </tr> </tbody> </table> <h3 id="241024-nistir-8528">24.10.24. <a href="https://csrc.nist.gov/pubs/ir/8528/final">NISTIR 8528</a></h3> <p>â€œStatus Report on the First Round of the Additional Digital Signature Schemes for the NIST Post-Quantum Cryptography Standardization Processâ€</p> <h5 id="table-1">[Table 1]</h5> <p>22.07, ì„œëª… í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ì–‘í™”ë¥¼ ìœ„í•´ ì¶”ê°€ ì „ìì„œëª… ëª¨ì§‘í•¨ì„ ê³µê³ .</p> <p>ì´ë¯¸ ê²©ì ê¸°ë°˜ ì„œëª… ë°©ì‹ ë‘ ê°€ì§€ê°€ í‘œì¤€í™”ë˜ì—ˆê¸° ë•Œë¬¸ì—, ê²©ìë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì¶”ê°€ ë²”ìš© ì„œëª… ë°©ì‹ê³¼ ì§§ì€ ì„œëª…ê³¼ ë¹ ë¥¸ ê²€ì¦ ì†ë„ë¥¼ ê°–ì¶˜ ì„œëª… ë°©ì‹ì— íŠ¹ë³„í•œ ê´€ì‹¬ì„ í‘œëª…</p> <p>22.09, call for proposal ë°œí‘œ</p> <p>23.06, 50ê°œ íŒ¨í‚¤ì§€ ì ‘ìˆ˜</p> <p>23.07, 40ê°œê°€ 1ì°¨ ë¼ìš´ë“œ í›„ë³´ë¡œ ìŠ¹ì¸</p> <ul> <li>*1ì°¨ í›„ë³´ ê²°ì • ê¸°ì¤€: C ì½”ë“œ êµ¬í˜„, KAT, ì„œë©´ ì‚¬ì–‘ì„œ, í•„ìˆ˜ ì§€ì  ì¬ì‚°ê¶Œ ì§„ìˆ ì„œ ë“± (ì•ˆì „ì„±, ì„±ëŠ¥, ì•Œê³ ë¦¬ì¦˜ ë° êµ¬í˜„ ë“±ì€ ê³ ë ¤í•˜ì§€ ì•ŠìŒ)**</li> </ul> <p>24.04, 5ì°¨ NIST PQC í‘œì¤€í™” ì»¨í¼ëŸ°ìŠ¤</p> <p>24.10, 14ê°œì˜ ì„œëª… ì•Œê³ ë¦¬ì¦˜ì„ 2ì°¨ ë¼ìš´ë“œ í›„ë³´ë¡œ ì„ ì •</p> <ul> <li>2<em>ì°¨ í›„ë³´ ê²°ì • ê¸°ì¤€: ì•ˆì „ì„±, ì„±ëŠ¥, ê¸°ë°˜ ë¬¸ì œ ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„±</em></li> <li><em>ê²©ì ê¸°ë°˜ì€ CRYSTALS-Dilithiumê³¼ Falcon ëª¨ë‘ë³´ë‹¤</em></li> </ul> <p><em>ì ì–´ë„ í•˜ë‚˜ì˜ ì••ë„ì  ì„±ëŠ¥ ìš°ìœ„ ì œê³µ</em></p> <ul> <li><em>ë¹„ê²©ì ê¸°ë°˜ì€ SPHINCS+ë³´ë‹¤ ì ì–´ë„ í•˜ë‚˜ì˜ ì••ë„ì  ì„±ëŠ¥ ìš°ìœ„ ì œê³µ</em></li> </ul> <p>25.01.17 ê°œì„  íŒ¨í‚¤ì§€ ì œì¶œ ë§ˆê° (í•´ë‹¹ íŒ¨í‚¤ì§€ë¡œ í‰ê°€)</p> <p>25.09 NIST PQC í‘œì¤€í™” ì»¨í¼ëŸ°ìŠ¤ ì˜ˆì •</p> <p>2026 3ë¼ìš´ë“œ í‰ê°€ë¥¼ ìœ„í•œ ê²°ì„  ì§„ì¶œì ì„ ì •</p> <h4 id="table-2-êµµì€-ê¸€ì”¨ê°€-2ë¼ìš´ë“œ-ì„ ì •-ì•Œê³ ë¦¬ì¦˜">[Table 2] êµµì€ ê¸€ì”¨ê°€ 2ë¼ìš´ë“œ ì„ ì • ì•Œê³ ë¦¬ì¦˜</h4> <table> <thead> <tr> <th>ê¸°ë°˜ë¬¸ì œ</th> <th>ì•Œê³ ë¦¬ì¦˜</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>Code-Based</td> <td><strong>CROSS</strong></td> <td><strong>LESS</strong></td> </tr> <tr> <td>Â </td> <td>Enhanced pqsigRM</td> <td>MEDS</td> </tr> <tr> <td>Â </td> <td>FuLeeca</td> <td>WAVE</td> </tr> <tr> <td>Lattice-Based</td> <td>EagleSign</td> <td><strong>HAWK</strong></td> </tr> <tr> <td>Â </td> <td>EHTv4</td> <td>HuFu</td> </tr> <tr> <td>Â </td> <td>HAETAE</td> <td>Raccoon</td> </tr> <tr> <td>Â </td> <td>Â </td> <td>SQUIRRELS</td> </tr> <tr> <td>MPC-in-the-Head</td> <td>Biscuit</td> <td><strong>PERK</strong></td> </tr> <tr> <td>Â </td> <td><strong>Mirath</strong>(MIRA + MiRitH)</td> <td><strong>RYDE</strong></td> </tr> <tr> <td>Â </td> <td><strong>MQOM</strong></td> <td><strong>SDitH</strong></td> </tr> <tr> <td>Multivariate</td> <td>3WISE</td> <td><strong>QR-UOV</strong></td> </tr> <tr> <td>Â </td> <td>DME-Sign</td> <td><strong>SNOVA</strong></td> </tr> <tr> <td>Â </td> <td>HPPC</td> <td>TUOV</td> </tr> <tr> <td>Â </td> <td><strong>MAYO</strong></td> <td><strong>UOV</strong></td> </tr> <tr> <td>Â </td> <td>PROV</td> <td>VOX</td> </tr> <tr> <td>Other</td> <td>ALTEQ</td> <td>PREON</td> </tr> <tr> <td>Â </td> <td>eMLE-Sig 2.0</td> <td>Xifrat1-Sign.I</td> </tr> <tr> <td>Â </td> <td>KAZ-SIGN</td> <td>Â </td> </tr> <tr> <td>Symmetric-Based</td> <td>AIMer</td> <td><strong>FAEST</strong></td> </tr> <tr> <td>Â </td> <td>Ascon-Sign</td> <td>SPHINCS-alpha</td> </tr> <tr> <td>Isogeny-Based</td> <td><strong>SQIsign</strong></td> <td>Â </td> </tr> </tbody> </table> <p><em>ì œì¶œ ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•¨</em></p> <h4 id="ì„ ì •ê¸°ì¤€"><strong>ì„ ì •ê¸°ì¤€</strong></h4> <p><strong>ì„ ì •ê¸°ì¤€1. ì•ˆì „ì„±</strong></p> <p>EUF-CMA ì•ˆì „ì„±, ë³´ì•ˆ ê°•ë„ 1,2ì™€ ê°€ëŠ¥í•œ 3 ì¶©ì¡± ë° 4,5 ì¤‘ í•˜ë‚˜ ì´ìƒ ë§Œì¡±, ë¶€ì±„ë„ ê³µê²©ê³¼ ì˜¤ë¥˜ì£¼ì… ê³µê²©ë“±ì— ëŒ€í•œ ì €í•­ì„±, í”„ë¡œí† ì½œ í˜¸í™˜ì„±</p> <p><strong>ì„ ì •ê¸°ì¤€2. ì„±ëŠ¥ ë° ë¹„ìš©</strong></p> <ul> <li>ê³µê°œ í‚¤ì™€ ì„œëª…ì˜ í¬ê¸°</li> <li>ì„œëª… ë° ê²€ì¦ ì—°ì‚°ì˜ ê³„ì‚° íš¨ìœ¨ì„±, í‚¤ ìƒì„±ì˜ ê³„ì‚° íš¨ìœ¨ì„± (= ì†ë„)</li> <li>ë©”ëª¨ë¦¬: SW êµ¬í˜„ì„ ìœ„í•œ RAM ì‚¬ìš©ëŸ‰ê³¼ HW êµ¬í˜„ì„ ìœ„í•œ ê²Œì´íŠ¸ ìˆ˜</li> </ul> <p><strong>ì„ ì •ê¸°ì¤€3. í™œìš© ìœ ì—°ì„±</strong></p> <p>í”Œë«í¼ í˜¸í™˜ì„±, ë³‘ë ¬ì²˜ë¦¬ ê°€ëŠ¥ì„±</p> <p><strong>ì„ ì •ê¸°ì¤€4. ì§€ì  ì¬ì‚°ê¶Œ</strong></p> <h4 id="2ë¼ìš´ë“œ-ì•Œê³ ë¦¬ì¦˜-ì„ ì •-ì´ìœ "><strong>[2ë¼ìš´ë“œ ì•Œê³ ë¦¬ì¦˜ ì„ ì • ì´ìœ ]</strong></h4> <table> <tbody> <tr> <td>CROSS</td> <td>SLH-DSAë³´ë‹¤ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ë¹„ê²©ì ê¸°ë°˜ ë°©ì‹ ì•ˆì „ì„±ì— ì§‘ì¤‘í•œ ë²„ì „ê³¼ ì„±ëŠ¥ì— ì§‘ì¤‘í•œ ë²„ì „ìœ¼ë¡œ ë‚˜ë‰¨. ëª¨ë“  ë§¤ê°œë³€ìˆ˜ ì§‘í•©ì—ì„œ ê³µê°œ í‚¤ê°€ ì‘ê³ , SLH-DSAì™€ ë¹„ìŠ·í•œ ê²€ì¦ ì†ë„ë¥¼ ë³´ì„. í‚¤ ìƒì„±ì€ ML-DSAì™€ ìœ ì‚¬í•œ ì •ë„ë¡œ ë§¤ìš° ë¹ ë¥´ê³ , ì„œëª… ì‹œê°„ë„ SLH-DSAë³´ë‹¤ ë‚˜ìŒ</td> </tr> <tr> <td>LESS</td> <td>LESS signatures are smaller than SLH-DSA with much larger public keys. ìµœê·¼ ë™ì¼í•œ ê³µê°œ í‚¤ í¬ê¸°ì—ì„œ ë” ì‘ì€ ì„œëª…ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ ì œì•ˆë˜ì–´ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŒ. ì•ˆì „ì„± ì¶”ê°€ ê²€ì¦ í•„ìš”</td> </tr> <tr> <td>HAWK</td> <td>Falconê³¼ ë¬¸ì œ êµ¬ì„± ë° ì„±ëŠ¥ì´ ìœ ì‚¬í•˜ì§€ë§Œ, í‚¤ ë° ì„œëª… í¬ê¸° ì¸¡ë©´ì—ì„œ ë” ìš°ìˆ˜í•˜ë©°, ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚° ì—†ì´ êµ¬í˜„ ê°€ëŠ¥í•¨. ë‹¨, Falconë³´ë‹¤ ì•ˆì „ì„± ì—°êµ¬ê°€ ëœ ì´ë¤„ì§</td> </tr> <tr> <td>Mirath</td> <td>1ë¼ìš´ë“œì˜ ë‘ ì•Œê³ ë¦¬ì¦˜ì€ SLH-DSAì™€ Falcon ì‚¬ì´ì˜ í‚¤ í¬ê¸°ë¥¼ ê°–ëŠ” ë§¤ê°œë³€ìˆ˜ ì„¸íŠ¸ë¥¼ ì œê³µí•˜ë©°, ì„œëª… í¬ê¸°ëŠ” SLH-DSAì™€ ìœ ì‚¬í•¨. í‚¤ ìƒì„± ì„±ëŠ¥ì€ ML-DSAì™€ ìœ ì‚¬í•˜ë©°, ì„œëª… ë° ê²€ì¦ ì„±ëŠ¥ì€ ML-DSAì™€ Falcon ì‚¬ì´ì— ìœ„ì¹˜í•¨. ë‘ ì•Œê³ ë¦¬ì¦˜ í†µí•©ìœ¼ë¡œì¨ ì„œëª… í¬ê¸°ê°€ ê°œì„ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, ê¸°ë³¸ì ì¸ ì•ˆì „ì„± ê°€ì •ì€ ë³€í•˜ì§€ ì•Šì„ ê²ƒìœ¼ë¡œ ë³´ì„</td> </tr> <tr> <td>MQOM</td> <td>ë§¤ìš° ì‘ì€ ê³µê°œ í‚¤. ì„œëª… í¬ê¸°ëŠ” ML-DSAì™€ SLH-DSA ì‚¬ì´ì˜ ì¤‘ê°„ í¬ê¸°. ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„ì™€ ì„œëª… í¬ê¸°ê°€ ê°œì„ ë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨</td> </tr> <tr> <td>PERK</td> <td>ML-DSAë³´ë‹¤ ëŠë¦¬ê³  SLH-DSAì™€ ë¹„ìŠ·í•œ ì†ë„ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒ. PERK ì„œëª…ì˜ í¬ê¸°ëŠ” ML-DSAë³´ë‹¤ í¬ì§€ë§Œ SLH-DSAë³´ë‹¤ëŠ” ìƒë‹¹íˆ ì‘ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ. ì•ˆì „ì„± ë³´ì™„ ì—°êµ¬ í•„ìš”</td> </tr> <tr> <td>RYDE</td> <td>MPCitH ë° VOLEitH ì„œëª… ë°©ì‹ ë²”ì£¼ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, íŠ¹íˆ ìµœê·¼ ê°œì„  ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ ì„ ì •</td> </tr> <tr> <td>SDitH</td> <td>ì „ì²´ì ì¸ ì„±ëŠ¥ì€ SLH-DSAë³´ë‹¤ ìš°ìˆ˜í•˜ì§€ë§Œ, ML-DSAë‚˜ Falconë³´ë‹¤ëŠ” ë–¨ì–´ì§. ë§¤ìš° ì‘ì€ í‚¤. SLH-DSAì˜ ì‘ê³  ë¹ ë¥¸ ë§¤ê°œë³€ìˆ˜ ì§‘í•© ì‚¬ì´ì— ìœ„ì¹˜í•˜ëŠ” ì„œëª… í¬ê¸°. ì¶”ê°€ì ì¸ ì•ˆì „ì„± ë¶„ì„ ë° ì„±ëŠ¥ ê°œì„ ì„ ê¸°ëŒ€í•¨</td> </tr> <tr> <td>UOV</td> <td>ì§§ì€ ì„œëª…ê³¼ ë§¤ìš° ë¹ ë¥¸ ì„œëª… ë° ê²€ì¦ ì†ë„. ì˜¤ëœ ê¸°ê°„ ìˆ˜í–‰ëœ ì•ˆì „ì„± ê²€ì¦. ë‹¨, ë‹¨ì ì€ ê³µê°œ í‚¤ì˜ í¬ê¸°. ì œì¶œ íŒ¨í‚¤ì§€ëŠ” ê³µê°œ í‚¤ í¬ê¸°ì™€ ê²€ì¦ ì†ë„ ê°„ì˜ ì ˆì¶©ì•ˆì„ ì œê³µí•˜ì§€ë§Œ, ì‘ì€ ê³µê°œ í‚¤ì¡°ì°¨ë„ ìƒë‹¹íˆ í¼. ì‘ì€ ì„œëª…, ë¹ ë¥¸ ê²€ì¦, ê·¸ë¦¬ê³  í‚¤ì˜ ì˜¤í”„ë¼ì¸ ì „ì†¡ì´ í•„ìš”í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì ìš©ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒ. ë¹„íŒì  ì•ˆì „ì„± ë¶„ì„ ì¶”ê°€ ì—°êµ¬ í•„ìš”</td> </tr> <tr> <td>MAYO</td> <td>UOVì— ë¹„í•´ ê³µê°œ í‚¤ í¬ê¸°ë¥¼ í¬ê²Œ ì¤„ì´ë©´ì„œ UOVì˜ ì‘ì€ ì„œëª… í¬ê¸°ë¥¼ ìœ ì§€. UOVë§Œí¼ ë¹ ë¥´ì§€ëŠ” ì•Šì§€ë§Œ, ê·¸ë˜ë„ ë§¤ìš° íš¨ìœ¨ì </td> </tr> <tr> <td>QR-UOV</td> <td>ê³µê°œ í‚¤ê°€ UOVì— ë¹„í•´ 50% ë” ì‘ìŒ. QR-UOVì˜ ì„±ëŠ¥ì€ UOVë³´ë‹¤ ëŠë¦¬ì§€ë§Œ ì—¬ì „íˆ ê²½ìŸë ¥ ìˆìŒ. ë‹¨, êµ¬ì¡° ë° ì•ˆì „ì„±ì— ëŒ€í•œ ì—°êµ¬ í•„ìš”</td> </tr> <tr> <td>SNOVA</td> <td>UOVì— ë¹„í•´ í›¨ì”¬ ì‘ì€ ê³µê°œ í‚¤(MAYOì™€ ë¹„ìŠ·í•˜ê±°ë‚˜ ì•½ê°„ ì‘ìŒ), ì œì¶œ ì•Œê³ ë¦¬ì¦˜ ì¤‘ ì•”í˜¸ ë¶„ì„ì— ëŒ€í•´ ê°€ì¥ ì‘ì€ ê³µê°œ í‚¤ì„. ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¥¸ ì„±ëŠ¥ ìœ ì§€. MAYOë³´ë‹¤ ì•½ê°„ ëŠë¦¬ì§€ë§Œ ë¹„ìŠ·í•œ ë²”ìœ„ì˜ ì†ë„. ì•ˆì „ì„±ì€ ì•½ê°„ ì˜ë¬¸</td> </tr> <tr> <td>FAEST</td> <td>ìœ ì‚¬í•œ MPCitH ë°©ì‹ì— ë¹„í•´ ì„œëª… í¬ê¸°ê°€ ìƒë‹¹íˆ ì‘ìŒ. ì´ë¡ ì  ì•ˆì „ì„±ì€ ëŒ€ì¹­ í‚¤ ê°€ì •ì—ë§Œ ê¸°ë°˜í•˜ê³  ìˆì§€ë§Œ, í•´ë‹¹ ì†ì„±ì„ ê°€ì§„ ëŒ€ë¶€ë¶„ì˜ ë‹¤ë¥¸ ë°©ì‹, íŠ¹íˆ SLH-DSAë³´ë‹¤ ì„±ëŠ¥ì´ í›¨ì”¬ ë›°ì–´ë‚¨. ë‹¨, ì„±ëŠ¥ì€ ML-DSAì™€ FN-DSA ê°™ì€ ê²©ì ê¸°ë°˜ ë°©ì‹ê³¼ëŠ” ê²½ìŸë ¥ì´ ì—†ìŒ</td> </tr> <tr> <td>SQIsign</td> <td>ì•Œë ¤ì§„ ê³µê²©ë“¤ì— ì €í•­ì„±ì„ ê°€ì§. 1ì°¨ ë¼ìš´ë“œ í›„ë³´ë“¤ ë° ML-DSAì™€ Falconì„ í¬í•¨í•œ ëª¨ë“  ë°©ì‹ ì¤‘ì—ì„œ ê°€ì¥ ì‘ì€ ê³µê°œ í‚¤ì™€ ì„œëª… í¬ê¸°. ì„œëª… ë° ê²€ì¦ì˜ ê³„ì‚° íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ, ë§¤ìš° ëŠë¦¬ì§€ë§Œ, ê²€ì¦ì€ ì„œëª…ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="Study"/><category term="Cryptography"/><category term="PQC"/><summary type="html"><![CDATA[Study note: AIS 31 (version 3.0)]]></summary></entry><entry><title type="html">Information Theory</title><link href="https://ryuj1eun.github.io/june.github.io/blog/2024/information-theory/" rel="alternate" type="text/html" title="Information Theory"/><published>2024-01-02T00:00:00+00:00</published><updated>2024-01-02T00:00:00+00:00</updated><id>https://ryuj1eun.github.io/june.github.io/blog/2024/information-theory</id><content type="html" xml:base="https://ryuj1eun.github.io/june.github.io/blog/2024/information-theory/"><![CDATA[<p>ğŸ“– Cover, Thomas M.Â <em>Elements of information theory</em>. John Wiley &amp; Sons, 1999.</p> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_153653.png" width="90%" alt="ì¼ë°˜ì ì¸ í†µì‹  ì‹œìŠ¤í…œì˜ ë„ì‹"/></p> <p>ì´ìƒì ì¸ channelì€ transmitterë¡œë¶€í„° receiverê¹Œì§€ ì˜¤ë¥˜ ì—†ì´ ì •ë³´ê°€ ì „ë‹¬ë˜ëŠ” channelì´ì§€ë§Œ, ì¼ë°˜ì ì¸ channelì€ noise channelì„</p> <h3 id="probability">Probability</h3> <p><strong>Event</strong> : ì¼ì–´ë‚  ìˆ˜ ìˆëŠ” ì–´ë–¤ ì‚¬ê±´, í™•ë¥  ë³€ìˆ˜ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’, $X=a$</p> <ul> <li> <p>í™•ë¥  ë³€ìˆ˜ê°€ ì£¼ì–´ì§ â‡” í™•ë¥  ë³€ìˆ˜ë¥¼ ì• â‡” í™•ë¥  ë¶„í¬ë¥¼ ì•</p> <p>ex. ë™ì „ì„ ë˜ì¡Œì„ ë•Œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ë©´ì— ëŒ€í•œ í™•ë¥  ë³€ìˆ˜ X</p> <p>í™•ë¥  ë³€ìˆ˜ Xâˆˆ{ì•ë©´, ë’·ë©´}</p> <p>í™•ë¥  ë¶„í¬ëŠ” P(X=ì•ë©´)=1/2, P(X=ë’·ë©´)=1/2</p> </li> </ul> <p>ê²°ê³¼ë¡œ ë„ì¶œë  ìˆ˜ ìˆëŠ” ëª¨ë“  eventë¥¼ ëª¨ì€ ì§‘í•©ì´ sample spaceì´ê³ , â€˜ì–´ë–¤ eventê°€ ì–¼ë§ˆë‚˜ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ”ê°€?â€™ì— ëŒ€í•œ í™•ë¥ ì´ ì£¼ì–´ì¡Œì„ ë•Œ eventì— ëŒ€í•œ ë³€ìˆ˜ê°€ í™•ë¥  ë³€ìˆ˜ì´ê³  ì´ í™•ë¥  ë³€ìˆ˜ì˜ eventsì— ëŒ€í•œ í™•ë¥ ì´ í™•ë¥  ë¶„í¬ë¥¼ ì´ë£¸</p> <p>â‡’ í™•ë¥  ë³€ìˆ˜ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì€ ì–´ë–¤ eventì˜ í™•ë¥ ì„ ë‹¤ë£¨ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë³€ìˆ˜ì˜ í™•ë¥  ë¶„í¬ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒ</p> <p>í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜(probability mass function) : ì´ì‚° í™•ë¥  ë³€ìˆ˜ì˜ í™•ë¥  ë¶„í¬ì— ë”°ë¥¸ í•¨ìˆ˜</p> <ul> <li> <p>í™•ë¥  ë³€ìˆ˜ $X$ë¥¼ ì‚¬ìš©í•œ í•¨ìˆ˜ $f(X)$ë„ ë³€ìˆ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê°–ëŠ” í™•ë¥  ë³€ìˆ˜ì„</p> <p><strong>í™•ë¥  ë³€ìˆ˜</strong></p> <p>ì‚¬ê±´ ê³µê°„ì—ì„œ ê°€ì¸¡ ê³µê°„ìœ¼ë¡œì˜ ê°€ì¸¡ í•¨ìˆ˜</p> <p>ex. $f(X)= \begin{cases} 1 &amp; \text{ if } X= \textrm{Head}<br/> 0 &amp; \text{ if } X= \textrm{Tail} \end{cases}$ ì´ë•Œ, $X$ëŠ” eventë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜</p> </li> </ul> <p>í™•ë¥  ë³€ìˆ˜ $f(X)$ì˜ í‰ê·  $\mathbb{E}[f(X)]:=\sum P(X=x)f(X=x)$</p> <h4 id="joint-distribution"><strong>Joint distribution</strong></h4> <p>P(X,Y) : joint distribution</p> <p>P(X), P(Y) : marginal distribution</p> <p>â†’ joint distributionì´ ì£¼ì–´ì§€ë©´ marginal distributionì„ êµ¬í•  ìˆ˜ ìˆìœ¼ë‚˜ ì—­ì€ ì„±ë¦½í•˜ì§€ ì•ŠìŒ</p> <table> <tbody> <tr> <td>P(Y</td> <td>X=x) : conditional distribution</td> </tr> </tbody> </table> <p>YëŠ” R.V.ì´ê³ , ì¡°ê±´ë¶€ì˜ xëŠ” value</p> <p>X=2ì¸ í–‰ì— ëŒ€í•œ í™•ë¥ ì€ X=2ì¼ ë•Œì˜ Yì— ëŒ€í•œ ì¡°ê±´ë¶€ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ„</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled.png" alt="marginal dist. and joint dist."/></p> <ul> <li> <p>ex</p> <table> <thead> <tr> <th>Y \ X</th> <th>0</th> <th>1</th> <th>2</th> <th>P(Y)</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1/4</td> <td>1/8</td> <td>1/8</td> <td>1/2</td> </tr> <tr> <td>1</td> <td>1/4</td> <td>0</td> <td>1/4</td> <td>1/2</td> </tr> <tr> <td>P(X)</td> <td>1/2</td> <td>1/8</td> <td>3/8</td> <td>Â </td> </tr> </tbody> </table> </li> </ul> <h3 id="information-and-uncertainty">Information and Uncertainty</h3> <ul> <li> <p><strong>Information : level of surprise</strong></p> <p>â†’ ë‚®ì€ í™•ë¥  = ë†’ì€ entropy</p> <p>ì–¼ë§ˆë‚˜ ì ì€ í™•ë¥ ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì•Œê²Œ ë˜ì—ˆì„ ë•Œ ì–¼ë§ˆë‚˜ ë†€ë¼ì›€ì„ ì£¼ëŠ”ê°€</p> <p>$X$: random variable, $X\in \mathcal{X}= \left{ x_1, \ldots, x_n \right}$</p> </li> <li> <p>The information of an event $X=x_i$ is defiend by $I(x_i) := \log\frac{1}{P(X=x_i)} = -\log p(x_i)$</p> <p>ì •ë³´ëŠ” $P(X=x_i)$ì—ë§Œ dependí•œë‹¤. ì¦‰, $I(x_i)=I(p(x_i))=-\log p(x_i)$</p> <p>ğŸ’¡ ì™œ $\log$ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ê°€?</p> <p>ì •ë³´ëŠ” ë‹¤ìŒì„ ë§Œì¡±í•¨</p> <ol> <li>í™•ë¥ ì´ ë‚®ìœ¼ë©´ ì •ë³´ê°€ ë§ê³ , í™•ë¥ ì´ ë†’ìœ¼ë©´ ì •ë³´ê°€ ì ìŒ</li> <li> <p>$0 \le P(X=x_i) \le 1$</p> <p>$I(x_i)\ge0\quad\because0&lt;p(x)\le1$</p> <p>(í™•ë¥ ì´ 0ì¸ eventëŠ” $\mathcal{X}$ì— ì†í•˜ì§€ ì•ŠìŒ)</p> </li> <li> <p>Fundamental Axioms (axiomatic apprach)</p> <p>$I(p)$ : information measure</p> <ol> <li>$I(p)\ge0$</li> <li>$p=1\quad \Rightarrow \quad I(p)=0$</li> <li> <p>For tow independent events $P(X=x_i), P(Y=y_i)$,</p> <p>$I(X=x_i, Y=y_j)=I(X=x_i)+I(Y=y_j)$</p> <p>because, $P(X=x_i ,\ Y=y_i)=P(X=x_i)(Y=y_i)$</p> </li> </ol> </li> <li>The information measure $I(p)$ is continuous</li> </ol> <p><img src="/june.github.io/assets/img/post_information_theory/PZLMYNWXnYn2pWQVXLCAm7rDpopTP36K_477OXTN8SD2LDdPCXXy-K3JFuJlMHfQRXRSGf7cP9WIUFRtU1-FcMDwuJdBazKIHPQ8psEu_w0CKdiMDVe82MpHK_qxUIluLzc-X_Zfm_S-NWXNkA9GVA.webp" alt="log_a function"/></p> <p>logì˜ ë°‘ì„ 2ë¡œ ì‚¬ìš©í•˜ë©´ ì •ë³´ëŸ‰ì˜ ë¹„íŠ¸ ìˆ˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŒ</p> </li> <li> <p>ë¶„í¬ì˜ information = $\mathbb{E}[I(X)] = \sum_x P(X=x)I(X=x) = -\sum_x p(x)\log p(x)$</p> <p>â€œí™•ë¥  ë³€ìˆ˜ë¥¼ ì• â‡” í™•ë¥  ë¶„í¬ë¥¼ ì•â€ì´ê¸° ë•Œë¬¸ì— R.V.ì˜ informationì„ ì•„ëŠ” ê²ƒì€ ë¶„í¬ì˜ informationì„ ì•„ëŠ” ê²ƒê³¼ ê°™ìŒ</p> <p>ë¶„í¬ì— ëŒ€í•œ informationì€ ê°ê°ì˜ $X=x_i$ì— ëŒ€í•œ informationì„ ë¬¼ì–´ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ëª¨ë“  $x$ì— ëŒ€í•œ informationì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•˜ì—¬ ë¶„í¬ì˜ informationì„ ì •ì˜í•¨</p> </li> </ul> <h4 id="mutual-information">Mutual information</h4> <ul> <li> <p>For two R.V. $X\leftarrow{x_1,x_2,\ldots,x_m},\ Y\leftarrow{y_1,y_2,\ldots,y_m}$,</p> <p>(case 1) $X, Y$ are independent</p> <p>$Y=y_j$ì˜ ë°œìƒì´ $X=x_i$ì— ëŒ€í•œ ì–´ë– í•œ ì •ë³´ë„ ì œê³µí•˜ì§€ ì•ŠìŒ</p> <p>(case 2) $X,Y$ are fully dependent</p> <p>$Y=y_j$ì˜ ë°œìƒì´ $X=x_i$ì˜ ë°œìƒì„ ê²°ì •</p> <p>â†’ í†µì‹  ê³¼ì •ì—ì„œ Y=yë¥¼ ì „ì†¡ ë°›ìœ¼ë©´ ì‹¤ì œë¡œ ë³´ë‚´ì§„ ê°’ Xê°€ xì˜€ìŒì„ ë³´ì¥í•  ìˆ˜ ìˆìŒ</p> <p><em>(case 2)ì˜ ê·¸ë¦¼</em></p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%201.png" alt="(case 2)ì˜ ê·¸ë¦¼"/></p> </li> <li> <p>$I(X;Y)$ : mutual information (average of mutual information $X=x_i, Y=y_j$) â€” for R.V.s</p> <p>$I(X;Y)=\sum_{x,y}p(x,y)I(x,y)$</p> <p>I(x,y)ëŠ” X=x,Y=yì¼ ë•Œì˜ ìƒí˜¸ ì •ë³´ëŸ‰ì„ ë‚˜íƒ€ëƒ„</p> <p>ì¼ë°˜ì ìœ¼ë¡œ joint informationì„ ì •ì˜í•œ ê¸°í˜¸ê°€ ì—†ê¸° ë•Œë¬¸ì— ê³ ì •ëœ $x_i, y_j$ì— ëŒ€í•œ mutual information í‘œê¸°ë¥¼ ;ì´ ì•„ë‹Œ ,ë¡œ ì‚¬ìš©í•¨</p> </li> <li> <table> <tbody> <tr> <td>$I(X;Y):=\sum_{x,y}p(x,y)[I(x)-I(x</td> <td>y)]=\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$</td> </tr> </tbody> </table> <p>(ìœ„ ì‹ì˜ ì²« ë“±ì‹ì— ë‚˜ì˜¤ëŠ” $p(x,y)$ëŠ” $x=x_i,y=y_j$ì— ëŒ€í•œ $p(x_i,y_j)$ì„. $I(x_i;y_j)=I(x_i)-I(x_i|y_j)$) â€” for event</p> <p>Y=yì„ì„ ì•Œê²Œ ë¨ìœ¼ë¡œì¨ ì¤„ì–´ë“œëŠ” xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„(ì •ë³´ëŸ‰)</p> <p>ìƒí˜¸ ì •ë³´ëŸ‰ì´ ë†’ìœ¼ë©´ Yë¥¼ ì•Œ ë•Œ Xë¥¼ ì•Œ í™•ë¥ ì´ ë†’ì•„ì§</p> <table> <tbody> <tr> <td>$I(x)-I(x</td> <td>y) = \log\frac{1}{p(x)}-\log\frac{1}{p(x</td> <td>y)} = \log\frac{p(x</td> <td>y)}{p(x)} = \log\frac{p(x,y)}{p(x)p(y)}$</td> </tr> </tbody> </table> <p>$I(X;Y)=I(Y;X)$</p> </li> <li> <table> <tbody> <tr> <td>$I(X;Y)= H(X)-H(X</td> <td>Y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>pf) $I(X;Y)=\sum_{x,y} p(x,y)\log\frac{p(x,y)}{p(x)p(y)} \ \qquad\qquad\quad = \sum_{x,y} p(x,y)\log \frac{1}{p(x)} - \sum_{x,y}p(x,y)\log\frac{p(y)}{p(x,y)} \ \qquad\qquad\quad = \sum_x p(x) \log\frac{1}{p(x)} - \sum_{x,y}p(x,y)\log\frac{1}{p(x</td> <td>y)} \ \qquad\qquad\quad = H(X)-H(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>ë„¤ ë²ˆì§¸ ë“±í˜¸ ë„˜ì–´ê°ˆ ë•Œ ì•„ë˜ ì°¸ê³ </p> <table> <tbody> <tr> <td>$H(X</td> <td>Y)=\sum_y p(y)H(X</td> <td>Y=y) \ \qquad\qquad = \sum_y p(y)\sum_x p(x</td> <td>y)\log\frac{1}{p(x</td> <td>y)} \ \qquad\qquad = \sum_{x,y}p(x</td> <td>y)p(y)\log\frac{1}{p(x</td> <td>y)} = \sum_{x,y} p(x,y)\log\frac{1}{p(x</td> <td>y)}$</td> </tr> </tbody> </table> <ul> <li> <p>ë³€í˜•</p> <table> <tbody> <tr> <td>$H(X)=H(X</td> <td>Y)+I(X;Y)$ ; Xì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì€ Yë¥¼ ì•Œ ë•Œ Xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„ì™€ Yë¥¼ ì•Œ ë•Œ Xë¥¼ ì•„ëŠ” ì •ë„ì˜ í•©</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <p>ì¢‹ì€ ì±„ë„ = ìƒí˜¸ ì •ë³´ëŸ‰ì´ ë†’ì€ ì±„ë„</p> <ul> <li>Ex 1 <ol> <li> <table> <tbody> <tr> <td>independent : $p(x</td> <td>y)=p(x) \Rightarrow I(x;y)=0$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>fully dependent : $p(x</td> <td>y)=1 \Rightarrow I(x;y)=I(x)$</td> </tr> </tbody> </table> </li> </ol> </li> <li> <p>Ex 2 (<em>binary symmetric channel</em> )</p> <p>$p$ : error rate</p> <p>(case 1)</p> <table> <thead> <tr> <th>X</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>P(X)</td> <td>1</td> <td>0</td> </tr> </tbody> </table> <p>â†’</p> <table> <thead> <tr> <th>Y</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>P(Y)</td> <td>1-p</td> <td>p</td> </tr> </tbody> </table> <p>(case 2)</p> <table> <thead> <tr> <th>X</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>P(X)</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <p>â†’</p> <table> <thead> <tr> <th>Y</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>P(Y)</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_183918.png" alt="ìŠ¤í¬ë¦°ìƒ· 2023-09-21 183918.png"/></p> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_184010.png" alt="ìŠ¤í¬ë¦°ìƒ· 2023-09-21 184010.png"/></p> <table> <tbody> <tr> <td>$P(Y=0)=P(Y=0</td> <td>X=0)P(X=0)+P(Y=0</td> <td>X=1)P(X=1)\ \qquad\qquad\ \ =(1-p)(1/2)+p(1/2)=1/2$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$P(Y=1)=P(Y=1</td> <td>X=0)P(X=0)+P(Y=1</td> <td>X=1)P(X=1)\ \qquad\qquad\ \ =p(1/2)+(1-p)(1/2)=1/2$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X=0;Y=0)=\log\frac{p(x</td> <td>y)}{p(x)}=\log\frac{p(y</td> <td>x)}{p(y)}=\log\frac{1-p}{1/2}=\log 2(1-p)=\log(1-p)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X=0;Y=1)=\log\frac{p(x</td> <td>y)}{p(x)}=\log\frac{p(y</td> <td>x)}{p(y)}=\log\frac{p}{1/2}=\log 2p=\log p$</td> </tr> </tbody> </table> <p>â†’ $pâ†’0$ì¼ ë•Œ $\log p\rightarrow -\infin$ ; ê°œë³„ ìƒí˜¸ ì •ë³´ëŸ‰ì€ ìŒìˆ˜ê°€ ë‚˜ì˜¤ëŠ” ê²½ìš°ë„ ìˆìœ¼ë‚˜, í‰ê· ì„ êµ¬í•˜ë©´ 0ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ë³´ì •ë¨</p> </li> </ul> <h4 id="conditional-mutual-information">Conditional Mutual Information</h4> <table> <tbody> <tr> <td>$I(X;Y\</td> <td>\ Z)=H(X</td> <td>Z)-H(X\,</td> <td>\,\ Y,Z)$</td> </tr> </tbody> </table> <ul> <li> <p><strong>Theorem (Chain rule for mutual information)</strong> $I(X_1,\ldots, X_n;Y)=\sum_{i=1}^n I(X_i;Y\ |\ X_{i-1},\ldots,X_1)$</p> <table> <tbody> <tr> <td>pf) $I(X_1,\ldots,X_n;Y) \ \qquad =H(X_1,\ldots,X_n)-H(X_1,\ldots,X_n\</td> <td>\ Y) \ \qquad = \sum_{i=1}^n H(X_i\</td> <td>\ X_{i-1},\ldots,X_1) -\sum_{i=1}^n H(X_i\</td> <td>\ X_{i-1},\ldots,X_1,Y) \ \qquad = \sum_{i=1}^n [H(X_i\</td> <td>\ X_{i-1},\ldots,X_1) - H(X_i\</td> <td>\ {\color{red}Y},X_{i-1},\ldots,X_1)] \ \qquad = \sum_{i=1}^n H(X_i;Y\</td> <td>\ X_{i-1},\ldots,X_1)$</td> </tr> </tbody> </table> </li> </ul> <h3 id="entropy">Entropy</h3> <p>The Shannon entropy of R.V. $X\sim p(x)$ is defined by</p> <p>$H(X):=-\sum_x p(x)\log p(x)$ ; $\log\frac{1}{p(x)}$ì˜ ê¸°ëŒ€ê°’(í‰ê· )</p> <ul> <li> <p>í™•ë¥  ë³€ìˆ˜ì˜ (í™•ë¥  ë¶„í¬ì˜) í‰ê·  information</p> <p>$H(X):=\mathbb{E}[I(X)]$</p> <ul> <li>ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥  ë³€ìˆ˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë¹„íŠ¸ ìˆ˜</li> <li>ë°ì´í„°ë¥¼ ìµœëŒ€ë¡œ ì••ì¶• ë•Œì˜ í¬ê¸° (ë°”ë¥¸ ë³µì›ì„ ë³´ì¥í•  ìˆ˜ ìˆì–´ì•¼ í•¨)</li> </ul> </li> <li> <p>Example 1.1.2 (<em>xì˜ í™•ë¥ ì´ uniformí•˜ì§€ ì•Šì€ ê²½ìš° code wordì˜ ê¸¸ì´ê°€ ì¤„ì–´ë“œëŠ” ìƒí™©</em> )</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%202.png" alt="example"/></p> <table> <thead> <tr> <th>$x_i$</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>5</th> <th>6</th> <th>7</th> <th>8</th> </tr> </thead> <tbody> <tr> <td>$p(x_i)$</td> <td>1/2</td> <td>1/4</td> <td>1/8</td> <td>1/16</td> <td>1/64</td> <td>1/64</td> <td>1/64</td> <td>1/64</td> </tr> <tr> <td>code</td> <td>0</td> <td>10</td> <td>110</td> <td>1110</td> <td>111100</td> <td>111101</td> <td>111110</td> <td>111111</td> </tr> </tbody> </table> <p>í‰ê·  ì½”ë“œ ê¸¸ì´ = entropy ; $\mathbb{E}[\textrm{code len}] = \mathbb{E}[f(X)]$</p> </li> <li> <p>Example 1.1.3</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>$p$</td> <td>$1-p$</td> </tr> </tbody> </table> <p>$H(X)=H(p)$</p> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_184032.png" alt="1-bit Shannon entropy"/></p> </li> </ul> <h4 id="joint-entropy">Joint entropy</h4> <p>R.V. $Z:= (X,Y) \ \sim\ p(z)=p(x,y):=P[X=x \textrm{ and } Y=y]$</p> <p>$H(Z) = H(X,Y) =\sum_x\sum_y p(x,y)\log\frac{1}{p(x,y)}=\mathbb{E}_{x,y}[-\log(X,Y)]$</p> <h4 id="conditional-entropy">Conditional entropy</h4> <table> <tbody> <tr> <td>$H(Y</td> <td>X)$ ; $X=x_i$ì˜ $x_i$ê°€ ë³€í™”í•  ë•Œ $Y$ì˜ ì—”íŠ¸ë¡œí”¼</td> </tr> </tbody> </table> <p>ğŸ’¡ Shannon entropy : ì •ë³´ëŸ‰ì˜ í‰ê· </p> <table> <tbody> <tr> <td>$H(Y</td> <td>X) := \sum P(X=x_i)H(Y</td> <td>X=x_i)$</td> </tr> </tbody> </table> <h4 id="mutual-entropy">Mutual entropy</h4> <ul> <li> <p>$H(X;Y):=\sum_x\sum_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$</p> <p>Y=yì„ì„ ì•Œê²Œ ë¨ìœ¼ë¡œì¨ ì¤„ì–´ë“œëŠ” xì— ëŒ€í•˜ì—¬ ëª¨ë¥´ëŠ” ì •ë„(ì •ë³´ëŸ‰)</p> <table> <tbody> <tr> <td>$I(x)-I(x</td> <td>y) = \log\frac{1}{p(x)}-\log\frac{1}{p(x</td> <td>y)} = \log\frac{p(x</td> <td>y)}{p(x)} = \log\frac{p(x,y)}{p(x)p(y)}$</td> </tr> </tbody> </table> </li> </ul> <h4 id="theorem-chain-rule">Theorem (Chain Rule)</h4> <table> <tbody> <tr> <td>$H(X,Y)=H(Y)+H(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>joint dist (X,Y)ì˜ ë¶ˆí™•ì‹¤ì„± = Yì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± + Yë¥¼ ì•Œ ë•Œ Xì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±</p> <ul> <li> <p>ì¦ëª…</p> <table> <tbody> <tr> <td>$H(X,Y) \ =-\sum_{x,y}\log p(x,y) \ = -\sum_{x,y} p(x,y)\log p(x)p(y</td> <td>x) \ = -\sum_{x,y} p(x,y)\log p(x) - \sum_{x,y} p(x,y)\log p(y</td> <td>x) \ = -\sum_x p(x)\log p(x) - \sum_{x,y} p(x,y)\log p(y</td> <td>x) \ = H(X)+H(Y</td> <td>X)$</td> </tr> </tbody> </table> </li> </ul> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%203.png" alt="conditional entropy"/></p> <ul> <li> <p><strong>Corollary (Diagram about Entropy &amp; M.I.)</strong></p> <table> <tbody> <tr> <td>$H(X,Y\,</td> <td>\,Z)=H(X</td> <td>Z)+H(X\,</td> <td>\,Y,Z)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X;Y)=H(X)-H(X</td> <td>Y)\ =I(Y;X)=H(Y)-H(Y</td> <td>X)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$H(X,Y)=H(X)+H(Y</td> <td>X)\ =H(Y,X)=H(Y)+H(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>$I(X;X)=H(X)$</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%204.png" alt="entropy diagram"/></p> </li> </ul> <p><strong>Chain rule for entropy</strong></p> <p>$X_1, X_2, \ldots, X_n$ are R.V.s $\sim\ p(x_1, \ldots, x_n)$</p> <table> <tbody> <tr> <td>$H(X_1, \ldots, X_n)=\sum_{i=1}^n H(X_i</td> <td>X{i-1},\ldots, X_1)$</td> </tr> </tbody> </table> <ul> <li> <p>pf 1</p> <table> <tbody> <tr> <td>$H(X_1,X_2)=H(X_1)+H(X_2</td> <td>X_1)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$H(X_1,X_2,X_3)=H(X_1)+H(X_2,X_3</td> <td>X_1)\ \qquad\qquad\qquad\ \ = H(X_1) + H(X_2</td> <td>X_1) + H(X_3</td> <td>X_2,X_1)$ (by corollary)</td> </tr> </tbody> </table> <p>â€¦</p> <table> <tbody> <tr> <td>$H(X_1,\ldots,X_n)\ \quad =H(X_1)+H(X_2,\ldots,X_n</td> <td>X_1) \ \quad = H(X_1)+H(X_2</td> <td>X_1)+ H(X_3</td> <td>X_2,X_1) + \cdots + H(X_n</td> <td>X_{n-1},\ldots,X_1) \ \quad = \sum_{i=1}^n H(X_i</td> <td>X_{i-1},\ldots,X_1)$</td> </tr> </tbody> </table> </li> <li> <p>pf 2</p> <table> <tbody> <tr> <td>$p(x_1,x_2) = p(x_1)p(x_2</td> <td>x_1)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$p(x_1,x_2,x_3)=p(x_1,x_2)p(x_3</td> <td>x_1,x_2)=p(x_1)p(x_2</td> <td>x_1)p(x_3</td> <td>x_1,x_2)$</td> </tr> </tbody> </table> <p>â€¦</p> <table> <tbody> <tr> <td>$p(x_1,x_2,\ldots,x_n)=\prod_{i=1}^n p(x_i</td> <td>x_{i-1},\ldots,x_1)$</td> </tr> </tbody> </table> <p>Then,</p> <table> <tbody> <tr> <td>$H(X_1,\ldots,X_n)=-\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\log p(x_1,\ldots,x_n) \ \qquad\qquad\qquad\ \ \ = -\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\log \prod_{i=1}^n p(x_i</td> <td>x_{i-1},\ldots,x_1) \ \qquad\qquad\qquad\ \ \ = -\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n)\sum_{i=1}^n \log p(x_i</td> <td>x_{i-1},\ldots,x_1) \ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n\sum_{x_1,\ldots,x_n}p(x_1,\ldots,x_n) \log p(x_i</td> <td>x_{i-1},\ldots,x_1) \ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n\sum_{x_1,\ldots,{\color{red} x_i}}p(x_1,\ldots,{\color{red} x_i}) \log p(x_i</td> <td>x_{i-1},\ldots,x_1) \ \qquad\qquad\qquad\ \ \ = -\sum_{i=1}^n H(X_i</td> <td>X_{i-1},\ldots,X_1)$</td> </tr> </tbody> </table> </li> </ul> <h4 id="relative-entropy">Relative entropy</h4> <ul> <li> <p>$D(p\parallel q) = \sum_i p(x_i)\log\frac{p(x_i)}{q(x_i)}$ â€” pì™€ qì˜ ì°¨ì´</p> <p>$\neq D(q\parallel p)$ (in general)</p> <ul> <li>A measure of the distance between two distributions $p$ and $q$</li> <li>$p$ê°€ ì¤‘ì‹¬ì´ ë˜ì–´ ë°”ë¼ë³¼ ë•Œ $q$ì˜ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ê°€</li> </ul> <p>R.V. distribution</p> <ul> <li>$X\sim p \; \wedge \; \widetilde{X}\sim q$</li> <li>$\chi={ x_1, \ldots, x_n }$ (the same sample space) â‡’ ì„œë¡œ ë‹¤ë¥¸ sample spaceì—ì„œ ì •ì˜ëœ ë³€ìˆ˜ëŠ” ë¹„êµ ëŒ€ìƒì´ ë  ìˆ˜ ì—†ìŒ</li> </ul> <p>$q(x_i):=P(\widetilde{X}=x_i)$</p> <p>$p(x_i):=P({X}=x_i)$</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%205.png" alt="prob dist."/></p> </li> <li> <p>$\sum p(x)\log\frac{1}{p(x)} + \sum p(x)\log\frac{p(x)}{q(x)}=\sum p(x)\log\frac{1}{q(x)}$</p> <p>$p$ : true distribution (unknown) $\wedge$ $q$ : approximation (guessing) of $p$</p> <p>If we can construct a â€œgood codeâ€ for $X\sim p$, then the average cod length = $H(p)$</p> <p>Instead, we use (of mis-use) a code for $X\sim q$, then the average code length = $\sum p(x)\log\frac{1}{q(x)}$ &gt; $H(p)$</p> <p>â‡’ We need $H(p)+D(p\parallel q)$ bits on the average.</p> <p>í˜„ì‹¤ì—ì„œ code wordë¥¼ ë¶€ì—¬í•˜ì—¬ ì½”ë”©í•  ë•Œ í•„ìš”í•œ í‰ê·  ë¹„íŠ¸ ìˆ˜ëŠ” ìµœì†Œ ìš”êµ¬ ë¹„íŠ¸ ìˆ˜ + ì˜ëª» ë¶€ì—¬í•¨ì— ì˜í•œ ì¶”ê°€ ë¹„íŠ¸ ìˆ˜</p> </li> <li> <p>ex. Code word</p> <table> <thead> <tr> <th>$x_i$</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>5</th> <th>6</th> <th>7</th> <th>8</th> </tr> </thead> <tbody> <tr> <td>$p(x_i)$</td> <td>1/2</td> <td>1/4</td> <td>1/8</td> <td>1/16</td> <td>1/64</td> <td>1/64</td> <td>1/64</td> <td>1/64</td> </tr> <tr> <td>code</td> <td>0</td> <td>10</td> <td>110</td> <td>1110</td> <td>111100</td> <td>111101</td> <td>111110</td> <td>111111</td> </tr> </tbody> </table> <p>sequence of R.V.s: $X_1, X_2,\ldots,X_n \textrm{ where } X_i\sim P(X)$</p> <p>â‡’ ë³€ìˆ˜ì˜ ì ˆë°˜ì€ 1, ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ 2, â€¦ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê°’ì´ ë‚˜ì˜¬ ê²ƒì„ ì˜ˆìƒí•  ìˆ˜ ìˆìŒ</p> <p>ì£¼ì–´ì§„ ìƒí™©ì—ì„œ $H(X) = - \sum p(x)\log p(x)$ì´ê³ , ì—”íŠ¸ë¡œí”¼ëŠ” eventì— ëŒ€í•œ informationì˜ í‰ê· ì´ë©° ë™ì‹œì— ìµœì í™”ëœ (minimized) code lengthì˜ í‰ê· ì„</p> <p>ê²°êµ­ informationì´ì code lengthì¸ $\log\frac{1}{p(x)}$ê°€ ë³€ìˆ˜ë¥¼ ë½‘ì„ ë•Œë§ˆë‹¤ ê³„ì‚°ë¨ â‡’ $X_1,\ldots,X_n$ ì¤‘ ì ˆë°˜ì€ code lengthê°€ 1, ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ 2, â€¦ ê°€ ë¨ $H(X)=2$ì´ë¯€ë¡œ $X_1$ë¶€í„° $X_n$ê¹Œì§€ í‘œí˜„í•˜ê¸° ìœ„í•œ ë¹„íŠ¸ëŠ” í‰ê· ì ìœ¼ë¡œ $2\cdot n$ bitsê°€ í•„ìš”í•¨</p> <p>R.V.ì˜ sequenceë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©´ ì´ sequenceë¥¼ ì–´ëŠ ì •ë„ì˜ ê¸¸ì´ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ê°€ëŠ” ë¶„í¬ì— ì˜í•´ ì •í•´ì§€ê³ , ìµœì í™”ëœ code lengthì¸ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒì´ Shannonâ€™s entropy ê°’ì„</p> </li> <li> <p>ì„¤ëª…</p> <p>known $X_1, X_2,\ldots,X_n \sim p(x) \Rightarrow \log\frac{1}{p(x)}$; code length $\Rightarrow \sum p(x)\log{1}{p(x)}$</p> <p>í™•ë¥  ë¶„í¬ë¥¼ ì•Œë©´ ìµœì í™”ëœ code wordì˜ ê¸¸ì´ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ</p> <p>ë°˜ëŒ€ë¡œ fixed $p(x)$ë¥¼ ëª¨ë¥´ëŠ” ê²½ìš°, $q(x)$ë¡œì¨ code wordì˜ ê¸¸ì´ë¥¼ guessingí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•¨</p> <p>$q(x)\Rightarrow \log\frac{1}{q(x)}$</p> <p>â‡’ ìµœì í™” code word ê¸¸ì´ = ì‹¤ì œ ë‚˜ì˜¬ í™•ë¥  $\times$ ë¶€ì—¬í•œ ì½”ë“œ ê¸¸ì´ $\Rightarrow\sum q(x)\log\frac{1}{q(x)}$</p> <p>$p(x)$ë¥¼ ëª¨ë¥´ëŠ” ì±„ $X_1, X_2,\ldots,X_n$ì„ ë½‘ì•„ì„œ average lengthë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŒ</p> <p>ì‹¤ì œë¡œ êµ¬í•´ì„œ ê³„ì‚°í•˜ê²Œ ë˜ëŠ” ê°’ì€ $\sum p(x)\log\frac{1}{p(x)}$ì´ë¯€ë¡œ ë‘ ê°’ì˜ ì°¨ì´ë¥¼ ì•Œ ìˆ˜ ìˆìŒ</p> <p>$D(p\parallel q) :=\sum p(x)\log\frac{1}{q(x)}-\sum p(x)\log\frac{1}{p(x)}=\sum p(x)\log\frac{p(x)}{q(x)}$</p> </li> <li> <p>Mutual informationê³¼ì˜ ê´€ê³„</p> <p><strong>Mutual inforamtion</strong></p> <table> <tbody> <tr> <td>$I(x_i\,;y_j)=I(x_i)-I(x_i</td> <td>y_j)=\log\frac{1}{p(x_i)}-\log\frac{1}{p(x_i</td> <td>y_i)}=\log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X;Y)\;=\sum p(x_i,y_j)I(x_i\,;y_j)\ \qquad\qquad =\sum p(x_i,y_j)\log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}=D(p(x,y)\parallel p(x)p(y))$ â€” joint</td> <td>Â </td> <td>product</td> </tr> </tbody> </table> <p>$\therefore I(X;Y)=0 \Leftrightarrow p(x,y)=p(x)p(y) \textrm{ for } \forall x,y$ â€” independent</p> <p>â‡’ joint ë¶„í¬ì™€ productì˜ ë¶„í¬ì˜ ì°¨ì´ê°€ 0</p> </li> </ul> <p>ğŸ’¡ $0\log\frac{0}{0} \rightarrow 0$ $0\log\frac{0}{g}\rightarrow 0$ $p\log\frac{p}{0}\rightarrow\infin$</p> <p>define $p(x), q(x)$ for $\exist \, x\in\chi$ s.t. $p(x)&gt;0, q(X)=0$ then $D(p\parallel q)=\infin$</p> <ul> <li> <p>ex. $D(p\parallel q) \neq D(q\parallel q)$</p> <p>$\chi={0,1}$</p> <p>$H(p)=-\sum p(x)\log p(x) \ \qquad\;\;= -(1-r)\log(1-r) \ \qquad\qquad-r\log r$</p> <p>$H(p)\leftarrow$</p> <p>$H(q)\leftarrow$</p> <p>$H(q) = -(1-s)\log(1-s)=s\log s$</p> <table> <thead> <tr> <th>x</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>p(x)</td> <td>1-r</td> <td>r</td> </tr> <tr> <td>q(x)</td> <td>1-s</td> <td>s</td> </tr> </tbody> </table> <p>For $r=\frac{1}{2},s=\frac{1}{4}$,</p> <p>$D(p\parallel q)=\sum p(X)\log\frac{p(x)}{q(x)}=\frac{1}{2}\log\frac{1/2}{3/4}+\frac{1}{2}\log\frac{1/2}{1/4}\approx 0.2075$</p> <p>and $D(q\parallel q)=\frac{3}{4}\log\frac{3/4}{1/2}+\frac{1}{4}\log\frac{1/4}{1/2}\approx 0.1887$</p> </li> </ul> <h4 id="jensens-inequality">Jensenâ€™s inequality</h4> <p>convex ë˜ëŠ” concaveë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œëŠ” ì¡°ê±´ì„ ë§Œì¡±í•˜ë„ë¡ êµ¬ê°„ì„ ì„¤ì •í•´ ì£¼ì–´ì•¼ í•¨</p> <p><strong>Def: Convex</strong></p> <p>$f:(a,b)\rightarrow\mathbb{R}$ is convex if for every $x_1, x_2\in (a,b),$</p> <p>$f(\lambda x_1+(1-\lambda)x_2)\le \lambda f(x_1)+(1-\lambda)f(x_2)$ where $0\le \forall\lambda \le 1$</p> <p>(êµ¬ê°„ ë‚´ x=aì¼ ë•Œ, ê³¡ì„  ìœ„ point â‰¤ êµ¬ê°„ ë‚´ x=aì¼ ë•Œ, ì§ì„  ìœ„ì˜ point)</p> <p><strong>NOTE:</strong> $\lambda x_1 + (1-\lambda)x_2 \Rightarrow$ $x_1$,$x_2$ë¥¼ $\lambda:1-\lambda$ë¡œ ë‚´ë¶„í•˜ëŠ” ì </p> <p><strong>Def: Concave</strong></p> <p>$f:(a,b)\rightarrow\mathbb{R}$ is concave if $-f$ is convex</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%206.png" alt="convex"/></p> <p>convex</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%207.png" alt="concave"/></p> <p>concave</p> <ul> <li> <p><strong>Theorem</strong> $f\in C^2$ ($fâ€™,fâ€™â€™$ exist and are continuous) and $fâ€™â€˜(x)\ge0$ on $(a,b)$, then $f$ is convex</p> <p>pf: $f(x)=f(x_0)+fâ€™(x_0)(x-x_0)+(fâ€™â€˜(x^<em>)/2)(x-x_0)^2,\quad x^</em>\in[x_0,x]$</p> <p>$(fâ€™â€˜(x^*)/2)(x-x_0)^2$ì´ $y=ax^2+bx+c,a&gt;0$ ê¼´ë¡œ convex</p> <p>$x_1, x_2$ : arbitrarily given points</p> <p>Let $x_0:=\lambda x_1 + (1-\lambda)x_2\;\; (0\le\lambda\le1)$</p> <p>$f(x_1)=f(x_0)+fâ€™(x_0)(x_1-x_0)$ $\qquad\qquad+(fâ€™â€˜(x_1^*)/2)(x_1-x_0)^2$</p> <p>($x_1^*$ lines between $x_0, x_1$)</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%208.png" alt="Jenson"/></p> <p>$f(x_2)=f(x_0)+fâ€™(x_0)(x_2-x_0)+(fâ€™â€˜(x_2^*)/2)(x_2-x_0)^2$</p> <p>($x_2^*$ lines between $x_0, x_1$)</p> <p>Since $fâ€™â€˜(x_1^<em>)\ge0,fâ€™â€˜(x_2^</em>)\ge0$,</p> <ol> <li>$f(x_1)\ge f(x_0)+fâ€™(x_0)(x_1-x_0)$</li> <li>$f(x_2)\ge f(x_0)+fâ€™(x_0)(x_2-x_0)$</li> </ol> <p>$\lambda f(x_1)+(1-\lambda)f(x_2)\ \quad \ge \lambda f(x_0)+\lambda fâ€™(x_0)(x_1-x_0) +(1-\lambda)f(x_2)+(1-\lambda)fâ€™(x_0)(x_2-x_0)\ \quad = f(x_0)fâ€™(x_0)[\lambda x_1 -\lambda x_0+(1-\lambda)x_2 -(1-\lambda)x_0]\ \quad = f(x_0) + fâ€™(x_0)[\lambda x_1+(1-\lambda)x_2-x_0] \ \quad =f(x_0)+fâ€™(x_0)[x_0-x_0] = f(x_0)\ \quad = f(\lambda x_1 + (1-\lambda)x_2)$</p> <p>$\therefore f$ is convex</p> </li> <li> <p><strong>Thm: Jensenâ€™s inequality</strong> $f$ : convex and $X$ : R.V. then $\mathbb{E}[f(X)]\ge f(\mathbb{E}[X])$</p> <p>pf: We use mathematical induction.</p> <table> <tbody> <tr> <td>Let $\chi_2 ={ x_1,x_2 },\ldots,\chi_k={ x_1,x_2,\ldots,x_n} \textrm{ where }</td> <td>\chi_i</td> <td>=i \textrm{ for all } 2\le i \le k$</td> </tr> </tbody> </table> <p>case 1) $\chi = {x_1, x_2}$</p> <p>$\;\mathbb{E}[f(X)] = p_1 f(x_1) + p_2 f(x_2)$</p> <p>$\qquad\qquad = p_1f(x_1)+(1-p_1)f(x_2)$</p> <table> <thead> <tr> <th>$x$</th> <th>$x_1$</th> <th>$x_2$</th> </tr> </thead> <tbody> <tr> <td>$p(x)$</td> <td>$p_1$</td> <td>$p_2$</td> </tr> </tbody> </table> <p>$\qquad\qquad\quad\; \ge f(p_1x_1 + (1-p_1)x_2)$ since $f$ is convex</p> <p>$\qquad\qquad =f(\mathbb{E}[X])$</p> <table> <tbody> <tr> <td>case 2) Suppose that Theorem is true for $</td> <td>\chi</td> <td>\le k-1$</td> </tr> </tbody> </table> <p>$X\sim{$ $\chi:={x_1,\ldots,x_k}$ and $p_i:=P[X=x_i] \;\ (i=1,\ldots,k)\ }$ are given.</p> <p>Define a distribution $Xâ€™$ with $\chi_{k-1}:={x_1,\ldots,{\color{red}x_{k-1}}}$ where $p_iâ€™:=P(X=x_i)=\frac{p_i}{1-p_k} \;\ (i=1,\ldots,k-1)$</p> <p>$\,\mathbb{E}[f(X)]=\sum p_i f(x_i) = p_k f(x_k)+\sum_{i=1}^{k-1}p_i f(x_i)$</p> <p>$\qquad\qquad=p_k f(x_k)+(1-p_k)\sum_{i=1}^{k-1}\frac{p_i}{1-p_k}f(x_i)$</p> <p>$\qquad\qquad=p_kf(x_k)+(1-p_k)\sum_{i=1}^{k-1}p_iâ€™ f(x_i)$</p> <p>$\qquad\qquad=p_k f(x_k) + (1-p_k)\mathbb{E}[f(Xâ€™)]$</p> <table> <tbody> <tr> <td>$\qquad\qquad\ge p_k f(x_k)+(1-p_k)f\left(\sum_{i=1}^{k-1} p_iâ€™ x_i\right)$ for $</td> <td>\chi</td> <td>\le k-1$</td> </tr> </tbody> </table> <p>$\qquad\qquad\ge f\left(p_k x_k + (1-p_k)\sum_{i=1}^{k-1} p_iâ€™ x_i\right)$ by $f$ : convex</p> <p>$\qquad\qquad=f\left(p_k x_k + (1-p_k)\sum_{i=1}^{k-1}\frac{p_i}{1-p_k}x_i\right)$</p> <p>$\qquad\qquad=f\left(\sum_{i=1}^k p_i x_i\right)$</p> <p>$\qquad\qquad=f(\mathbb{E}[X])$</p> </li> <li> <p>Theorem: Information inequality $D(p\parallel q)\ge 0$ , $I(X;Y)\ge0$ , $H(X)\le\log|\chi|$</p> <p>$p(x), q(x)$ are p.m.f. (probability mass function) on $x\in\chi$</p> <ol> <li>$D(p\parallel q)\ge 0$ (equality holds $\iff$ $p(x)\equiv q(x)$ <ul> <li> <p>pf</p> <p>$supp(f)$ : support of $f$</p> <table> <tbody> <tr> <td>$supp(f) := \overline{{x</td> <td>f(x)&gt;0}}={x</td> <td>f(x)&gt;0}$ where $x$ is discrete</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Let $A:={x</td> <td>p(x)&gt;0}$ (i.e. $A:= supp(p)$)</td> </tr> </tbody> </table> <p>$\;-D(p\parallel q) = -\sum_{x\in\chi} p(x) \log\frac{p(x)}{q(x)} = -\sum_{x\in A} p(x)\log\frac{p(x)}{q(x)}$</p> <p>$\qquad\qquad\quad= \sum_{x\in a} p(x) \log\frac{q(x)}{p(x)}\;\;\cdots\;\; \mathbb{E}\left[\log\frac{q(X)}{p(X)}\right]$</p> <p>$\qquad\qquad\quad\le\log\left[ \sum_{x\in A} p(x)\frac{q(x)}{p(x)} \right]$ by Jensenâ€™s inequality</p> <p>$\qquad\qquad\quad=\log\left[ \sum_{x\in A} q(x) \right]$</p> <p>$\qquad\qquad\quad\le\log\left[\sum_{x\in\chi} q(x)\right]=\log 1=0$</p> <p>$\therefore\ D(p\parallel q)\ge0$</p> </li> </ul> </li> <li>$I(X;Y)\ge0$ (equality holds $\iff$ $X,Y$ are independent <ul> <li> <p>pf</p> <p>$I(X;Y)=D\left(p(x,y)\parallel p(x)q(y)\right)\ge0$</p> </li> </ul> </li> <li> <table> <tbody> <tr> <td>$X$: R.V. $x\in\chi$, $H(X)\le\log</td> <td>\chi</td> <td>$ (entropyëŠ” distributionì´ uniformì¼ ë•Œ ìµœëŒ€)</td> </tr> </tbody> </table> <ul> <li> <p>pf</p> <table> <tbody> <tr> <td>Let $u(x):=\frac{1}{</td> <td>\chi</td> <td>}$ (constant function on $\chi$)</td> </tr> </tbody> </table> <p>$u(x)$ is uniform p.m.f. over $\chi$.</p> <p>Then for any R.V. $X\sim P$</p> <p>$D(p\parallel q)=\sum p(x)\log\frac{p(x)}{u(x)}$</p> <table> <tbody> <tr> <td>$\qquad\qquad=\sum p(x)\log</td> <td>\chi</td> <td>+\sum p(x)\log p(x)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\qquad\qquad=\log</td> <td>\chi</td> <td>-\sum p(x)\log\frac{1}{p(x)}$ since $\log</td> <td>\chi</td> <td>$ is constant and $\sum p(x)=1$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\qquad\qquad=\log</td> <td>\chi</td> <td>-H(X)\ge0$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\therefore H(X)\le\log</td> <td>\chi</td> <td>$</td> </tr> </tbody> </table> <p>â‡’ entropy of $X$ is smaller than Entropy of uniform disribution</p> </li> </ul> </li> </ol> </li> <li> <p>Theorem: Conditioning reduces entropy $H(X|Y)\le H(X)$</p> <table> <tbody> <tr> <td>pf: $0\le I(X;Y)=H(X)-H(X</td> <td>Y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\therefore\ H(X)\ge H(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>ex. $H(X) = H\left( \frac{1}{8},\frac{7}{8} \right)$ or</p> <p>$\qquad\quad = H(p) \textrm{\,\ where\,\ } p=\frac{1}{8}, q=1-p$</p> <table> <tbody> <tr> <td>$H(X</td> <td>Y=2)=H\left(\frac{1}{2},\frac{1}{2}\right)=1$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\therefore H(X)\ll H(X</td> <td>Y=2)$</td> </tr> </tbody> </table> <p>â‡’ ì •ë¦¬ì— ëª¨ìˆœì¸ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ,</p> <p>ì •ë¦¬ëŠ” averageì— ëŒ€í•œ ë‚´ìš©ì´ê³ </p> <p>ì˜ˆì œëŠ” single term $Y=2$ë¼ëŠ”</p> <p>íŠ¹ì • ìƒí™©ì— ëŒ€í•œ ê²°ê³¼ì„</p> <table> <thead> <tr> <th>$Y$ \ $X$</th> <th>1</th> <th>2</th> <th>$P(Y)$</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0</td> <td>3/4</td> <td>3/4</td> </tr> <tr> <td>2</td> <td>1/8</td> <td>1/8</td> <td>1/4</td> </tr> <tr> <td>$P(X)$</td> <td>1/8</td> <td>7/8</td> <td>Â </td> </tr> </tbody> </table> </li> <li> <p>Theorem: independence bound $H(X_1,\ldots, X_n)\le\sum_{i=1}^n H(X_i)$ (equality â‡” independence)</p> <p>pf: By the Chain rule,</p> <table> <tbody> <tr> <td>$\;H(X_1,\ldots,X_n)=\sum_{i=1}^n H(X_i</td> <td>X_{i-1},\ldots,X_1)$</td> </tr> </tbody> </table> <p>$\qquad\qquad\qquad\quad\le \sum_{i=1}^n H(X_i)$ by previous theorems</p> </li> </ul> <h2 id="log-sum-inequality-and-its-applications">Log-sum inequality and its Applications</h2> <h3 id="theorem-log-sum-inequality">Theorem: Log-sum inequality</h3> <ul> <li> <p>For non-negative numbers $a_1,\ldots,a_n$ and $b_1,\ldots, b_n$, $\sum_{i=1}^n a_i\left(\log\frac{a_i}{b_i}\right) \ge \left(\sum_{i=1}n a_i\right)\cdot\log\left( \sum_{i=1}^n a_i/\sum_{i=1}^n b_i \right)$ (equality â‡” $\frac{a_i}{b_i}$ is constant)</p> <p>pf: case 1) For $a_i\ge0, b_i&gt;0$.</p> <p>Let $f(t):= t\log t$, then $f(t)$ is convex.</p> <p>ğŸ’¡ $t\times a&gt;t\textrm{\;\; as\;\;} t\rightarrow\infin \textrm{\;\; where\;\;} a&gt;1$</p> <p>Since $(\log t)â€™=\left( \frac{\log_e t}{\log_e 2} \right)â€™=\frac{1}{\log_e 2}\cdot\frac{1}{t}$,</p> <p>$fâ€™(t)=\log t+\log e$</p> <p>$fâ€™â€˜(t)=\frac{1}{\log_e 2}\cdot\frac{1}{t}=\frac{\log e}{t} &gt; \frac{1}{t}&gt;0$</p> <p>$\therefore\ fâ€™â€˜(t)&gt;0 \;\Rightarrow \;f$ is convex</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%209.png" alt="Graph shape"/></p> <p>By Jensenâ€™s inequality $\mathbb{E}[f(X)] \ge f(\mathbb{E}[X])$ for convex $f$,</p> <p>$\sum \alpha_i f(t_i) \ge f\left(\sum \alpha_i t_i\right)$</p> <p>for $X\leftarrow t_i, P_i\leftarrow \alpha_i$ where $\sum \alpha_i = \sum p_i =1, \alpha_i\ge0, p_i\ge0$</p> <p>Set $\alpha_i = \frac{b_i}{\sum_j b_j}$ because we need to property $\sum_i \alpha_i=\sum_i\frac{b_i}{\sum_j b_j} = \frac{1}{\sum_j b_j}\left(\sum_i b_i\right) = 1$.</p> <p>set $t_i=\frac{a_i}{b_i}$</p> <p>Then we obtain $\sum_i \alpha_i f(t_i)=\sum_i\frac{b_i}{\sum_j b_j}\,f!\left(\frac{a_i}{b_i}\right) = \sum_i\frac{b_i}{\sum_j b_j}\frac{a_i}{b_i}\log\frac{a_i}{b_i}$</p> <p>$\;\,\,\qquad\qquad\qquad\qquad\qquad = \sum_i\frac{a_i}{\sum_j b_j}\log\frac{a_i}{b_i} = \frac{1}{\sum_j b_j}\sum_i a_i\log\frac{a_i}{b_i}$</p> <p>$\;\,\,\qquad\qquad\qquad\qquad\qquad \ge f\left( \sum_i \alpha_i t_i \right)$ by Jensenâ€™s inequality</p> <p>$\;\,\,\qquad\qquad\qquad\qquad\qquad = \left[ \sum_i \left( \frac{b_i}{\sum_j b_j} \right)\frac{a_i}{b_i} \right] \log \left[ \sum_i \left( \frac{b_i}{\sum_j b_j} \right)\frac{a_i}{b_i} \right]$</p> <p>$\;\,\,\qquad\qquad\qquad\qquad\qquad = \frac{1}{\sum_j b_j} \sum_i a_i \log \left( \frac{1}{\sum_j b_j}\sum_i a_i \right)$</p> <p>In this inequality, $\frac{1}{\sum_j b_j}\sum_i a_i\log\frac{a_i}{b_i} \ge \frac{1}{\sum_j b_j} \sum_i a_i \log \left( \frac{1}{\sum_j b_j}\sum_i a_i \right)$</p> <p>$\therefore\; \sum_i a_i\log\frac{a_i}{b_i} \ge \sum_i a_i \log \left( \frac{\sum_i a_i}{\sum_i b_i}\right)$</p> </li> </ul> <h4 id="applying-log-sum-inequality">Applying Log-sum inequality</h4> <ul> <li> <p>$D(p\parallel q)\ge0$ where $p ,q$ are p.m.f.</p> <p>Relatibe entropy $D(p\parallel q):=\sum_x p(x)\log\frac{p(x)}{q(x)}\ge\sum_x p(x)\log\frac{\sum_x p(x)}{\sum_x q(x)} =\sum_x p(x)\log1=0$</p> <p>$\therefore \; D(p\parallel q)\ge0$ (equality â‡” $\frac{p(x)}{q(x)}=1$ for all $x$)</p> </li> <li> <p><strong>Thm: Convexity of relative entropy</strong> $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)\le \lambda D(p_1\parallel q_1) + (1-\lambda)D(p_2\parallel q_2)$</p> <p>$D(p\parallel q)$ is convex in the pair $(p,q)$ where $p,q$ are p.m.f. i.e. If $(p_1, q_1), (p_2,q_2)$ are two pairs of p.m.f. and $0\le \lambda\le1$, then $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)\le \lambda D(p_1\parallel q_1) + (1-\lambda)D(p_2\parallel q_2)$</p> <p>pf: First, we check $\lambda p_1+(1-\lambda) p_2$ and $\lambda p_1+(1-\lambda) p_2$ are p.m.f.</p> <p>Itâ€™s trivial!</p> <p>Next, $D(\lambda p_1+(1-\lambda)p_2\parallel \lambda q_1+(1-\lambda)q_2)$</p> <p>$\qquad := \sum_x\left[ \lambda p_q(x)+(1-\lambda) p_2(x) \right]\log\frac{\lambda p_1+(1-\lambda)p_2}{\lambda q_1+(1-\lambda)q_2}$</p> <p>$\qquad\le \sum_x\left[\sum_i \left{ \lambda p_1+(1-\lambda)p_2\right} \log\frac{\lambda p_1+(1-\lambda)p_2}{\lambda q_1+(1-\lambda)q_2}\right]$</p> <p>$\qquad=\sum_x\left[ \lambda p_1(x)\log\frac{ \lambda p_1(x) }{ \lambda q_1(x) } + (1-\lambda)p_2(x)\log\frac{ (1-\lambda) p_2(x) }{ (1-\lambda) q_2(x) } \right]$</p> <p>$\qquad=\lambda\sum_xp_1(x)\log\frac{p_1(x)}{q_1(x)}+(1-\lambda)\sum_xp_2(x)\log\frac{p_2(x)}{q_2(x)}$</p> <p>$\qquad=\lambda D(p_1\parallel q_1)+(1-\lambda) D(p_2\parallel q_2)$</p> <p>ì¦‰, ë‘ p.m.f.ë¥¼ combinationí•œ ë’¤ relative entropyëŠ” ê°ê°ì˜ relative entropyì˜ í‰ê· ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìŒ</p> </li> <li> <p><strong>Thm. Concavity of entropy</strong> $H(p)$ is a concave function of p.m.f. $p$</p> <table> <tbody> <tr> <td>pf 1: Let $p(x)$ is p.m.f., $x\in \chi={x_1,\ldots,x_n}$ with $</td> <td>\chi</td> <td>=n$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>and $u(x):=\frac{1}{</td> <td>\chi</td> <td>}$ is uniform distribution</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$H(p)=\log</td> <td>\chi</td> <td>-D(p\parallel u)$</td> </tr> </tbody> </table> <p>ğŸ’¡ $D(p\parallel u)=\sum p(x)\log\frac{p(x)}{u(x)} = \sum p(x)\log\frac{1}{u(x)}-\sum p(x)\log\frac{1}{p(x)}$</p> <table> <tbody> <tr> <td>Since $\log\frac{1}{u(x)}=\log</td> <td>\chi</td> <td>$ is constant, $D(p\parallel u)=\log</td> <td>\chi</td> <td>-H(p)$.</td> </tr> </tbody> </table> <p>Thus $D(p\parallel u)$ is convex.</p> <p>â‡’ $-D(p\parallel u)$ is concave</p> <table> <tbody> <tr> <td>$H(p)=\log</td> <td>\chi</td> <td>-D(p\parallel u)$</td> </tr> </tbody> </table> <p>$\therefore\ H(p)$ is concave</p> <p>pf 2:</p> </li> </ul> <h2 id="channel-capacity">Channel Capacity</h2> <ul> <li>í•œë²ˆì— (ì¼ì • ì‹œê°„ ë‚´ì—) ìµœëŒ€ë¡œ ì „ì†¡í•  ìˆ˜ ìˆëŠ” ë°ì´í„°</li> </ul> <p>ğŸ’¡ Communication channel</p> <p>outputì´ í™•ë¥ ì ìœ¼ë¡œ inputì— ì˜ì¡´í•˜ëŠ” ì‹œìŠ¤í…œ</p> <ul> <li> <p>Ex (<em>noisy four-symbol channel )</em></p> <p>error rate $p=1/2$</p> <p>(case 1) $P(X=x)$ê°€ uniformí•œ ê²½ìš°</p> <table> <thead> <tr> <th>X \ Y</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>P(X)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>1/8</td> <td>1/8</td> <td>0</td> <td>0</td> <td>1/4</td> </tr> <tr> <td>2</td> <td>0</td> <td>1/8</td> <td>1/8</td> <td>0</td> <td>1/4</td> </tr> <tr> <td>3</td> <td>0</td> <td>0</td> <td>1/8</td> <td>1/8</td> <td>1/4</td> </tr> <tr> <td>4</td> <td>1/8</td> <td>0</td> <td>0</td> <td>1/8</td> <td>1/4</td> </tr> <tr> <td>P(Y)</td> <td>1/4</td> <td>1/4</td> <td>1/4</td> <td>1/4</td> <td>Â </td> </tr> </tbody> </table> <p>Yë¥¼ ë°›ì•˜ì„ ë•Œ Xë¥¼ í•˜ë‚˜ë¡œ ê²°ì •í•  ìˆ˜ ì—†ìŒ</p> <p>â†’ decoding ì‹¤íŒ¨. $I(X;Y)\neq1$</p> <p>(case 2) $P(X=x)$ê°€ uniformí•˜ì§€ ì•Šì€ ê²½ìš°</p> <table> <thead> <tr> <th>X \ Y</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>P(X)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>1/4</td> <td>1/4</td> <td>0</td> <td>0</td> <td>1/2</td> </tr> <tr> <td>2</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>3</td> <td>0</td> <td>0</td> <td>1/4</td> <td>1/4</td> <td>1/2</td> </tr> <tr> <td>4</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>P(Y)</td> <td>1/4</td> <td>1/4</td> <td>1/4</td> <td>1/4</td> <td>Â </td> </tr> </tbody> </table> <p>noisy n-symbol channel</p> <p><img src="/june.github.io/assets/img/post_information_theory/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-09-21_183927.png" alt="n-symbol channel"/></p> <p>Yë¥¼ ë°›ìœ¼ë©´ Xë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŒ</p> <p>â†’ decoding ì„±ê³µ. $I(X;Y)=1$</p> <p>4ê°œì˜ ì •ë³´ë¥¼ ì „í•  ìˆ˜ ìˆëŠ” 2ë¹„íŠ¸ ì±„ë„ì„ ê°€ì§€ê³  ìˆì§€ë§Œ,</p> <p>ì‹¤ì œë¡œëŠ” 1ê³¼ 3 ë‘ ì •ë³´ë§Œ ì£¼ê³ ë°›ëŠ” 1ë¹„íŠ¸ ì±„ë„ë¡œ ì‚¬ìš©í•¨</p> <p>â†’ ì±„ë„ ìš©ëŸ‰ $C=1$ bit</p> <p>ë” ì¢‹ì€ ë°©ë²•ì´ ìˆë‹¤ë©´ C ìˆ˜ì •</p> </li> <li> <p>$C:=\max I(X;Y)$</p> <p>The capacity is the maximum tate at which we can send information over the channel</p> <p>ì±„ë„ì˜ ê¸°ë³¸ì ì¸ ì„¤ì •(ì…ë ¥ ê°€ëŠ¥í•œ ê°’ë“¤ì˜ ë²”ìœ„, error rate ë“±)ì€ ì½”ë”©ì„ í†µí•´ ë°”ê¿€ ìˆ˜ ì—†ìŒ</p> <p>ì½”ë”©ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥í•œ ê°’ì€ $p(x)$</p> <p>ì´ë¥¼ ì¡°ì ˆí•˜ì—¬ ê°€ì¥ í° $C$ë¥¼ ì–»ëŠ” distributionìœ¼ë¡œ ì±„ë„ì„ êµ¬ì„±í•´ì•¼ íš¨ìœ¨ì´ ê°€ì¥ ì¢‹ìŒ</p> </li> </ul> <h3 id="data-processing-inequality">Data Processing inequality</h3> <h4 id="data-flow">Data flow</h4> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%2010.png" alt="noisy channel"/></p> <ul> <li> <p><strong>Def: Markov Chain</strong> $X, Y, Z$ : R.V.s $X\rightarrow Y\rightarrow Z$ (i.e. Markov chain) $\Leftrightarrow p(x,y,z)=p(x)p(y|x)p(z|y)$ and we also can denote as $X\longleftrightarrow Y\longleftrightarrow Z$</p> <p>ğŸ’¡ For $X_1, X_2,\ldots,X_n$, $X_1\rightarrow X_2\rightarrow \cdots X_{n-1}\rightarrow X_n \rightarrow\cdots$ is Markov chain when $P(X_n|X{n-1},\ldots,X_1)=P(X_n|X_{n-1})$</p> <p>Def â‡” $Z$ depends only on $Y$ â€” (1)</p> <p>â‡” $Z$ is conditionally independent of $X$ given $Y$ â€” (2)</p> <table> <tbody> <tr> <td>i.e. $P(X,Z\</td> <td>\ Y)=P(X</td> <td>Y)P(Z</td> <td>Y)$</td> </tr> </tbody> </table> <p>ğŸ’¡ Conditional probability $p(x,y)=p(x)p(y|x)$ $p(x,y,z)=p(x)p(y,z\,|\,x)=p(x)p(y|z)p(z\,|\,y,x)$ â€¦ $p(x_1, x_2,\ldots,x_n)= \prod_{i=1}^n p(x_i\,|\,x_{i-1},\ldots,x_1)$</p> <p>(1) For Markov chain $X\rightarrow Y\rightarrow Z$,</p> <table> <tbody> <tr> <td>we need to show $p(x,y,z)=p(x)p(y</td> <td>x)p(z\,</td> <td>\,y,x)=p(x)p(y</td> <td>x)p(z</td> <td>y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>By the definition, $p(z</td> <td>\ y,x)=p(z</td> <td>y)$ is trivial.</td> </tr> </tbody> </table> <p>ğŸ’¡ $Z$ depends on $X$ only through $Y$</p> <p>ì¦‰, $X$ì˜ ë³€í™”ëŠ” $Y$ë¥¼ í†µí•´ì„œë§Œ $Z$ì— ë°˜ì˜ë¨</p> <p>â‡” $X$ì™€ $Z$ê°€ dependí•˜ì§€ë§Œ, $Y$ë¥¼ í†µí•´ì„œë§Œ dependí•˜ë¯€ë¡œ $Y$ì—ë§Œ dependí•˜ ë³´ì„</p> <p>â‡” $Z$ê°€ $X$ë¡œë¶€í„° dependí•˜ëŠ” ì •ë„ê°€ ì´ë¯¸ $Y$ì— ë°˜ì˜ë˜ì–´ ìˆìŒ</p> <table> <tbody> <tr> <td>Thus, $p(x,y,z)=p(x)p(y</td> <td>x)p(z</td> <td>y)$</td> </tr> </tbody> </table> <p>(2) Given $Y$,</p> <table> <tbody> <tr> <td>$p(x,z\</td> <td>y)=p(x,y,z)/p(y)$ and by the property of Markov chain,</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\qquad\qquad\ =[p(x)p(y</td> <td>x)p(z</td> <td>y)]/{p(y)}=[p(x,y)/p(y)]p(z</td> <td>y)=p(x</td> <td>y)p(z</td> <td>y)$</td> </tr> </tbody> </table> <p>Therefore, $X,Z$ are independent given $Y$</p> <p>By (1), (2), $X\rightarrow Y\rightarrow Z$ $\iff$ $X,Z$ are independent given $Y$</p> <p>and it also can be represented as $Z,X$ are independent given $Y$</p> <p>then, $\iff \ Z\rightarrow Y\rightarrow X$ holds.</p> <p>We may write $X\longleftrightarrow Y\longleftrightarrow Z$</p> <p>Then if $Z=f(Y)$, $X\rightarrow Y\rightarrow f(Y)$?</p> <p>ì§ê´€ì ìœ¼ë¡œ ìƒê°í•  ë•Œ, $Z$ëŠ” $Y$ê°€ ê²°ì •ë˜ëŠ” ìˆœê°„ ê°™ì´ ê²°ì •ë¨</p> <p>i.e., $Y$ determines $Z$ completely.</p> <p>It means $Z$ depends only $Y$ and is the definition of Markov chain.</p> <p>So, this chain is holds.</p> </li> </ul> <p>ğŸ’¡ R.V.s $X, Y, Z$ê°€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ Markov chainì„</p> <ul> <li> <table> <tbody> <tr> <td>$P(Z</td> <td>Y)=P(Z</td> <td>\ Y,X)$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(X,Y,Z)=P(X)P(Y</td> <td>X)P(Z</td> <td>Y)$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(Z</td> <td>Y)P(X</td> <td>Y)=P(X,Z\</td> <td>Y)$</td> </tr> </tbody> </table> </li> <li> <p>ex</p> <p>$X\xrightarrow[channel\,1]{BSC} Y\xrightarrow[channel\,2]{BSC} Z$,</p> <p>error rate of channel1 = $p_1$, error rate of channel2 = $p_2$</p> <p><strong>BSC channel 1</strong></p> <p>Input data</p> <p>given $X=0$</p> <p>given $X=1$</p> <p>Joint distribution</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>3/4</td> <td>1/4</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(Y</td> <td>X=0)$</td> <td>$1-p_1$</td> <td>$p_1$</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(Y</td> <td>X=1)$</td> <td>$p_1$</td> <td>$1-p_1$</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X$ \ $Y$</th> <th>0</th> <th>1</th> <th>$P(X)$</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>$\frac{3}{4}(1-p_1)$</td> <td>$\frac{3}{4}p_1$</td> <td>3/4</td> </tr> <tr> <td>1</td> <td>$\frac{1}{4}p_1$</td> <td>$\frac{1}{4}(1-p_1)$</td> <td>1/4</td> </tr> <tr> <td>$P(Y)$</td> <td>$\frac{3}{4}-\frac{1}{2}p_1$</td> <td>$\frac{1}{4}-\frac{1}{2}p_1$</td> <td>Â </td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Check: $P(X</td> <td>Y)$</td> </tr> </tbody> </table> <p>given $Y$ (ë¹ˆ ì¹¸ ì±„ìš°ê¸°)</p> <p>given $Y=0$</p> <p>given $Y=1$</p> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(X</td> <td>Y=0)$</td> <td>Â </td> <td>Â </td> </tr> </tbody> </table> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(X</td> <td>Y=1)$</td> <td>Â </td> <td>Â </td> </tr> </tbody> </table> <p>$Z=f(Y):=2Y-1$ ë¼ë©´</p> <p>Joint distribution</p> <p>$p_1=1/4$ì¼ ë•Œ $Y$ì˜</p> <p>input data</p> <p>$P(Y=0)=5/8$</p> <p>$P(Y=1)=3/8$</p> <table> <thead> <tr> <th>$Y$ \ $Z$</th> <th>-1</th> <th>1</th> <th>$P(Y)$</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>5/8</td> <td>0</td> <td>5/8</td> </tr> <tr> <td>1</td> <td>0</td> <td>3/8</td> <td>3/8</td> </tr> <tr> <td>$P(Z)$</td> <td>5/8</td> <td>3/8</td> <td>Â </td> </tr> </tbody> </table> <p>$Y$ì˜ ë¶„í¬ë¥¼ ì•Œë©´ $Z$ì˜ ë¶„í¬ê°€ fixë¨</p> <p>($X$ì˜ ë¶„í¬ë¥¼ ëª¨ë¥´ê³  $Y$ ë¶„í¬ë§Œ ì•Œì•„ë„ ê³ ì •ë¨, ê°œì…í•  ì—¬ì§€ê°€ ì—†ì–´ì§)</p> </li> </ul> <h4 id="theorem-data-processing-inequality">Theorem: Data Processing Inequality</h4> <ul> <li> <p>$X\rightarrow Y\rightarrow Z \ \Longrightarrow\ I(X;Y)\ge I(X;Z)$</p> <p>ğŸ’¡ <strong>Probability</strong> $p(x_1,\ldots,x_n)=\prod_{i=1}^n p(x_i\,|\,x_{i-1},\ldots,x_1)$</p> <p><strong>Entropy</strong> $H(x_1,\ldots,x_n)=\sum_{i=1}^n H(x_i\,|\,x_{i-1},\ldots,x_1)$ $=H(x_1)+H(x_2|x_1)+H(x_3\,|\,x_2,x_1)+\cdots$</p> <p><strong>Mutual information</strong> $I(X;Y)=H(X)-H(X|Y)$ $I(X_1,\ldots,X_n\;;\,Y)=\sum_{i=1}^n I(X_i;Y\;|\;X_{i-1},\ldots,X_1)$</p> <p>pf: Since $I(X;\ Y,Z)=I(Y,Z\; ;X)=I(Z,Y\; ;X)$</p> <table> <tbody> <tr> <td>$\qquad\qquad\qquad\qquad\;\;\,=I(Z;X)+I(Y;X\;</td> <td>Z)=I(X;Z)+I(X;Y\;</td> <td>Z)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\qquad\qquad\qquad\qquad\;\;\,=I(X;\ Z,Y)=I(X;Y)+I(X;Z\;</td> <td>Y)$,</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X;Z)+I(X;Y\</td> <td>Z)=I(X;Y)+I(X;Z\</td> <td>Y)$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$I(X;Z\</td> <td>Y)=0$ by property of Markov Chain</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>$X,Z$ are conditionally independent given $Y$ $\Rightarrow\ I(X;Z\</td> <td>Y)=0$</td> </tr> </tbody> </table> </li> </ul> <table> <tbody> <tr> <td>$I(X;Y)=I(X;Z)+I(X;Y\</td> <td>Z)\ge I(X;Z)$ $\because I(A;B)\ge0$</td> </tr> </tbody> </table> </li> </ul> <p>ğŸ’¡ ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³¼ ë•Œ, $X$ë¡œë¶€í„° $Y$ë¥¼ ì•„ëŠ” ê²ƒì´ $X$ë¡œë¶€í„° $Z$ë¥¼ ì•„ëŠ” ê²ƒë³´ë‹¤ ì‰¬ì›€ (ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ë” ë§ìŒ). ì´ë•Œ ë“±í˜¸ê°€ ì„±ë¦½í•œë‹¤ëŠ” ê²ƒì€ $Y$ë¥¼ ê±°ì³ $Z$ì—ì„œ $X$ê°€ ì˜í–¥ì„ ë¯¸ì§€ëŠ” ì •ë„ì— ì†ì‹¤ì´ ì—†ìŒì„ ì˜ë¯¸í•¨</p> <ul> <li> <p>Corollary $Z=g(Y)\ \Longrightarrow\ I(X;Y)\ge I\left(X;g(Y)\right)$</p> <p>pf: $X\longrightarrow Y\longrightarrow g(Y)$</p> <p>$Y\rightarrow g(Y)$ëŠ” ìµœëŒ€ 1-1 ëŒ€ì‘ ê´€ê³„ì´ë¯€ë¡œ $g(Y)$ì˜ ì •ë³´ëŠ” $Y$ë³´ë‹¤ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì˜ ë²”ìœ„ê°€ ì‘ìŒ</p> <p>â‡’ trivial!</p> </li> <li> <p>Corollary $X\longrightarrow Y\longrightarrow Z\;\Longrightarrow\; I(X;Y\ |Z)\le I(X;Y)$</p> <table> <tbody> <tr> <td>pf: Since $I(X;\ Y,Z)=I(X;Y)+I(X;Z\</td> <td>Y)=I(X;Y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>and $I(X;\ Y,Z)=I(X;Z)+I(X;Y\</td> <td>Z)$,</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\Rightarrow\ I(X;Y)=I(X;Z)+I(X;Y\</td> <td>Z)\ge I(X;Y\</td> <td>Z)$ $\because I(X;Z)\ge0$</td> </tr> </tbody> </table> </li> </ul> <p>ğŸ’¡ $X\longrightarrow Y\longrightarrow Z$ The dependency of $X$ and $Y$ is decreased (or unchanged) by the observation of a downstream (to receiver) R.V. $Z$ i.e., $I(X;Y)\ge I(X;Y\ |Z)$</p> <ul> <li> <p>ex</p> <p>case1) $X_1, X_2, X_3$ : independent R.V.s</p> <p>case2) $X_1$ : dice {1, 2, â€¦ , 6}, $X_2$ : biased coin {0, 1}, $X_3=2X_2-1$</p> <table> <thead> <tr> <th>$X_1$</th> <th>1</th> <th>â€¦</th> <th>6</th> </tr> </thead> <tbody> <tr> <td>$P(X_1)$</td> <td>1/6</td> <td>Â </td> <td>1/6</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X_2$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X_2)$</td> <td>$1-\theta$</td> <td>$\theta$</td> </tr> </tbody> </table> <p>$\theta:=\frac{1}{X_1}\in\left{ 1, \frac{1}{2},\frac{1}{3},\ldots,\frac{1}{6} \right}$</p> <p>$X_1=2$ â‡’ fair coin toss</p> <p>Which case is a Markov chain as $X\longrightarrow Y\longrightarrow Z$?</p> <table> <tbody> <tr> <td>Definition : $p(x,y,z)=p(x)p(y</td> <td>x)p(z</td> <td>y)$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>and we can lead $P(Z</td> <td>Y)P(X</td> <td>Y)=P(X,Z\</td> <td>Y)$</td> </tr> </tbody> </table> <p>â‡’ given $Y$, $X$and $Z$ are independent from Markov chain</p> <p>case1) and case2) are both satisfied the definition of Markov chain</p> <table> <tbody> <tr> <td>case1) $P(X_1)P(X_3)=P(X_1,X_3)$, $P(X_1,X_3\</td> <td>X_2)=P(X_1</td> <td>X_2)P(X_3</td> <td>X_2)$</td> </tr> </tbody> </table> <p>case2) $X\longrightarrow Y\longrightarrow g(Y)$ í˜•íƒœì´ë©´ Markov chain</p> <p>$X_2$ depends $X_1$, $X_3$ is determined by $X_2$.</p> <p>ë”°ë¼ì„œ $X_3$ëŠ” $X_1$ì— dependí•˜ì§€ë§Œ, $X_2$ë¥¼ í†µí•´ì„œ ì˜í–¥ì„ ë°›ìŒ</p> <p>($X_1$ì€ $X_2$ë¥¼ í†µí•´ì„œê°€ ì•„ë‹Œ ë°©ë²•ìœ¼ë¡œ $X_3$ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ìŒ)</p> </li> </ul> <p><strong>Caution!</strong> $I(X;Y\ |Z)&gt;I(X;Y)$ but $I(X;Y\ |Z)\neq I(X;Y)$ is possible</p> <ul> <li> <p>ex: $X,Y$ are fair coin tosses (independent)<br/> $Z:=X+Y$ â€” Not Markov chain</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$Y$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(Y)$</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X$ \ $Y$</th> <th>0</th> <th>1</th> <th>$P(X)$</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1/4</td> <td>1/4</td> <td>1/2</td> </tr> <tr> <td>1</td> <td>1/4</td> <td>1/4</td> <td>1/2</td> </tr> <tr> <td>$P(Y)$</td> <td>1/2</td> <td>1/2</td> <td>Â </td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X,Y$ \ $Z$</th> <th>0</th> <th>1</th> <th>2</th> <th>$P(X,Y)$</th> </tr> </thead> <tbody> <tr> <td>0, 0</td> <td>1/4</td> <td>0</td> <td>0</td> <td>1/2</td> </tr> <tr> <td>0, 1</td> <td>0</td> <td>1/4</td> <td>0</td> <td>1/2</td> </tr> <tr> <td>1, 0</td> <td>0</td> <td>1/4</td> <td>0</td> <td>1/4</td> </tr> <tr> <td>1, 1</td> <td>0</td> <td>0</td> <td>1/4</td> <td>1/4</td> </tr> <tr> <td>$P(Z)$</td> <td>1/4</td> <td>1/2</td> <td>1/4</td> <td>Â </td> </tr> </tbody> </table> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> <th>Â </th> </tr> </thead> <tbody> <tr> <td>$P(X</td> <td>Z=1)$</td> <td>1/2</td> <td>1/2</td> </tr> </tbody> </table> <p>ğŸ’¡ - $I(X;Y)=0$ since $X,Y$ are independent</p> <ul> <li> <table> <tbody> <tr> <td>$I(X;Y\</td> <td>Z)=H(X</td> <td>Z)-H(X</td> <td>\; Y,Z)$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$H(X</td> <td>\;Y,Z)=0$ since given $Y,Z$, $X=Z-Y$ determined</td> </tr> </tbody> </table> </li> </ul> <table> <tbody> <tr> <td>$I(X;Y\</td> <td>Z)=H(X</td> <td>Z) \ \qquad\qquad\quad =P(Z=0)H(X</td> <td>Z=0) \quad\longrightarrow P(X=0)=1 \quad (X=0,Y=0)\ \qquad\qquad\qquad +P(Z=1)H(X</td> <td>Z=1)\ \qquad\qquad\qquad +P(Z=2)H(X</td> <td>Z=2) \quad\longrightarrow P(X=1)=1\quad (X=1,Y=1) \ \qquad\qquad\quad =P(Z=1)H(X</td> <td>Z=1)=\frac{1}{2}H\left(\frac{1}{2},\frac{1}{2}\right)=1/2$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>ì¦‰, $I(X;Y)=0&lt;I(X;Y\</td> <td>Z)=1/2$</td> </tr> </tbody> </table> <p>Markov chainì´ë©´ ë¶€ë“±í˜¸ê°€ ë°˜ëŒ€ì—¬ì•¼ í•˜ë¯€ë¡œ $X\longrightarrow Y\longrightarrow Z$ëŠ” Markov chainì´ ì•„ë‹˜</p> </li> </ul> <h3 id="sufficient-statistics">Sufficient Statistics</h3> <ul> <li>Given $\theta \longrightarrow X \longrightarrow T(X)$, if $I(\theta;T(X))=I(\theta;X)$, then $T(X)$ is called sufficient for $\theta$. <ul> <li> <p>${f_{\theta}(x)}$: family of p.mp.f. indexed by $\theta$ ; 1-parameter family</p> <p>$\theta$ê°€ ê²°ì •ë˜ë©´ $f$ê°€ ê²°ì •ë˜ê³  $x$ì— ì˜í•œ ê°’ì„ ì–»ì„ ìˆ˜ ìˆëŠ” í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜</p> <p>($\theta$ : index parameter, $\theta$ì— ëŒ€í•˜ì—¬ í™•ë¥ ì ì¸ ë¶€ë¶„ì´ ìˆë‹¤ë©´ R.V.)</p> <p>â†’ parameter í•˜ë‚˜ê°€ ê²°ì •ë˜ë©´ í•¨ìˆ˜ì˜ ë³€ìˆ˜ $X$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ìˆìŒ</p> <p>ex:</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>$1-\theta$</td> <td>$\theta$</td> </tr> </tbody> </table> <p>$P(X=1)=\theta$</p> </li> </ul> <p>$X\sim f_{\theta}(x)$ : $X$ is a R.V. from a distribution in this family ${f_{\theta}(x)}$</p> <p><img src="/june.github.io/assets/img/post_information_theory/Untitled%2011.png" alt="coin tossing"/></p> <p>coinë§ˆë‹¤ $\theta$ê°€ ì •í•´ì ¸ ìˆìŒ</p> <p>${f_{\theta}(x)}\longrightarrow f_{\theta}(x)\longrightarrow X$</p> <p>family â†’ p.m.f. â†’ R.V.</p> <p>$X \longrightarrow T(X)$ : statistic, $X$ë¡œë¶€í„° ì–»ì€ í†µê³„ëŸ‰, funtion of $X$</p> <p>Then, $\theta, X, T(X)$ form a Markov chain. i.e., $\theta \longrightarrow X \longrightarrow T(X)$</p> <p>â‡’ $\theta \longrightarrow X \longrightarrow T(X) \;\Longrightarrow I(\theta;T(X))\le I(\theta;X)$</p> <p>If $I(\theta;T(X))=I(\theta;X)$, then $T(X)$ is called sufficient for $\theta$.</p> <p>ğŸ’¡ ì‹¤í—˜ ë°ì´í„°ë¥¼ í•´ì„í•  ë•Œ,</p> <ul> <li> <p>ì‹¤í—˜ ëŒ€ìƒì´ familyë¡œë¶€í„° ì‹¤í—˜ ëŒ€ìƒì„ ê±°ì³ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì–»ê¸°ê¹Œì§€</p> <p>ex: í˜„ì‹¤ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ë©´ familyì— ëŒ€í•œ ì •ë³´ëŠ” ì•Œ ìˆ˜ ì—†ê³  ì‹¤í—˜ ê²°ê³¼ë§Œ ì•Œ ìˆ˜ ìˆìŒ</p> <p>ë™ì „ì„ 10ë²ˆ ë˜ì ¸ì„œ 8ë²ˆì´ Hê°€ ë‚˜ì˜¤ë©´, Hê°€ ë‚˜ì˜¬ í™•ë¥ ì´ $\theta=4/5$ì¸ ë™ì „ì„ ì‚¬ìš©í–ˆë‹¤ê³  ì¶”ì •í•˜ëŠ” ê²ƒì´ íƒ€ë‹¹í•¨.</p> <p>ë¬¼ë¡  ì‹¤ì œë¡œ ì‚¬ìš©í•œ ë™ì „ì´ Hê°€ 4/5ì˜ í™•ë¥ ë¡œ ë‚˜ì˜¤ëŠ” ë™ì „ì¸ì§€ëŠ” ì•Œ ìˆ˜ ì—†ìŒ Hê°€ ë‚˜ì˜¬ í™•ë¥ ì´ 1/3ì¸ë° ë˜ì§€ëŠ” ë°©ì‹ì— ì˜í•´ Hë§Œ ë‚˜ì™”ì„ ìˆ˜ë„ ìˆìŒ</p> <p>ì´ë ‡ë“¯ $n$ë²ˆì˜ ì‹œí–‰ìœ¼ë¡œë¶€í„° ì–»ì€ ë°ì´í„°ì—ì„œ $T(X)$ë¥¼ êµ¬í•˜ê³  ê°œë³„ ì‹œí–‰ì˜ ê²°ê³¼ëŠ” ì‹œí–‰ í•  ë•Œë§ˆë‹¤ ë‹¤ë¥´ê² ì§€ë§Œ $T(X)$ë¥¼ í†µí•´ $n$ë²ˆ ì¤‘ Hê°€ ë‚˜ì˜¤ëŠ” íšŸìˆ˜ì˜ ê¸°ëŒ€ê°’ $\theta$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ê³  ìš”ì•½ ê°€ëŠ¥í•¨</p> <p>â‡’ sufficient statistics</p> <p>= í†µê³„ëŸ‰ì„ ì˜ ì¡ìœ¼ë©´ ëª‡ íšŒì˜ ì‹œí–‰ì´ ìˆì—ˆëŠ”ì§€, ëª‡ ë²ˆ ì›í•˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ì§€ ëª¨ë¥´ë”ë¼ë„ $\theta$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŒ</p> <p>information lossê°€ í•˜ë‚˜ë„ ì—†ìŒì„ ë³´ì´ê¸° ìœ„í•œ ê°€ì •ìœ¼ë¡œ ì“°ì„</p> </li> </ul> </li> <li>Def <ul> <li> <p>Informal version</p> <p>Informmaly, $T(X)$ is called sufficient for $\theta$ if it contains all information in $X$ about $\theta$.</p> <p>ì¦‰, $\theta$ì— ëŒ€í•´ì„œ $X$ê°€ ê°€ì§€ê³  ìˆëŠ” informationì„ $T(X)$ê°€ ëª¨ë‘ ê°€ì§€ê³  ìˆìŒ</p> </li> <li> <p>1st formal version</p> <p>$T(X)$ is said to be a sufficient statistic relative to ${f_{\theta}(x) }$</p> <p>if $X$ is independent of $\theta$ given $T(X)$ for any distributioni $f_{\theta}(x)$.</p> <p>ì¦‰, $T(X)$ë¥¼ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ indepentí•¨</p> <ul> <li> <p>$X$ì™€ $\theta$ì˜ dependí•œ ì •ë³´ëŠ” ëª¨ë‘ $T(X)$ì— ì†í•´ìˆìŒ</p> <p>$X$ì™€ $\theta$ëŠ” ë…ë¦½ì´ ì•„ë‹˜ â‡’ $X$ì™€ $\theta$ê°€ ë…ë¦½ì´ë©´ ì„œë¡œì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŒ (ì§ê´€ì ìœ¼ë¡œ ë…ë¦½ì´ë©´ ì„œë¡œ ë¬´ê´€í•˜ê¸° ë•Œë¬¸ì— ì˜í–¥ì„ ì£¼ê³  ë°›ê³ ë¥¼ ë”°ì§ˆ ì˜ë¯¸ê°€ ì—†ì–´ ë³´ì„)</p> <p>ì£¼ì–´ì§„ $T(X)$ì— ëŒ€í•´ì„œ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ ë…ë¦½ì´ë¼ëŠ” ë§ì€ $X$ì™€ $\theta$ê°€ ì£¼ê³ ë°›ëŠ” ì •ë³´ë¥¼ $T(X)$ê°€ ëª¨ë‘ ê°€ì ¸ê°”ìŒì„ ì˜ë¯¸í•¨</p> <p>â‡’ $T(X)$ì— ì†í•œ $X,\theta$ì˜ ì •ë³´ë¥¼ ì œì™¸í•˜ë©´ $X$ì™€ $\theta$ëŠ” ë…ë¦½ì„</p> </li> </ul> </li> <li> <p>2nd formal version</p> <p>If $\theta\longrightarrow X\longrightarrow T(X)$ is Markov chain then $\theta\longrightarrow T(X)\longrightarrow X$</p> <ul> <li> <p>$T(X)$ê°€ $\theta$ì™€ $X$ë¥¼ ì—°ê²°í•´ì£¼ëŠ” ëª¨ë“  ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ</p> <p>â‡” $\theta$ì™€ $X$ë¥¼ ì—°ê²°í•´ì£¼ëŠ” $T(X)$ê°€ ì£¼ì–´ì§€ë©´ $X$ì™€ $\theta$ê°€ indepentí•¨</p> <p>â‡” $T(X)$ë¥¼ ì•Œê³  ìˆì„ ë•Œ $X$ì™€ $\theta$ê°€ indepentí•¨</p> </li> </ul> </li> </ul> <p>ğŸ’¡ If $T(X)$ is a sufficient statistic, $I(\theta\,;T(X))= I(\theta\,;X)$</p> <ul> <li> <p>i.e., No information loss</p> <p>2nd formal version definition shows that $I(\theta\,;T(X))\ge I(\theta\,;X)$.</p> <p>And in general, $\theta\longrightarrow X\longrightarrow T(X)$ then $I(\theta\,;T(X))\le I(\theta\,;X)$.</p> <p>Therefore, if $T(X)$ is a sufficient statistic, $I(\theta\,;T(X))= I(\theta\,;X)$.</p> </li> </ul> </li> </ul> <p>ğŸ’¡ Where $X_1, \ldots, X_n \sim f_{\theta}(x)$, $Y:=g(X_1, \ldots,X_n)$ is sufficient for $\theta$</p> <ul> <li> <table> <tbody> <tr> <td>if $P(X_1=x_1,\ldots,X_n=x_n\;</td> <td>Y=y)$ does not depend on $\theta$.</td> </tr> </tbody> </table> <hr/> <p>Let $X_1, \ldots,X_n$ be R.V.s from a p.m.f. with parameter $\theta$</p> <p>i.e., $X_1, \ldots, X_n \sim f_{\theta}(x)$</p> <p>and $Y:=g(X_1, \ldots,X_n)$ is a funtion of $X_1, \ldots,X_n$</p> <p>i.e., $Y$ is deterministic for $X_1, \ldots,X_n$</p> <p>Then $Y$ is sufficient for $\theta$, if the conditional probability $P(X_1=x_1,\ldots,X_n=x_n\;|Y=y)$ does not depend on $\theta$.</p> <hr/> <p>Since $Y=g(X_1,\ldots,X_n)$, $\theta\longrightarrow Y\longrightarrow (X_1,\ldots,X_n)$.</p> <table> <tbody> <tr> <td>â‡’ $P(X_1=x_1,\ldots,X_n=x_n\;</td> <td>Y=y)\ \quad = P(X_1=x_1,\ldots,X_n=x_n\;</td> <td>Y=y,\Theta=\theta)$</td> </tr> </tbody> </table> <p>ì¦‰, ì£¼ì–´ì§„ $Y$ì— ëŒ€í•˜ì—¬ $\theta$ì™€ $(X_1,\ldots,X_n)$ê°€ independent.</p> </li> <li> <p>ex. Bernoulli distribution</p> <table> <thead> <tr> <th>$X$</th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>$P(X)$</td> <td>$1-\theta$</td> <td>$\theta$</td> </tr> </tbody> </table> <p>For sample space $\chi$, $x\in\chi={0,1}$.</p> <p>$P_\theta(X=x)= \left{\begin{matrix} \theta &amp; x=1 <br/> 1-\theta &amp; x=0 <br/> \end{matrix}\right.\quad\textrm{(p.m.f.)} \ \qquad\qquad\quad = \theta^x (1-\theta)^{1-x}$</p> <p>Let $X_1, \ldots, X_n$ be R.V.s sample from a Bernoulli distribution with $\theta$.</p> <p>Bernoulli : i.i.d.(independent and identically distributed)</p> <p>i.e., $P(X_1=x_1,\ldots,X_n=x_n)=P(X_1=x_1)\cdots P(X_n=x_n)$</p> <p>$\qquad\qquad\qquad\qquad\qquad\quad\;\, =\theta^{x_1}(1-\theta)^{1-x_1}\cdots \theta^{x_n}(1-\theta)^{1-x_n}$</p> <p>$\qquad\qquad\qquad\qquad\qquad\quad\;\, =\prod_{i=1}^n\theta^{x_i}(1-\theta)^{1-x_i}$</p> <p>Introduce a statistic $Y:=T(X_1,\ldots,X_n)=\sum_{i=1}^n X_i$.</p> <p>â‡’ counting 1â€™s out of $x_1,\ldots,x_n$ â‡” counting the number of Heads in $n$ tosses</p> <p>$Y\sim B(n,\theta)$ : Binomial ditribution</p> <p>â‡’ $P(Y=k)=\binom{n}{k}\theta^k(1-\theta)^{n-k}$</p> <p>Consider the conditional probabiltiy</p> <p>$P(X_1=x_1,\ldots,X_n=x_n)=\frac{P(X_1,=x_1\ ,\ldots,\ X_n=x_n \textrm{ and } Y=k)}{P(Y=k)}$</p> <p>$\qquad\qquad\qquad\qquad\qquad=\frac{\prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i}}{\binom{n}{k}\theta^k(1-\theta)^{n-k}}\textrm{\quad where }\sum_{i=1}^n x_i =k$</p> <p>$\qquad\qquad\qquad\qquad\qquad=\frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i} }{\binom{n}{k}\theta^k(1-\theta)^{n-k}}$</p> <p>$\qquad\qquad\qquad\qquad\qquad=\frac{\theta^{k}(1-\theta)^{n-k} }{\binom{n}{k}\theta^k(1-\theta)^{n-k}}$</p> <p>$\qquad\qquad\qquad\qquad\qquad=\frac{1}{\binom{n}{k}}$</p> <p>â‡’ $P(X_1=x_1,\ldots,X_n=x_n)=\frac{1}{\binom{n}{k}}$ does not depend on $\theta$,</p> <p>which means that given $Y=k\left( =\sum_{i=1}^n x_i \right)$,</p> <p>the individual values of the $x_i$â€™s cannot provide additional information about $\theta$.</p> <p>ì¦‰, $Y$ ê°’ì´ ì£¼ì–´ì§€ë©´ $x_i$ ê° ê°’ì€ (ê° $x_i$ê°€ 0ì¸ì§€ 1ì¸ì§€ëŠ”) $\theta$ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ</p> </li> <li> <p>ex. Poisson distribution</p> <p>p.m.f. $p(k)=\frac{\lambda^k e^{-\lambda}}{k!}$ for $k=0,1,2,\ldots$ ($\lambda$ë¥¼ ì•Œë©´ ëª¨ë“  parameterê°€ ê²°ì •ë¨)</p> <p>Let $X_1, \ldots, X_n$ be sample from a Poisson distribution with $\lambda$ and $Y=\sum_{i=1}^n X_i$ (=í†µê³„ëŸ‰)</p> <p>â€» $r$ : average rate of the event</p> <p>$t$ : time interval ($\lambda=rt$ : ì£¼ì–´ì§„ ì‹œê°„ $t$ë™ì•ˆ eventê°€ ë°œìƒí•˜ëŠ” íšŸìˆ˜ì˜ í‰ê· , ë¹ˆë„)</p> <p>â‡’ $P(k\textrm{ events in interval }t) = \frac{(rt)^k e^{-rt}}{k!}$</p> <p>$Y=\sum_{i=1}^n X_i \sim$ Poisson dist with $\lambda y=n\lambda$</p> <ul> <li> <p>$Y$ is sufficient for $\lambda$.</p> <p>Consider the conditional probability</p> <table> <tbody> <tr> <td>$P(X_1=x_1,\ldots, X_n=x_n\;</td> <td>Y=y)=\frac{P(X_1=x_1,\ldots, X_n=x_n\textrm{ and }Y=y)}{P(Y=y)}$</td> </tr> </tbody> </table> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{P(X_1=x_1,\ldots,X_n=x_n)}{P(Y=y)}\quad\textrm{ where }\sum x_i=y$</p> <p>(note that $\sum x_i\neq y\Rightarrow P(*)=0$)</p> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\prod_{i=1}^n\frac{\lambda^{x_i}e^{-\lambda}}{x_i!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$</p> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\frac{\lambda^{x_1+\cdots+x_n}e^{-\lambda}}{x_1!x_2!\cdots x_n!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$</p> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{\frac{\lambda^y e^{-\lambda}}{x_1!\cdots x_n!} }{\frac{(n\lambda)^y e^{-n\lambda}}{y!} }$</p> <p>$\qquad\qquad\qquad\qquad\qquad\qquad\quad\;=\frac{y!}{n^yx_1!\cdots x_n!}$ does not depend on $\lambda$.</p> </li> </ul> </li> <li> <p>Thm: Factorization Theorem $Y=g(X_1,\ldots,X_n)$ is sufficient for $\theta$ $\iff P(x_1,\ldots,x_n\;|\,\theta)=\phi(g(x_1,\ldots,x_n)\,|\,\theta)h(x_1,\ldots,x_n)$</p> <hr/> <p>Let $X_1,\ldots,X_n$ be i.i.d. R.V.s from p.m.f. $f_\theta(x)\in {f_\theta(x)}$,</p> <p>$Y=g(X_1,\ldots,X_n)$ is sufficient for $\theta$</p> <table> <tbody> <tr> <td>$\iff P(x_1,\ldots,x_n\;</td> <td>\,\theta)=\phi(g(x_1,\ldots,x_n)\,</td> <td>\,\theta)h(x_1,\ldots,x_n)$</td> </tr> </tbody> </table> <p>where $\phi$ depends on $x_i$â€™s only through $g$ (i.e., $Y$) and $h$ does not depend on $\theta$.</p> <hr/> <p>$\theta$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê° $X_i$ê°€ ê°’ $x_i$ë¥¼ ê°€ì§ˆ í™•ë¥ ì— ëŒ€í•˜ì—¬</p> <p>$\theta$ì— ì˜ì¡´í•˜ëŠ” ê°’ë“¤ì˜ í™•ë¥ ì€ $\phi$, ì¦‰ $g(x_1,\ldots,x_n)=Y$ì— ëª°ë ¤ìˆìŒ ($h$ëŠ” $\theta$ì— independent)</p> <p>â‡’ $Y$ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ” ê°’ë“¤ì€ $\theta$ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ</p> <ul> <li> <p>Remark</p> <p>ğŸ’¡ $P(x_1,\ldots,x_n):=P(X_1=x_1,\ldots,X_n=x_n)$</p> <p>$P(X_1,\ldots,X_n)$ê°€ $x_1,\ldots,x_n$ì„ ì–´ë–»ê²Œ ê²°ì •í•˜ëŠëƒì— ë”°ë¼ ë‹¬ë¼ì§ì„ ë°˜ì˜í•œ í‘œê¸°ì„</p> <table> <tbody> <tr> <td>ğŸ’¡ $P(<em>;\theta)=P(</em></td> <td>\theta)$ì´ê¸° ìœ„í•´ì„œëŠ” $\theta$ê°€ R.V.ì—¬ì•¼ í•¨</td> </tr> </tbody> </table> <p>ìš°í•­ì˜ í‘œê¸°ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ì§€ë§Œ, $\theta$ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•˜ê³  ì‹¶ì§€ ì•ŠìŒ</p> <p>ì¢Œí•­ì˜ $\theta$ ëŠ” determined $\theta$ ë˜ëŠ” fixed $\theta$, indexed by $\theta$ì´ê³  ìš°í•­ì˜ $\theta$ëŠ” p.m.f.ë¥¼ ê°–ëŠ” R.V.ì„</p> </li> </ul> </li> <li> <p>ex: For Bernoulli distribution,</p> <p>$P(x_1,\ldots,x_n\ ;\theta)=\prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i})=\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}$</p> <p>Let $y=\sum_{i=1}^n x_i$,</p> <p>$\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}=\theta^{y}(1-\theta)^{n-y}\times1:=\phi(y\,;\theta)\times h(x_1,\ldots,x_n)$</p> <p>â‡’ Bernoulli distributionì— ëŒ€í•˜ì—¬</p> <p>$P(x_1,\ldots,x_n\ ;\theta)=\phi(y\,;\theta)\times h(x_1,\ldots,x_n)$ë¥¼ ë§Œì¡±í•˜ë¯€ë¡œ</p> <p>$Y=\sum X_i$ëŠ” $\theta$ì— ëŒ€í•˜ì—¬ sufficient statisticì„</p> </li> <li> <p>ex: For Poisson distribution,</p> <p>$p(x_1, \ldots,x_n\,;\lambda)=\frac{\lambda^{x_1+\cdots+x_n}e^{-n\lambda} }{x_1!\cdots x_n!}=\lambda^y e^{-n\lambda}\times\frac{1}{x_1!\cdots x_n!}$</p> <p>â‡’ Poisson distributionì— ëŒ€í•˜ì—¬</p> <p>$P(x_1,\ldots,x_n\ ;\lambda)=\phi(y\,;\lambda)\times h(x_1,\ldots,x_n)$ë¥¼ ë§Œì¡±í•˜ë¯€ë¡œ</p> <p>$Y=\sum X_i$ëŠ” $\lambda$ì— ëŒ€í•˜ì—¬ sufficient statisticì„</p> </li> </ul> <h4 id="minimal-sufficient-statistic">Minimal sufficient statistic</h4> <p>Def: $T(X)$ is a minimal sufficient statistic relative to ${ f_\theta(x) }$ if it is a function of every other sufficient statistic $U$.</p> <p>ì¦‰, sufficient statisticì´ë©´ì„œ ë‹¤ë¥¸ sufficient statisticë¡œ í‘œì‹œë˜ëŠ” í•¨ìˆ˜</p> <p>$\theta \longrightarrow T(X) \longrightarrow U(X) \longrightarrow X$</p> <p>ğŸ’¡ A minimal sufficient statistic maximally compresses the information about $\theta$ in the sample.</p> <p>Other sufficient statistics may contain additional irrelevant information.</p> <ul> <li>ex <ul> <li> <p>$Y=X_1+\cdots+X_n$ is minimal sufficient statistic for $\theta$ in the independent coin toss example.</p> <p>Let $Y_{odd}=X_1+X_3+\cdots+X_{2n-1}$, $Y_{even}=X_2+X_4+\cdots+X_{2n}$ and $\widetilde{Y}=(Y_{odd}, Y_{even})$, then $\widetilde{Y}$ is a sufficient statistic</p> <p>$Y=g(\widetilde{Y})=g(Y_{odd},Y_{even})=Y_{odd}+Y_{even}$</p> <p>â‡’ $Y$ì™€ ë¹„êµí•  ë•Œ, $Y_{odd}+Y_{even}$ëŠ” ë¶ˆí•„ìš”í•˜ê²Œ ì„¸ë¶€ì ìœ¼ë¡œ ë‚˜ë‰œ í•¨ìˆ˜ì„</p> </li> <li> <p>í•™êµì—ì„œ ì „êµ ìˆ˜í•™ì ìˆ˜ í‰ê· ì„ ì•Œê¸¸ ë°”ë„ ë•Œ,</p> <ul> <li>ì „ì²´ ë°ì´í„°ë¡œë¶€í„° ê° ë°˜ì˜ í‰ê·  ì ìˆ˜ë¥¼ êµ¬í•˜ê³  ì´ê²ƒë“¤ì„ ëª¨ì•„ì„œ ë‹¤ì‹œ ì „ì²´ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒë³´ë‹¤</li> <li>ì „ì²´ ë°ì´í„°ë¡œë¶€í„° í•œ ë²ˆì— ì „ì²´ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ í›¨ì”¬ íš¨ìœ¨ì ì„ (ë°˜ í‰ê· ì´ë¼ëŠ” ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ)</li> </ul> </li> </ul> </li> </ul>]]></content><author><name></name></author><category term="Study"/><category term="MATH"/><summary type="html"><![CDATA[Information theory: Probability and Entropy]]></summary></entry></feed>